test_accreal_type_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_all_any_empty_xpu (__main__.TestReductionsXPU) ... ok
test_all_any_vs_numpy_xpu_bool (__main__.TestReductionsXPU) ... ok
test_all_any_vs_numpy_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_all_any_vs_numpy_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_all_any_vs_numpy_xpu_float16 (__main__.TestReductionsXPU) ... FAIL
test_all_any_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... FAIL
test_all_any_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... FAIL
test_all_any_vs_numpy_xpu_int16 (__main__.TestReductionsXPU) ... ok
test_all_any_vs_numpy_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_all_any_vs_numpy_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_all_any_vs_numpy_xpu_int8 (__main__.TestReductionsXPU) ... ok
test_all_any_vs_numpy_xpu_uint8 (__main__.TestReductionsXPU) ... ok
test_all_any_with_dim_xpu (__main__.TestReductionsXPU) ... ok
test_all_any_xpu (__main__.TestReductionsXPU) ... ok
test_amax_xpu_bool (__main__.TestReductionsXPU) ... ok
test_amax_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_amax_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_amax_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_amax_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_amin_amax_some_dims_xpu (__main__.TestReductionsXPU) ... ok
test_amin_xpu_bool (__main__.TestReductionsXPU) ... ok
test_amin_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_amin_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_amin_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_amin_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_aminmax_xpu_bfloat16 (__main__.TestReductionsXPU) ... ok
test_aminmax_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_aminmax_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_argminmax_axis_with_dim_one_xpu (__main__.TestReductionsXPU) ... ok
test_argminmax_large_axis_xpu (__main__.TestReductionsXPU) ... FAIL
test_argminmax_multiple_xpu_float16 (__main__.TestReductionsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py:1539: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x[mask] = torch.tensor(max_val + 1, dtype=dtype)
/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py:1542: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x[mask] = torch.tensor(min_val - 1, dtype=dtype)
FAIL
test_argminmax_multiple_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_argminmax_multiple_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_argminmax_multiple_xpu_int16 (__main__.TestReductionsXPU) ... FAIL
test_argminmax_multiple_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_argminmax_multiple_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_argminmax_multiple_xpu_int8 (__main__.TestReductionsXPU) ... FAIL
test_argminmax_multiple_xpu_uint8 (__main__.TestReductionsXPU) ... FAIL
test_bincount_xpu (__main__.TestReductionsXPU) ... ok
test_bucketization_xpu (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::searchsorted.Tensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::searchsorted.Tensor' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_count_nonzero_xpu_float16 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_float32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_float64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_int16 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_int32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_int64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_int8 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_cumprod_integer_upcast_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_cumsum_integer_upcast_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_dim_arg_reduction_scalar_xpu_bfloat16 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_int16 (__main__.TestReductionsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py:1828: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  x = torch.tensor(example, device=device, dtype=dtype)
/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py:1834: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  x = torch.tensor(example, device=device, dtype=dtype)
ok
test_dim_arg_reduction_scalar_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_int8 (__main__.TestReductionsXPU) ... ok
test_dim_arg_reduction_scalar_xpu_uint8 (__main__.TestReductionsXPU) ... ok
test_dim_default_all_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_amax_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_amin_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_any_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_argmax_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_argmin_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_default_keepdim_all_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_amax_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_default_keepdim_amin_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_default_keepdim_any_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_argmax_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_argmin_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_mean_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_nanmean_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_default_keepdim_nansum_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_prod_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_std_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_sum_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_keepdim_var_xpu (__main__.TestReductionsXPU)
Tests that the default dim, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_default_mean_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_nanmean_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_nansum_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_prod_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_std_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_sum_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_default_var_xpu (__main__.TestReductionsXPU)
Tests that the default dim reduces all dimensions. ... ok
test_dim_ndim_limit_all_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_amax_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_amin_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_any_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_argmax_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_argmin_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... skipped "not_implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_ndim_limit_mean_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_nanmean_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_nansum_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_prod_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_std_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_sum_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_ndim_limit_var_xpu (__main__.TestReductionsXPU)
Tests that an exception is raised when reducing a tensor with more ... ok
test_dim_none_all_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_amax_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_amin_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_any_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_argmax_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_argmin_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_none_keepdim_all_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_amax_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_none_keepdim_amin_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_none_keepdim_any_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_argmax_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_argmin_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_mean_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_nanmean_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... ok
test_dim_none_keepdim_nansum_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_prod_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_std_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_sum_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_keepdim_var_xpu (__main__.TestReductionsXPU)
Tests that dim=None, when keepdim=True, reduces all dimensions to size 1. ... skipped 'Skipped!'
test_dim_none_mean_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_nanmean_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... ok
test_dim_none_nansum_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_prod_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_std_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_sum_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_none_var_xpu (__main__.TestReductionsXPU)
Tests that dim=None reduces all dimensions. ... skipped 'Skipped!'
test_dim_offbounds_all_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_amax_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_amin_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_any_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_argmax_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_argmin_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... skipped "not_implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_offbounds_mean_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_nanmean_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_nansum_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_prod_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_std_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_sum_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_offbounds_var_xpu (__main__.TestReductionsXPU)
Tests that passing an off-bounds dim throws ... ok
test_dim_reduction_less_than_64_xpu (__main__.TestReductionsXPU) ... ok
test_dim_reduction_xpu_bfloat16 (__main__.TestReductionsXPU) ... FAIL
test_dim_reduction_xpu_float16 (__main__.TestReductionsXPU) ... FAIL
test_dim_reduction_xpu_float32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::median.dim_values' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::median.dim_values' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_reduction_xpu_float64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::median.dim_values' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::median.dim_values' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_reduction_xpu_int16 (__main__.TestReductionsXPU) ... FAIL
test_dim_reduction_xpu_int32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::median.dim_values' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::median.dim_values' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_reduction_xpu_int64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::median.dim_values' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::median.dim_values' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_reduction_xpu_int8 (__main__.TestReductionsXPU) ... FAIL
test_dim_single_all_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_amax_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_amin_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_any_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_argmax_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_argmin_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dim_single_keepdim_all_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_amax_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_amin_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_any_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_argmax_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_argmin_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... skipped 'Skipped!'
test_dim_single_keepdim_mean_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_nanmean_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_nansum_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_prod_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_std_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_sum_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_keepdim_var_xpu (__main__.TestReductionsXPU)
Tests that dim=i, when keepdim=True, reduces dimension i to size 1. ... ok
test_dim_single_mean_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_nanmean_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_nansum_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_prod_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_std_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_sum_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_dim_single_var_xpu (__main__.TestReductionsXPU)
Tests that dim=i reduces dimension i. ... ok
test_empty_tensor_empty_slice_all_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_amax_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_amin_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_any_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_argmax_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_argmin_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_empty_tensor_empty_slice_mean_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_nanmean_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_nansum_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_prod_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_std_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_sum_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_empty_slice_var_xpu (__main__.TestReductionsXPU)
Tests for consistent behavior when reducing over an empty slice. ... ok
test_empty_tensor_nonempty_slice_all_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_amax_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ERROR
test_empty_tensor_nonempty_slice_amin_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ERROR
test_empty_tensor_nonempty_slice_any_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_argmax_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_argmin_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_count_nonzero_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_empty_tensor_nonempty_slice_mean_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_nanmean_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_nansum_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_prod_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_std_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_sum_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_empty_tensor_nonempty_slice_var_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an ... ok
test_histc_xpu (__main__.TestReductionsXPU) ... FAIL
test_histogram_error_handling_xpu_float32 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_histogram_error_handling_xpu_float64 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_histogram_xpu_float32 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_histogram_xpu_float64 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_logsumexp_dim_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_logsumexp_xpu (__main__.TestReductionsXPU) ... ok
test_max_elementwise_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_max_mixed_devices_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_max_with_inf_xpu_bfloat16 (__main__.TestReductionsXPU) ... ok
test_max_with_inf_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_max_with_inf_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_max_with_inf_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_max_xpu_bool (__main__.TestReductionsXPU) ... ok
test_max_xpu_float16 (__main__.TestReductionsXPU) ... FAIL
test_max_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_max_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_mean_dim_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_median_corner_cases_xpu (__main__.TestReductionsXPU) ... ERROR
test_median_nan_values_xpu_float16 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_nan_values_xpu_float32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_nan_values_xpu_float64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_real_values_xpu_float16 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_real_values_xpu_float32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_real_values_xpu_float64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_real_values_xpu_int32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_median_real_values_xpu_int64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_min_elementwise_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_min_max_nan_xpu (__main__.TestReductionsXPU) ... ok
test_min_mixed_devices_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_min_with_inf_xpu_bfloat16 (__main__.TestReductionsXPU) ... ok
test_min_with_inf_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_min_with_inf_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_min_with_inf_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_min_xpu_bool (__main__.TestReductionsXPU) ... FAIL
test_min_xpu_float16 (__main__.TestReductionsXPU) ... FAIL
test_min_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_min_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_minmax_illegal_dtype_xpu (__main__.TestReductionsXPU) ... ok
test_mode_large_xpu (__main__.TestReductionsXPU) ... skipped 'not ready on XPU'
test_mode_wrong_device_xpu (__main__.TestReductionsXPU) ... ok
test_mode_wrong_dtype_xpu (__main__.TestReductionsXPU) ... ok
test_mode_xpu (__main__.TestReductionsXPU) ... FAIL
test_nansum_complex_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_nansum_complex_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_nansum_out_dtype_xpu (__main__.TestReductionsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ok
test_nansum_vs_numpy_xpu_float16 (__main__.TestReductionsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ok
test_nansum_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_nansum_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_nansum_vs_numpy_xpu_int16 (__main__.TestReductionsXPU) ... ok
test_nansum_vs_numpy_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_nansum_vs_numpy_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_nansum_vs_numpy_xpu_int8 (__main__.TestReductionsXPU) ... ok
test_nansum_vs_numpy_xpu_uint8 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_bfloat16 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_int16 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_int8 (__main__.TestReductionsXPU) ... ok
test_nansum_xpu_uint8 (__main__.TestReductionsXPU) ... ok
test_noncontiguous_all_all_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_all_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_all_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_all_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_all_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_all_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_amin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_any_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_any_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_any_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_any_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_any_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_any_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_argmin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_count_nonzero_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_all_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_count_nonzero_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_all_count_nonzero_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_all_count_nonzero_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_all_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_all_mean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_mean_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_mean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'Skipped!'
test_noncontiguous_all_mean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_nanmean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_nanmean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'Skipped!'
test_noncontiguous_all_nanmean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_nansum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_nansum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'Skipped!'
test_noncontiguous_all_nansum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_nansum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_nansum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_prod_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_prod_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_prod_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_prod_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_std_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_std_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_std_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_sum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_sum_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_sum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'Skipped!'
test_noncontiguous_all_sum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_sum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_sum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_var_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_all_var_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_all_var_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing all dimensions of a noncontiguous tensor. ... ok
test_noncontiguous_expanded_all_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_all_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_all_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_all_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_all_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_all_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_amin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_any_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_any_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_any_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_any_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_any_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_any_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_argmin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_count_nonzero_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_expanded_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_count_nonzero_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_expanded_count_nonzero_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_expanded_count_nonzero_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_expanded_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_expanded_mean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_mean_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_mean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_mean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nanmean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nanmean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nanmean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nansum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nansum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nansum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nansum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_nansum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_prod_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_prod_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_prod_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_prod_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_std_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_std_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_std_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_sum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_sum_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_sum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_sum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_sum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_sum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_var_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... skipped 'dtype not support on XPU'
test_noncontiguous_expanded_var_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_expanded_var_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a tensor with expanded singleton dimensions. ... ok
test_noncontiguous_innermost_all_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_all_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_all_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_all_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_all_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_all_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_amin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_any_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_any_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_any_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_any_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_any_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_any_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_argmin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_count_nonzero_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_innermost_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_count_nonzero_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_innermost_count_nonzero_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_innermost_count_nonzero_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_innermost_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_innermost_mean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_mean_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_mean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_mean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nanmean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nanmean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nanmean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nansum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nansum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nansum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nansum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_nansum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_prod_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_prod_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_prod_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_prod_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_std_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_std_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_std_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_sum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_sum_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_sum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_sum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_sum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_sum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_var_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_innermost_var_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_innermost_var_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous innermost dimension. ... ok
test_noncontiguous_outermost_all_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_all_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_all_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_all_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_all_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_all_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_amin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_any_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_any_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_any_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_any_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_any_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_any_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_argmin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_count_nonzero_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_outermost_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_count_nonzero_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_outermost_count_nonzero_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_outermost_count_nonzero_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_outermost_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_outermost_mean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_mean_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_mean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_mean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nanmean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nanmean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nanmean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nansum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nansum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nansum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nansum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_nansum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_prod_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_prod_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_prod_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_prod_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_std_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_std_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_std_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_sum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_sum_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_sum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_sum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_sum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_sum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_var_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... skipped 'dtype not support on XPU'
test_noncontiguous_outermost_var_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_outermost_var_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing along noncontiguous outermost dimension. ... ok
test_noncontiguous_transposed_all_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_all_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_all_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_all_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_all_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_all_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_amin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_any_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_any_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_any_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_any_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_any_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_any_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmax_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmax_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmax_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmax_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmin_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmin_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmin_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_argmin_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_count_nonzero_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_transposed_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_count_nonzero_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_transposed_count_nonzero_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_transposed_count_nonzero_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_transposed_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_noncontiguous_transposed_mean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_mean_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_mean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_mean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nanmean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nanmean_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nanmean_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nansum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nansum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nansum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nansum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_nansum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_prod_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_prod_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_prod_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_prod_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_std_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_std_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_std_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_sum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_sum_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_sum_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_sum_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_sum_xpu_int64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_sum_xpu_uint8 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_var_xpu_complex64 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... skipped 'dtype not support on XPU'
test_noncontiguous_transposed_var_xpu_float16 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_noncontiguous_transposed_var_xpu_float32 (__main__.TestReductionsXPU)
Tests reducing a transposed tensor. ... ok
test_numpy_named_args_xpu (__main__.TestReductionsXPU) ... ok
test_prod_bool_xpu (__main__.TestReductionsXPU) ... ERROR
test_prod_gpu_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_prod_gpu_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_prod_integer_upcast_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_prod_xpu_float32 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_quantile_backward_xpu (__main__.TestReductionsXPU) ... FAIL
test_quantile_error_xpu (__main__.TestReductionsXPU) ... ok
test_quantile_xpu_float32 (__main__.TestReductionsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py:2330: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.
Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they. (Deprecated NumPy 1.22)
  expected = numpy_op(a.cpu().numpy(), q.cpu().numpy(), dim,
ERROR
test_quantile_xpu_float64 (__main__.TestReductionsXPU) ... ERROR
test_reduction_empty_any_all_xpu (__main__.TestReductionsXPU) ... ok
test_reduction_split_xpu (__main__.TestReductionsXPU) ... ok
test_reduction_vectorize_along_input_corner_xpu_bfloat16 (__main__.TestReductionsXPU) ... FAIL
test_reduction_vectorize_along_input_corner_xpu_float16 (__main__.TestReductionsXPU) ... FAIL
test_reduction_vectorize_along_input_corner_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_reduction_vectorize_along_input_corner_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_reduction_vectorize_along_output_xpu_bfloat16 (__main__.TestReductionsXPU) ... ok
test_reduction_vectorize_along_output_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_reduction_vectorize_along_output_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_reduction_vectorize_along_output_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_repeated_dim_xpu (__main__.TestReductionsXPU) ... ok
test_result_dtype_all_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_all_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_all_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_all_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_amax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amax_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_amin_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_any_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_any_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_any_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_argmax_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmax_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_argmin_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_count_nonzero_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_count_nonzero_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_count_nonzero_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_count_nonzero_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_result_dtype_mean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_mean_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_mean_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_mean_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_mean_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_mean_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nanmean_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nanmean_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nanmean_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nanmean_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_nansum_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_prod_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_prod_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_prod_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_std_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_std_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_std_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_std_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_std_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_sum_xpu_bfloat16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_bool (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_sum_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_sum_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_int16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_int32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_int64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_int8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_sum_xpu_uint8 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... ok
test_result_dtype_var_xpu_complex128 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_var_xpu_complex64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'dtype not support on XPU'
test_result_dtype_var_xpu_float16 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_var_xpu_float32 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_result_dtype_var_xpu_float64 (__main__.TestReductionsXPU)
Tests that the result has the correct dtype ... skipped 'Skipped!'
test_std_correction_vs_numpy_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_std_correction_vs_numpy_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_std_correction_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... FAIL
test_std_correction_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... FAIL
test_std_dim_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_std_mean_all_dims_xpu (__main__.TestReductionsXPU) ... ok
test_std_mean_correction_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_std_mean_correction_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_std_mean_correction_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_std_mean_correction_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_std_mean_some_dims_xpu (__main__.TestReductionsXPU) ... ok
test_std_mean_xpu (__main__.TestReductionsXPU) ... ok
test_std_vs_numpy_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_std_vs_numpy_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_std_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::std.correction_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::std.correction_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_std_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::std.correction_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::std.correction_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sum_all_xpu_bool (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_sum_all_xpu_float64 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_sum_cpu_device_mismatch_xpu (__main__.TestReductionsXPU) ... FAIL
test_sum_dim_reduction_uint8_overflow_xpu (__main__.TestReductionsXPU) ... ok
test_sum_dim_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_sum_integer_upcast_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_sum_noncontig_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_sum_out_xpu_float64 (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_sum_parallel_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_sum_vs_numpy_xpu_float16 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_int16 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_int32 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_int64 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_int8 (__main__.TestReductionsXPU) ... ok
test_sum_vs_numpy_xpu_uint8 (__main__.TestReductionsXPU) ... ok
test_tensor_compare_ops_argmax_argmix_kthvalue_dim_empty_xpu (__main__.TestReductionsXPU) ... ERROR
test_tensor_compare_ops_empty_xpu (__main__.TestReductionsXPU) ... ERROR
test_tensor_reduce_ops_empty_xpu (__main__.TestReductionsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/_methods.py:181: RuntimeWarning: invalid value encountered in true_divide
  ret = um.true_divide(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3757: RuntimeWarning: Degrees of freedom <= 0 for slice
  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide
  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/_methods.py:253: RuntimeWarning: invalid value encountered in true_divide
  ret = um.true_divide(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
ok
test_var_correction_vs_numpy_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_var_correction_vs_numpy_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_var_correction_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... FAIL
test_var_correction_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... FAIL
test_var_dim_xpu (__main__.TestReductionsXPU) ... skipped 'Only runs on cpu'
test_var_large_input_xpu (__main__.TestReductionsXPU) ... ok
test_var_mean_all_dims_xpu (__main__.TestReductionsXPU) ... ok
test_var_mean_correction_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_var_mean_correction_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_var_mean_correction_xpu_float32 (__main__.TestReductionsXPU) ... ok
test_var_mean_correction_xpu_float64 (__main__.TestReductionsXPU) ... ok
test_var_mean_some_dims_xpu (__main__.TestReductionsXPU) ... ok
test_var_mean_xpu (__main__.TestReductionsXPU) ... ok
test_var_stability2_xpu (__main__.TestReductionsXPU) ... ok
test_var_stability_xpu (__main__.TestReductionsXPU) ... ok
test_var_unbiased_xpu (__main__.TestReductionsXPU) ... ok
test_var_vs_numpy_xpu_complex128 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_var_vs_numpy_xpu_complex64 (__main__.TestReductionsXPU) ... skipped 'dtype not support on XPU'
test_var_vs_numpy_xpu_float32 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::var.correction_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::var.correction_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_var_vs_numpy_xpu_float64 (__main__.TestReductionsXPU) ... skipped "not implemented: Could not run 'aten::var.correction_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::var.correction_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_var_xpu (__main__.TestReductionsXPU) ... ok

======================================================================
ERROR: test_empty_tensor_nonempty_slice_amax_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 301, in test_empty_tensor_nonempty_slice
    result = op(t, *args, dim=dim, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 626, in __call__
    return self.op(*args, **kwargs)
RuntimeError: iter.numel() > 0 && iter.ntensors() - iter.noutputs() == 1 && iter.noutputs() >= 1INTERNAL ASSERT FAILED at "/home/gta/xunsongh/ipex-gpu/csrc/aten/operators/Reduce.h":1017, please report a bug to PyTorch. 

======================================================================
ERROR: test_empty_tensor_nonempty_slice_amin_xpu (__main__.TestReductionsXPU)
Tests that reducing a nonempty slice of an empty tensor returns an
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 301, in test_empty_tensor_nonempty_slice
    result = op(t, *args, dim=dim, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 626, in __call__
    return self.op(*args, **kwargs)
RuntimeError: iter.numel() > 0 && iter.ntensors() - iter.noutputs() == 1 && iter.noutputs() >= 1INTERNAL ASSERT FAILED at "/home/gta/xunsongh/ipex-gpu/csrc/aten/operators/Reduce.h":1017, please report a bug to PyTorch. 

======================================================================
ERROR: test_median_corner_cases_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2263, in test_median_corner_cases
    check(torch.median, [], [], nan)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2250, in check
    res = op(t, *args)
RuntimeError: median cannot be called with empty tensor

======================================================================
ERROR: test_prod_bool_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1250, in test_prod_bool
    result = torch.prod(torch.tensor(val, device=device), dtype=torch.bool).item()
RuntimeError: "prod" not implemented for 'Bool'

======================================================================
ERROR: test_quantile_xpu_float32 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2332, in test_quantile
    self.assertEqual(result.cpu(), torch.from_numpy(np.array(expected)).type(result.type()))
ValueError: invalid type: 'torch.xpu.FloatTensor'

======================================================================
ERROR: test_quantile_xpu_float64 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2332, in test_quantile
    self.assertEqual(result.cpu(), torch.from_numpy(np.array(expected)).type(result.type()))
ValueError: invalid type: 'torch.xpu.DoubleTensor'

======================================================================
ERROR: test_tensor_compare_ops_argmax_argmix_kthvalue_dim_empty_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2991, in test_tensor_compare_ops_argmax_argmix_kthvalue_dim_empty
    self.assertEqual(torch.empty((2, 0), device=device, **dtype), fn(master_input, dim=2), msg=error_msg)
RuntimeError: cannot perform reduction function argmax on a tensor with no elements because the operation does not have an identity

======================================================================
ERROR: test_tensor_compare_ops_empty_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2952, in test_tensor_compare_ops_empty
    self.assertEqual(torch.empty((2, 0), device=device), fn(master_input, dim=2), msg=error_msg)
RuntimeError: iter.numel() > 0 && iter.ntensors() - iter.noutputs() == 1 && iter.noutputs() >= 1INTERNAL ASSERT FAILED at "/home/gta/xunsongh/ipex-gpu/csrc/aten/operators/Reduce.h":1017, please report a bug to PyTorch. 

======================================================================
FAIL: test_all_any_vs_numpy_xpu_float16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1660, in test_all_any_vs_numpy
    _test_all_any(x)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1604, in _test_all_any
    self.compare_with_numpy(torch.all, np.all, x)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1798, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1959, in assertEqual
    super().assertEqual(x, y, msg=msg)
AssertionError: True != False

======================================================================
FAIL: test_all_any_vs_numpy_xpu_float32 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1660, in test_all_any_vs_numpy
    _test_all_any(x)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1604, in _test_all_any
    self.compare_with_numpy(torch.all, np.all, x)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1798, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1959, in assertEqual
    super().assertEqual(x, y, msg=msg)
AssertionError: True != False

======================================================================
FAIL: test_all_any_vs_numpy_xpu_float64 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1660, in test_all_any_vs_numpy
    _test_all_any(x)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1604, in _test_all_any
    self.compare_with_numpy(torch.all, np.all, x)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1798, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1959, in assertEqual
    super().assertEqual(x, y, msg=msg)
AssertionError: True != False

======================================================================
FAIL: test_argminmax_large_axis_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2156, in test_argminmax_large_axis
    self.assertEqual(x.argmax(0), x.shape[0] - 1)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1788, in assertEqual
    self.assertEqual(x.item(), y, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 0 and 2147483647 gives a difference of 2147483647, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_argminmax_multiple_xpu_float16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1548, in test_argminmax_multiple
    self.compare_with_numpy(torch.argmin, np.argmin, x, device=None, dtype=None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1791, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 1 and 4 gives a difference of 3, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_argminmax_multiple_xpu_int16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1547, in test_argminmax_multiple
    self.compare_with_numpy(torch.argmax, np.argmax, x, device=None, dtype=None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1791, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 5 and 2 gives a difference of 3, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_argminmax_multiple_xpu_int8 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1547, in test_argminmax_multiple
    self.compare_with_numpy(torch.argmax, np.argmax, x, device=None, dtype=None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1791, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 5 and 4 gives a difference of 1, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_argminmax_multiple_xpu_uint8 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1547, in test_argminmax_multiple
    self.compare_with_numpy(torch.argmax, np.argmax, x, device=None, dtype=None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1791, in assertEqual
    self.assertEqual(x, y.item(), atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 5 and 0 gives a difference of 5, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_dim_reduction_xpu_bfloat16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1913, in test_dim_reduction
    self.assertEqual(x.min(1), (torch.tensor([-1, 3], dtype=dtype),
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1945, in assertEqual
    self.assertEqual(x_, y_, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0.016 and atol=0.01, found 1 element(s) (out of 2) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 3.0 (0.0 vs. 3.0), which occurred at index 1.

======================================================================
FAIL: test_dim_reduction_xpu_float16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1913, in test_dim_reduction
    self.assertEqual(x.min(1), (torch.tensor([-1, 3], dtype=dtype),
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1945, in assertEqual
    self.assertEqual(x_, y_, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0.001 and atol=0.01, found 1 element(s) (out of 2) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 3.0 (0.0 vs. 3.0), which occurred at index 1.

======================================================================
FAIL: test_dim_reduction_xpu_int16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1913, in test_dim_reduction
    self.assertEqual(x.min(1), (torch.tensor([-1, 3], dtype=dtype),
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1945, in assertEqual
    self.assertEqual(x_, y_, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!Found 1 different element(s) (out of 2), with the greatest difference of 3 (0 vs. 3) occuring at index 1.

======================================================================
FAIL: test_dim_reduction_xpu_int8 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1913, in test_dim_reduction
    self.assertEqual(x.min(1), (torch.tensor([-1, 3], dtype=dtype),
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1945, in assertEqual
    self.assertEqual(x_, y_, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!Found 1 different element(s) (out of 2), with the greatest difference of 3 (0 vs. 3) occuring at index 1.

======================================================================
FAIL: test_histc_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
RuntimeError: bins should be > 0, but is -1 instead

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2656, in test_histc
    torch.histc(torch.tensor([1], dtype=torch.float, device=device), bins=-1)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1282, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: "bins must be > 0" does not match "bins should be > 0, but is -1 instead"

======================================================================
FAIL: test_max_xpu_float16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1008, in test_max
    self._test_minmax_helper(torch.max, np.amax, device, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 982, in _test_minmax_helper
    self.compare_with_numpy(lambda x: get_values(torchfn(x, d, False)), lambda x: reffn(x, d, keepdims=False), xinp)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0.001 and atol=0.001, found 2 element(s) (out of 5) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.1181640625 (1.1181640625 vs. 0.0), which occurred at index 4.

======================================================================
FAIL: test_min_xpu_bool (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1014, in test_min
    self._test_minmax_helper(torch.min, np.amin, device, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 982, in _test_minmax_helper
    self.compare_with_numpy(lambda x: get_values(torchfn(x, d, False)), lambda x: reffn(x, d, keepdims=False), xinp)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!Found 5 different element(s) (out of 5), with the greatest difference of 1 (1 vs. 0) occuring at index 0.

======================================================================
FAIL: test_min_xpu_float16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1014, in test_min
    self._test_minmax_helper(torch.min, np.amin, device, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 982, in _test_minmax_helper
    self.compare_with_numpy(lambda x: get_values(torchfn(x, d, False)), lambda x: reffn(x, d, keepdims=False), xinp)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1660, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=0.001 and atol=0.001, found 2 element(s) (out of 5) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.263671875 (-1.12109375 vs. -0.857421875), which occurred at index 3.

======================================================================
FAIL: test_mode_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 813, in test_mode
    self.assertEqual(res1val, res2val, atol=0, rtol=0)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!Attempted to compare equality of tensors with different sizes. Got sizes torch.Size([10]) and torch.Size([0]).

======================================================================
FAIL: test_quantile_backward_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2350, in test_quantile_backward
    check([[1., 2], [2, 1]], 0., 0, [[1, 0], [0, 1]])
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2344, in check
    self.assertEqual(t.grad, expected_grad)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1904, in assertEqual
    self.assertEqual(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1945, in assertEqual
    self.assertEqual(x_, y_, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1945, in assertEqual
    self.assertEqual(x_, y_, atol=atol, rtol=rtol, msg=msg,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 1.0 and 0 gives a difference of 1.0, but the allowed difference with rtol=1.3e-06 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_reduction_vectorize_along_input_corner_xpu_bfloat16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2059, in test_reduction_vectorize_along_input_corner
    self.assertEqual(x.argmax().item(), i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 0 and 1 gives a difference of 1, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_reduction_vectorize_along_input_corner_xpu_float16 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2059, in test_reduction_vectorize_along_input_corner
    self.assertEqual(x.argmax().item(), i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1957, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Scalars failed to compare as equal! Comparing 0 and 1 gives a difference of 1, but the allowed difference with rtol=0 and atol=0.001 is only 0.001!

======================================================================
FAIL: test_std_correction_vs_numpy_xpu_float32 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2569, in test_std_correction_vs_numpy
    self.assertEqual(torch_res, numpy_res)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1899, in assertEqual
    self.assertEqual(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=1.3e-06 and atol=0.001, found 1 element(s) (out of 1) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.06012392044067383 (5.240574836730957 vs. 5.300698757171631), which occurred at index 0.

======================================================================
FAIL: test_std_correction_vs_numpy_xpu_float64 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2569, in test_std_correction_vs_numpy
    self.assertEqual(torch_res, numpy_res)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1899, in assertEqual
    self.assertEqual(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=1e-07 and atol=0.001, found 1 element(s) (out of 1) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.05947027782660719 (5.1836572255314985 vs. 5.243127503358106), which occurred at index 0.

======================================================================
FAIL: test_sum_cpu_device_mismatch_xpu (__main__.TestReductionsXPU)
----------------------------------------------------------------------
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, xpu:0 and cpu!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 1786, in test_sum_cpu_device_mismatch
    torch.sum(x, dim=[0], dtype=torch.float32, out=y)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1282, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: "Expected out tensor to have device xpu:0, but got cpu instead" does not match "Expected all tensors to be on the same device, but found at least two devices, xpu:0 and cpu!"

======================================================================
FAIL: test_var_correction_vs_numpy_xpu_float32 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2535, in test_var_correction_vs_numpy
    self.assertEqual(torch_res, numpy_res)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1899, in assertEqual
    self.assertEqual(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=1.3e-06 and atol=0.001, found 1 element(s) (out of 1) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.6336708068847656 (27.459009170532227 vs. 28.092679977416992), which occurred at index 0.

======================================================================
FAIL: test_var_correction_vs_numpy_xpu_float64 (__main__.TestReductionsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_reductions.py", line 2535, in test_var_correction_vs_numpy
    self.assertEqual(torch_res, numpy_res)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1899, in assertEqual
    self.assertEqual(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!With rtol=1e-07 and atol=0.001, found 1 element(s) (out of 1) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.5581278641106238 (24.18554446091787 vs. 24.743672325028495), which occurred at index 0.

----------------------------------------------------------------------
Ran 808 tests in 75.702s

FAILED (failures=25, errors=8, skipped=208)
Raised CalledProcessError: return code 1.