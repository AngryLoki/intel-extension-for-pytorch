test_Sparse_to_Sparse_copy__xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_Sparse_to_Sparse_copy__xpu_float64 (__main__.TestSparseXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py:1598: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /home/gta/xunsongh/ipex-gpu/csrc/aten/operators/BinaryOps_divfloor.cpp:86.)
  x = x.coalesce()
skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_Sparse_to_Sparse_copy_multi_gpu_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_Sparse_to_Sparse_copy_multi_gpu_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_add_dense_sparse_mismatch_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_add_dense_sparse_mismatch_xpu_float64 (__main__.TestSparseXPU) ... skipped "not_implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_add_noncontiguous_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_add_noncontiguous_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_add_sub_nnz_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_add_sub_nnz_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_add_zeros_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_add_zeros_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_allow_tensor_metadata_change_xpu (__main__.TestSparseXPU) ... FAIL
test_any_xpu (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::any.all_out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::any.all_out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_float32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_int16 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_int32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_int64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_int8 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_asin_arcsin_xpu_uint8 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_assign_xpu_float64 (__main__.TestSparseXPU) ... ok
test_basic_ops_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_basic_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_basic_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::indices' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::indices' is only available for these backends: [SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_bmm_deterministic_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::bmm' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::bmm' is only available for these backends: [CPU, XPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: registered at ../aten/src/ATen/autocast_mode.cpp:309 [kernel]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1020 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_bmm_windows_error_xpu_float64 (__main__.TestSparseXPU) ... skipped "not_implemented: Could not run 'aten::bmm' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::bmm' is only available for these backends: [CPU, XPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: registered at ../aten/src/ATen/autocast_mode.cpp:309 [kernel]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1020 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_bmm_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::bmm' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::bmm' is only available for these backends: [CPU, XPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: registered at ../aten/src/ATen/autocast_mode.cpp:309 [kernel]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1020 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_bmm_xpu_version_error_xpu_float64 (__main__.TestSparseXPU) ... skipped "not_implemented: Could not run 'aten::bmm' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::bmm' is only available for these backends: [CPU, XPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: registered at ../aten/src/ATen/autocast_mode.cpp:309 [kernel]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1020 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_cat_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_cat_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_change_tensor_metadata_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_change_tensor_metadata_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_clone_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_clone_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_coalesce_reference_cycle_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_coalesce_transpose_mm_xpu_float64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_coalesce_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_coalesce_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_constructor_device_legacy_xpu (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_contig_hybrid_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_contig_hybrid_xpu_float64 (__main__.TestSparseXPU) ... ok
test_contig_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_contig_xpu_float64 (__main__.TestSparseXPU) ... ok
test_cpu_sparse_dense_mul_xpu (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_ctor_large_sizes_xpu_float64 (__main__.TestSparseXPU) ... ok
test_ctor_size_checks_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_ctor_size_checks_xpu_float64 (__main__.TestSparseXPU) ... ok
test_div_by_sparse_error_xpu (__main__.TestSparseXPU) ... skipped "not_implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_div_rounding_mode_xpu_float32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_div_rounding_mode_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dsmm_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::mm.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::mm.out' is only available for these backends: [CPU, XPU, Meta, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_dtypes_xpu (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_empty_full_xpu (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_empty_like_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_empty_like_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_factory_copy_xpu (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_default_xpu (__main__.TestSparseXPU) ... ok
test_factory_dense_dim_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_dense_dim_xpu_float64 (__main__.TestSparseXPU) ... ok
test_factory_device_type_inference_xpu (__main__.TestSparseXPU) ... ok
test_factory_empty_indices_xpu (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::_sparse_coo_tensor_with_dims' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims' is only available for these backends: [SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: registered at aten/src/ATen/RegisterBackendSelect.cpp:665 [kernel]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_factory_nnz_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_nnz_xpu_float64 (__main__.TestSparseXPU) ... ok
test_factory_nnz_zero_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_nnz_zero_xpu_float64 (__main__.TestSparseXPU) ... ok
test_factory_size_check_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_size_check_xpu_float64 (__main__.TestSparseXPU) ... ok
test_factory_type_inference_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_type_inference_xpu_complex64 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_type_inference_xpu_float16 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_type_inference_xpu_float32 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_type_inference_xpu_float64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_type_inference_xpu_int64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_xpu_complex64 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_factory_xpu_float16 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_xpu_float32 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_factory_xpu_float64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_floor_divide_by_sparse_error_xpu (__main__.TestSparseXPU) ... skipped "not_implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_hsmm_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::hspmm' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hspmm' is only available for these backends: [SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_index_select_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_index_select_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::index_select' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_select' is only available for these backends: [CPU, XPU, QuantizedCPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_is_nonzero_xpu (__main__.TestSparseXPU) ... ok
test_is_sparse_xpu (__main__.TestSparseXPU) ... ok
test_isnan_xpu (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::isnan' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::isnan' is only available for these backends: [CPU, XPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_legacy_constructor_xpu (__main__.TestSparseXPU) ... ok
test_legacy_new_xpu (__main__.TestSparseXPU) ... ok
test_log1p_xpu_float32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_log1p_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_log1p_xpu_int16 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_log1p_xpu_int32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_log1p_xpu_int64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_log1p_xpu_int8 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_log1p_xpu_uint8 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_mm_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_mm_xpu_float64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_mv_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::mv' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::mv' is only available for these backends: [CPU, XPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: registered at ../aten/src/ATen/autocast_mode.cpp:309 [kernel]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1020 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_narrow_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_narrow_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_neg_negative_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_neg_negative_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_new_device_multi_gpu_xpu (__main__.TestSparseXPU) ... ERROR
test_new_device_single_gpu_xpu (__main__.TestSparseXPU) ... ERROR
test_new_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_new_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_norm_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_norm_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::norm.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::norm.out' is only available for these backends: [CPU, XPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_pickle_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_print_coalesced_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_print_uncoalesced_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_resize_as_xpu (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::zero_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::zero_' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1020 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_resize_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_resize_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_saddmm_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_saddmm_xpu_float64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_same_gpu_xpu (__main__.TestSparseXPU) ... ERROR
test_scalar_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_scalar_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_select_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_select_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_shared_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_shared_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_softmax_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_spadd_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_add_coalesce_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_add_coalesce_xpu_complex64 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_add_coalesce_xpu_float32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_add_coalesce_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_add_out_bfloat16_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_add_out_bfloat16_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_addmm_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_addmm_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_bool_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_bool_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_mask_hybrid_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_mask_hybrid_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_mask_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sparse_mask_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::add.out' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, XPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_matmul_xpu_float64 (__main__.TestSparseXPU)
This function test `torch.sparse.mm` when both the mat1 and mat2 are sparse tensors. ... /home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py:3247: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:198.)
  return torch.sparse_coo_tensor([result.row, result.col], result.data, result.shape,
skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_mm_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_sum_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_sparse_to_numpy_xpu (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_sspaddmm_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_sspaddmm_xpu_float64 (__main__.TestSparseXPU) ... skipped 'Only runs on cpu'
test_storage_not_null_xpu (__main__.TestSparseXPU) ... ERROR
test_t_empty_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_t_empty_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_dense_hybrid_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_to_dense_hybrid_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_dense_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_to_dense_xpu_complex64 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_to_dense_xpu_float16 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_dense_xpu_float32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_dense_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_sparse_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_to_sparse_xpu_complex64 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_to_sparse_xpu_float16 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_sparse_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_to_sparse_xpu_int32 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_transpose_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_transpose_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_unsqueeze_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_unsqueeze_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMkldnnCPU: registered at aten/src/ATen/RegisterMkldnnCPU.cpp:595 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_xpu_empty_xpu (__main__.TestSparseXPU) ... ERROR
test_zeros_like_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_zeros_like_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_zeros_xpu_complex128 (__main__.TestSparseXPU) ... skipped 'dtype not support on XPU'
test_zeros_xpu_float64 (__main__.TestSparseXPU) ... skipped "not implemented: Could not run 'aten::sparse_resize_and_clear_' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_resize_and_clear_' is only available for these backends: [Meta, SparseCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"

======================================================================
ERROR: test_new_device_multi_gpu_xpu (__main__.TestSparseXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2273, in test_new_device_multi_gpu
    self._test_new_device((), 1)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2256, in _test_new_device
    x = torch.xpu.sparse.DoubleTensor(*size)
AttributeError: module 'intel_extension_for_pytorch.xpu' has no attribute 'sparse'

======================================================================
ERROR: test_new_device_single_gpu_xpu (__main__.TestSparseXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2265, in test_new_device_single_gpu
    self._test_new_device((), 0)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2256, in _test_new_device
    x = torch.xpu.sparse.DoubleTensor(*size)
AttributeError: module 'intel_extension_for_pytorch.xpu' has no attribute 'sparse'

======================================================================
ERROR: test_same_gpu_xpu (__main__.TestSparseXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 905, in multi_fn
    return fn(slf, devices, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2230, in test_same_gpu
    x = self.sparse_tensor(i, v, torch.Size([3]), device=1)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 46, in sparse_tensor_factory
    return torch.sparse_coo_tensor(*args, **kwargs)
RuntimeError: PyTorch is not linked with support for cuda devices

======================================================================
ERROR: test_storage_not_null_xpu (__main__.TestSparseXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2212, in test_storage_not_null
    x = torch.xpu.sparse.FloatTensor(2)
AttributeError: module 'intel_extension_for_pytorch.xpu' has no attribute 'sparse'

======================================================================
ERROR: test_xpu_empty_xpu (__main__.TestSparseXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 558, in only_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 763, in test_xpu_empty
    x = torch.xpu.sparse.HalfTensor(2, 3, 4)
AttributeError: module 'intel_extension_for_pytorch.xpu' has no attribute 'sparse'

======================================================================
FAIL: test_allow_tensor_metadata_change_xpu (__main__.TestSparseXPU)
----------------------------------------------------------------------
RuntimeError: unsupported tensor layout: Sparse

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2786, in test_allow_tensor_metadata_change
    do_test(self.sparse_empty([3, 0], device=device).data)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_sparse.py", line 2767, in do_test
    t.mul_(t)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1282, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: "resize_and_clear_ is not allowed on a Tensor created from .data or .detach()" does not match "unsupported tensor layout: Sparse"

----------------------------------------------------------------------
Ran 169 tests in 8.192s

FAILED (failures=1, errors=5, skipped=148)
Raised CalledProcessError: return code 1.