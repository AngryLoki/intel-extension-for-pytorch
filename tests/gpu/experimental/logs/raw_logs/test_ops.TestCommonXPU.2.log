test_dtypes___getitem___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___radd___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rand___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rdiv___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rmatmul___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rmod___xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes___rmul___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___ror___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rpow___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rsub___xpu (__main__.TestCommonXPU) ... ok
test_dtypes___rxor___xpu (__main__.TestCommonXPU) ... ok
test_dtypes_abs_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_acos_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_acosh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_add_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_addbmm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_addcdiv_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_addcmul_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_addmm_decomposed_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_addmm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_addmv_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_addr_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_all_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_amax_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_amin_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_aminmax_xpu (__main__.TestCommonXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_dtypes_angle_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_any_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_argmax_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_argmin_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_asin_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_asinh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_atan2_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_atan_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_atanh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_baddbmm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_bitwise_and_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_bitwise_left_shift_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_bitwise_not_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_bitwise_right_shift_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_block_diag_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_bmm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_broadcast_tensors_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_broadcast_to_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_cat_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_cdist_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_ceil_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cholesky_inverse_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_cholesky_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_chunk_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_clamp_scalar_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_clamp_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_clone_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_complex_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_conj_physical_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_conj_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_contiguous_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_copysign_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_corrcoef_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cos_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cosh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_count_nonzero_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cov_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at  ../aten/src/ATen/native/Correlation.cpp:99.)
  return self.op(*args, **kwargs)
ok
test_dtypes_cross_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cummax_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cummin_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cumprod_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cumsum_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_cumulative_trapezoid_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_deg2rad_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_diag_embed_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_diag_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_diagonal_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_diff_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_digamma_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_dist_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_div_floor_rounding_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_div_no_rounding_mode_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_div_trunc_rounding_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_dot_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_dsplit_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_dstack_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_eig_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.
torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.
L, _ = torch.eig(A)
should be replaced with
L_complex = torch.linalg.eigvals(A)
and
L, V = torch.eig(A, eigenvectors=True)
should be replaced with
L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2882.)
  return self.op(*args, **kwargs)
ok
test_dtypes_einsum_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_eq_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_erf_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_erfc_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_erfinv_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_exp2_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_exp_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_expand_as_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_expand_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_expm1_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fft_fft_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fft_fftn_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fft_hfft_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fft_ifft_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fft_ifftn_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fft_ihfft_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fft_irfft_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fft_irfftn_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fft_rfft_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fft_rfftn_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fill__xpu (__main__.TestCommonXPU) ... ok
test_dtypes_flip_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fliplr_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_flipud_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_float_power_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_floor_divide_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /home/gta/xunsongh/ipex-gpu/csrc/aten/operators/BinaryOps_divfloor.cpp:102.)
  return self.op(*args, **kwargs)
FAIL
test_dtypes_floor_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fmax_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fmin_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_fmod_autodiffed_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_fmod_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_frac_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_frexp_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_gather_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_ge_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_geqrf_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_gradient_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:2367: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
  a = torch.tensor(coords, dtype=dtype, device=device)
ok
test_dtypes_gt_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_histogram_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_hsplit_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_hstack_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_hypot_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_i0_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_igamma_grad_other_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_igamma_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_igammac_grad_other_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_igammac_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_imag_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_index_add_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_index_copy_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_index_fill_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_index_put_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_index_select_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_inner_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_inverse_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_isin_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_kron_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_kthvalue_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_le_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_lerp_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_lgamma_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_cholesky_ex_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_cholesky_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_cond_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_det_singular_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_linalg_det_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_eig_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_eigh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_eigvals_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_eigvalsh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_householder_product_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_inv_ex_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_inv_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_lstsq_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_matrix_norm_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_matrix_power_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_matrix_rank_hermitian_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_matrix_rank_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_multi_dot_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_norm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_pinv_hermitian_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_pinv_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_qr_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_slogdet_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_solve_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_svd_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_linalg_svdvals_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_tensorinv_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_linalg_vector_norm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_log10_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_log1p_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_log2_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_log_softmax_dtype_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_log_softmax_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_log_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_logaddexp2_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_logaddexp_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_logcumsumexp_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_logdet_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_logical_not_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_logit_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_logsumexp_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_lt_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_lu_solve_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_lu_unpack_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_lu_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_masked_fill_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_masked_scatter_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_masked_select_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_matmul_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_matrix_exp_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_max_binary_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_max_reduction_no_dim_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_max_reduction_with_dim_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_maximum_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_mean_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_median_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_meshgrid_list_of_tensors_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_meshgrid_variadic_tensors_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_min_binary_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_min_reduction_no_dim_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_min_reduction_with_dim_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_minimum_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_mm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_mode_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_movedim_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_msort_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_mul_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_mv_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_mvlgamma_mvlgamma_p_1_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_mvlgamma_mvlgamma_p_3_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_mvlgamma_mvlgamma_p_5_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_nan_to_num_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nanmean_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_nanmedian_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nanquantile_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_nansum_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_narrow_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_ne_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_neg_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_nextafter_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_adaptive_avg_pool2d_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_avg_pool2d_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_conv2d_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:646.)
  return self.op(*args, **kwargs)
FAIL
test_dtypes_nn_functional_conv_transpose2d_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_cosine_similarity_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_dropout_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_gelu_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_grid_sample_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_hardshrink_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_hardswish_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_hardtanh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_interpolate_area_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
FAIL
test_dtypes_nn_functional_interpolate_bicubic_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
ok
test_dtypes_nn_functional_interpolate_bilinear_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
FAIL
test_dtypes_nn_functional_interpolate_linear_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
FAIL
test_dtypes_nn_functional_interpolate_nearest_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_interpolate_trilinear_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
FAIL
test_dtypes_nn_functional_layer_norm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_leaky_relu_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_logsigmoid_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_max_pool2d_xpu (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_dtypes_nn_functional_mse_loss_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_nll_loss_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_normalize_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_one_hot_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_nn_functional_pad_circular_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_nn_functional_pad_constant_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_nn_functional_pad_reflect_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_pad_replicate_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_relu6_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_relu_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_softplus_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_nn_functional_unfold_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_norm_fro_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_norm_inf_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_norm_nuc_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_norm_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_ormqr_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_outer_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_permute_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_pinverse_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_polar_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_polygamma_polygamma_n_0_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_polygamma_polygamma_n_1_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_polygamma_polygamma_n_2_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_polygamma_polygamma_n_3_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_polygamma_polygamma_n_4_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_positive_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_pow_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_prod_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_put_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_qr_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1931.)
  return self.op(*args, **kwargs)
FAIL
test_dtypes_quantile_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_rad2deg_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_ravel_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_real_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_reciprocal_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_remainder_autodiffed_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_remainder_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_renorm_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_repeat_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_reshape_as_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_reshape_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_resize__xpu (__main__.TestCommonXPU) ... ok
test_dtypes_resize_as__xpu (__main__.TestCommonXPU) ... ok
test_dtypes_resolve_conj_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_resolve_neg_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_roll_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_rot90_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_round_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_rsqrt_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_rsub_rsub_scalar_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_rsub_rsub_tensor_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_scatter_add_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_scatter_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_select_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_sgn_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_sigmoid_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_sign_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_signbit_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_sin_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_sinc_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_sinh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_softmax_with_dtype_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_softmax_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_solve_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:758.)
  return self.op(*args, **kwargs)
FAIL
test_dtypes_sort_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_entr_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_erfcx_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_i0e_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_i1_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_i1e_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_ndtr_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_ndtri_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_polygamma_special_polygamma_n_0_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_xlog1py_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_zeta_grad_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_special_zeta_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_split_list_args_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_split_with_sizes_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_split_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_sqrt_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_square_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_squeeze_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_stack_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_std_mean_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_std_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_sub_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_sum_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_svd_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_symeig_xpu (__main__.TestCommonXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:626: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2487.)
  return self.op(*args, **kwargs)
FAIL
test_dtypes_t_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_take_along_dim_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_take_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_tan_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_tanh_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_tensor_split_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_tensordot_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_tile_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_to_sparse_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_topk_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_trace_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_transpose_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_trapezoid_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_trapz_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_triangular_solve_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_tril_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_triu_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_true_divide_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_trunc_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_unfold_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_unsqueeze_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_var_mean_xpu (__main__.TestCommonXPU) ... skipped 'Skipped!'
test_dtypes_var_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_vdot_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_view_as_complex_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_view_as_real_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_view_as_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_view_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_vsplit_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_vstack_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_where_xpu (__main__.TestCommonXPU) ... ok
test_dtypes_xlogy_xpu (__main__.TestCommonXPU) ... FAIL
test_dtypes_zero__xpu (__main__.TestCommonXPU) ... ok
test_multiple_devices___getitem___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___getitem___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___radd___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___radd___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rand___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rdiv___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rdiv___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rmatmul___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rmod___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rmod___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rmul___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rmul___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___ror___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rpow___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rpow___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rsub___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rsub___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices___rxor___xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_abs_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_abs_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_acos_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_acos_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_acosh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_acosh_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_add_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addbmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addcdiv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addcmul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addcmul_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addmm_decomposed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addmv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_addr_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_all_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_all_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_amax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_amax_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_amin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_amin_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_aminmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_aminmax_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_angle_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_angle_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_any_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_any_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_argmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_argmax_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_argmin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_argmin_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_asin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_asin_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_asinh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_asinh_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_atan2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_atan2_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_atan_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_atan_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_atanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_atanh_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_baddbmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_bitwise_and_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_bitwise_left_shift_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_bitwise_not_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_bitwise_right_shift_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_block_diag_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_block_diag_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_bmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_bmm_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_broadcast_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_broadcast_tensors_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_broadcast_to_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_broadcast_to_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cat_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cat_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cdist_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ceil_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cholesky_inverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cholesky_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_chunk_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_chunk_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_clamp_scalar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_clamp_scalar_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_clamp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_clamp_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_clone_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_clone_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_complex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_conj_physical_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_conj_physical_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_conj_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_conj_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_contiguous_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_contiguous_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_copysign_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_copysign_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_corrcoef_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_corrcoef_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cos_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cos_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cosh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cosh_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_count_nonzero_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_count_nonzero_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cov_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cov_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cross_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cross_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cummax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cummin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cumprod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cumprod_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cumsum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cumulative_trapezoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_cumulative_trapezoid_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_deg2rad_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_deg2rad_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diag_embed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diag_embed_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diag_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diag_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diagonal_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diagonal_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diff_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_diff_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_digamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_digamma_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dist_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_div_floor_rounding_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_div_floor_rounding_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_div_no_rounding_mode_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_div_no_rounding_mode_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_div_trunc_rounding_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_div_trunc_rounding_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dot_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dsplit_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_dstack_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_eig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_einsum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_einsum_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_eq_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_eq_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_erf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_erf_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_erfc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_erfc_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_erfinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_erfinv_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_exp2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_exp2_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_exp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_exp_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_expand_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_expand_as_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_expand_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_expand_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_expm1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_expm1_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_fft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_fftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_hfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_ifft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_ifftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_ihfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_irfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_irfftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_rfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fft_rfftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fill__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fill__xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_flip_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_flip_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fliplr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fliplr_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_flipud_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_flipud_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_float_power_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_float_power_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_floor_divide_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_floor_divide_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_floor_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmax_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmin_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmod_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmod_autodiffed_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_fmod_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_frac_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_frexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_gather_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_gather_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ge_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ge_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_geqrf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_gradient_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_gradient_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_gt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_gt_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_hsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_hsplit_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_hstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_hstack_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_hypot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_i0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_i0_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_igamma_grad_other_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_igamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_igammac_grad_other_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_igammac_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_add_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_copy_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_copy_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_fill_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_fill_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_put_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_put_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_index_select_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_inner_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_inverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_isin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_kron_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_kron_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_kthvalue_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_kthvalue_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_le_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_le_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lerp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lgamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lgamma_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_cholesky_ex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_cholesky_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_cond_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_det_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_eig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_eigh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_eigvals_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_eigvalsh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_householder_product_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_inv_ex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_inv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_lstsq_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_matrix_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_matrix_power_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_matrix_rank_hermitian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_matrix_rank_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_multi_dot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_pinv_hermitian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_pinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_qr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_slogdet_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_svd_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_svdvals_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_tensorinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_linalg_vector_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log10_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log10_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log1p_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log1p_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log2_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log_softmax_dtype_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log_softmax_dtype_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log_softmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_log_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logaddexp2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logaddexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logcumsumexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logdet_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logical_not_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logical_not_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logit_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_logsumexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lt_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lu_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lu_unpack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_lu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_masked_fill_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_masked_fill_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_masked_scatter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_masked_scatter_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_masked_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_masked_select_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_matmul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_matrix_exp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_max_binary_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_max_binary_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_max_reduction_no_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_max_reduction_no_dim_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_max_reduction_with_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_max_reduction_with_dim_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_maximum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_maximum_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_median_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_median_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_meshgrid_list_of_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_meshgrid_list_of_tensors_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_meshgrid_variadic_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_meshgrid_variadic_tensors_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_min_binary_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_min_binary_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_min_reduction_no_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_min_reduction_no_dim_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_min_reduction_with_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_min_reduction_with_dim_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_minimum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_minimum_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mode_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mode_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_movedim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_movedim_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_msort_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_msort_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mul_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mv_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mvlgamma_mvlgamma_p_1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mvlgamma_mvlgamma_p_1_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mvlgamma_mvlgamma_p_3_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mvlgamma_mvlgamma_p_3_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mvlgamma_mvlgamma_p_5_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_mvlgamma_mvlgamma_p_5_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nan_to_num_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nan_to_num_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nanmean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nanmedian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nanmedian_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nanquantile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nansum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nansum_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_narrow_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_narrow_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ne_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ne_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_neg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_neg_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nextafter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_adaptive_avg_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_avg_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_conv2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_conv2d_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_conv_transpose2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_cosine_similarity_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_dropout_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_gelu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_grid_sample_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_hardshrink_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_hardswish_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_hardtanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_interpolate_area_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_interpolate_bicubic_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_interpolate_bilinear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_interpolate_linear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_interpolate_nearest_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_interpolate_trilinear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_layer_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_leaky_relu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_logsigmoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_max_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_mse_loss_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_nll_loss_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_normalize_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_one_hot_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_pad_circular_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_pad_circular_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_pad_constant_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_pad_constant_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_pad_reflect_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_pad_replicate_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_relu6_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_relu6_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_relu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_softplus_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_nn_functional_unfold_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_norm_fro_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_norm_inf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_norm_nuc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ormqr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_outer_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_outer_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_permute_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_permute_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_pinverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_0_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_1_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_2_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_3_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_3_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_4_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_polygamma_polygamma_n_4_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_positive_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_positive_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_pow_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_pow_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_prod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_prod_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_put_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_put_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_qr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_quantile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rad2deg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rad2deg_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ravel_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_ravel_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_reciprocal_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_reciprocal_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_remainder_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_remainder_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_renorm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_repeat_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_repeat_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_reshape_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_reshape_as_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_reshape_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_reshape_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resize__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resize__xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resize_as__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resize_as__xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resolve_conj_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resolve_conj_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resolve_neg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_resolve_neg_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_roll_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_roll_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rot90_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rot90_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_round_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rsqrt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rsqrt_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rsub_rsub_scalar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rsub_rsub_scalar_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rsub_rsub_tensor_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_rsub_rsub_tensor_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_scatter_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_scatter_add_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_scatter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_scatter_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_select_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sgn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sgn_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sigmoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sigmoid_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sign_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sign_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_signbit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_signbit_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sin_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sinc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sinc_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sinh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sinh_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_softmax_with_dtype_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_softmax_with_dtype_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_softmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sort_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sort_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_entr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_entr_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_erfcx_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_erfcx_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_i0e_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_i0e_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_i1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_i1_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_i1e_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_i1e_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_ndtr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_ndtr_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_ndtri_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_ndtri_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_polygamma_special_polygamma_n_0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_polygamma_special_polygamma_n_0_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_xlog1py_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_xlog1py_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_zeta_grad_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_zeta_grad_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_zeta_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_special_zeta_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_split_list_args_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_split_list_args_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_split_with_sizes_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_split_with_sizes_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_split_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_split_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sqrt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sqrt_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_square_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_square_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_squeeze_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_squeeze_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_stack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_stack_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_std_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_std_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sub_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sub_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_sum_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_svd_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_symeig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_t_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_t_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_take_along_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_take_along_dim_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_take_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_take_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tan_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tan_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tanh_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tensor_split_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tensor_split_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tensordot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tile_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_to_sparse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_to_sparse_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_topk_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_topk_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trace_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trace_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_transpose_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_transpose_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trapezoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trapezoid_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trapz_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trapz_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_triangular_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tril_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_tril_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_triu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_triu_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_true_divide_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_true_divide_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_trunc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_unfold_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_unfold_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_unsqueeze_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_unsqueeze_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_var_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_var_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_vdot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_vdot_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_view_as_complex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_view_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_view_as_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_view_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_view_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_vsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_vsplit_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_vstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_vstack_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_where_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_where_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_xlogy_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_xlogy_xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_zero__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_multiple_devices_zero__xpu_int64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___getitem___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___radd___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___rdiv___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___rmatmul___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___rmod___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___rmul___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___rpow___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out___rsub___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_abs_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_acos_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_acosh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addbmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addcdiv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addcmul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addmm_decomposed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addmv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_addr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_all_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_amax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_amin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_aminmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_angle_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_any_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_argmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_argmin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_asin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_asinh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_atan2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_atan_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_atanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_baddbmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_bitwise_left_shift_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_bitwise_right_shift_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_block_diag_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_bmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_broadcast_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_broadcast_to_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cat_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cdist_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_ceil_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cholesky_inverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cholesky_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_chunk_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_clamp_scalar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_clamp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_clone_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_complex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_conj_physical_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_conj_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_contiguous_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_copysign_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_corrcoef_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cos_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cosh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_count_nonzero_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cov_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cross_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cummax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cummin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cumprod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cumsum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_cumulative_trapezoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_deg2rad_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_diag_embed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_diag_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_diagonal_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_diff_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_digamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_dist_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_div_floor_rounding_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_div_no_rounding_mode_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_div_trunc_rounding_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_dot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_dsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_dstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_eig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_einsum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_eq_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_erf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_erfc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_erfinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_exp2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_exp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_expand_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_expand_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_expm1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_fft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_fftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_hfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_ifft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_ifftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_ihfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_irfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_irfftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_rfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fft_rfftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fill__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_flip_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fliplr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_flipud_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_float_power_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_floor_divide_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_floor_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fmin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fmod_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_fmod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_frac_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_frexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_gather_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_ge_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_geqrf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_gradient_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_gt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_hsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_hstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_hypot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_i0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_igamma_grad_other_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_igamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_igammac_grad_other_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_igammac_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_index_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_index_copy_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_index_fill_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_index_put_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_index_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_inner_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_inverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_isin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_kron_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_kthvalue_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_le_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_lerp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_lgamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_cholesky_ex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_cholesky_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_cond_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_det_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_eig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_eigh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_eigvals_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_eigvalsh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_householder_product_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_inv_ex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_inv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_lstsq_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_matrix_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_matrix_power_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_matrix_rank_hermitian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_matrix_rank_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_multi_dot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_pinv_hermitian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_pinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_qr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_slogdet_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_svd_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_svdvals_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_tensorinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_linalg_vector_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_log10_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_log1p_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_log2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_log_softmax_dtype_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_log_softmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_log_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logaddexp2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logaddexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logcumsumexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logdet_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logical_not_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_logsumexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_lt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_lu_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_lu_unpack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_lu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_masked_fill_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_masked_scatter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_masked_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_matmul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_matrix_exp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_max_binary_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_max_reduction_no_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_max_reduction_with_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_maximum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_median_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_meshgrid_list_of_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_meshgrid_variadic_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_min_binary_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_min_reduction_no_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_min_reduction_with_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_minimum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mode_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_movedim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_msort_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mvlgamma_mvlgamma_p_1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mvlgamma_mvlgamma_p_3_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_mvlgamma_mvlgamma_p_5_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nan_to_num_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nanmean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nanmedian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nanquantile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nansum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_narrow_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_ne_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_neg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nextafter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_adaptive_avg_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_avg_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_conv2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_conv_transpose2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_cosine_similarity_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_dropout_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_gelu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_grid_sample_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_hardshrink_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_hardswish_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_hardtanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_interpolate_area_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_interpolate_bicubic_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_interpolate_bilinear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_interpolate_linear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_interpolate_nearest_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_interpolate_trilinear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_layer_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_leaky_relu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_logsigmoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_max_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_mse_loss_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_nll_loss_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_normalize_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_pad_circular_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_pad_constant_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_pad_reflect_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_pad_replicate_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_relu6_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_relu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_softplus_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_nn_functional_unfold_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_norm_fro_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_norm_inf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_norm_nuc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_ormqr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_outer_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_permute_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_pinverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_polar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_polygamma_polygamma_n_0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_polygamma_polygamma_n_1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_polygamma_polygamma_n_2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_polygamma_polygamma_n_3_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_polygamma_polygamma_n_4_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_positive_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_pow_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_prod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_put_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_qr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_quantile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_rad2deg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_ravel_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_reciprocal_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_remainder_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_remainder_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_renorm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_repeat_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_reshape_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_reshape_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_resize__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_resize_as__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_resolve_conj_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_resolve_neg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_roll_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_rot90_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_round_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_rsqrt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_rsub_rsub_scalar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_rsub_rsub_tensor_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_scatter_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_scatter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sgn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sigmoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sign_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_signbit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sinc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sinh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_softmax_with_dtype_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_softmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sort_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_entr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_erfcx_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_i0e_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_i1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_i1e_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_ndtr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_ndtri_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_polygamma_special_polygamma_n_0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_xlog1py_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_zeta_grad_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_special_zeta_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_split_list_args_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_split_with_sizes_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_split_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sqrt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_square_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_squeeze_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_stack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_std_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_std_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sub_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_sum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_svd_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_symeig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_t_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_take_along_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_take_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_tan_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_tanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_tensor_split_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_tensordot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_tile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_to_sparse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_topk_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_trace_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_transpose_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_trapezoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_trapz_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_triangular_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_tril_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_triu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_true_divide_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_trunc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_unfold_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_unsqueeze_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_var_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_var_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_vdot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_view_as_complex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_view_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_view_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_vsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_vstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_where_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_xlogy_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_out_zero__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_reference_testing_add_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_add_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_add_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_addbmm_xpu_float32 (__main__.TestCommonXPU) ... ERROR
test_reference_testing_aminmax_xpu_float32 (__main__.TestCommonXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_reference_testing_aminmax_xpu_int64 (__main__.TestCommonXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_reference_testing_cat_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_cat_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_cat_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_fmod_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_fmod_autodiffed_xpu_int64 (__main__.TestCommonXPU) ... FAIL
test_reference_testing_fmod_xpu_float32 (__main__.TestCommonXPU) ... ERROR
test_reference_testing_fmod_xpu_int64 (__main__.TestCommonXPU) ... ERROR
test_reference_testing_linalg_tensorinv_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_linalg_tensorinv_xpu_float32 (__main__.TestCommonXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_reference_testing_max_binary_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_max_binary_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_maximum_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_maximum_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_meshgrid_variadic_tensors_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_meshgrid_variadic_tensors_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_meshgrid_variadic_tensors_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_min_binary_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_min_binary_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_minimum_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_minimum_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_nn_functional_layer_norm_xpu_float32 (__main__.TestCommonXPU) ... ERROR
test_reference_testing_nn_functional_mse_loss_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_nn_functional_one_hot_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_nn_functional_softplus_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_remainder_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_remainder_xpu_float32 (__main__.TestCommonXPU) ... ERROR
test_reference_testing_repeat_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_repeat_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_repeat_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_roll_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_roll_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_roll_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_sub_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_sub_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_sub_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_tensor_split_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_tensor_split_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_tensor_split_xpu_int64 (__main__.TestCommonXPU) ... ok
test_reference_testing_tile_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'dtype not support on XPU'
test_reference_testing_tile_xpu_float32 (__main__.TestCommonXPU) ... ok
test_reference_testing_tile_xpu_int64 (__main__.TestCommonXPU) ... ok
test_variant_consistency_eager___getitem___xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___getitem___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___radd___xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___radd___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rdiv___xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rdiv___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rmatmul___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rmod___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rmul___xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rmul___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rpow___xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rpow___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rsub___xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager___rsub___xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_abs_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_abs_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_acos_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_acos_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_acosh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_acosh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_add_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addbmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addcdiv_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addcdiv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addcmul_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addcmul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addmm_decomposed_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addmm_decomposed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addmm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addmv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addr_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_addr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_all_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_all_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_amax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_amin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_aminmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_angle_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_angle_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_any_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_any_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_argmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_argmin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_asin_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_asin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_asinh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_asinh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_atan2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_atan_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_atan_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_atanh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_atanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_baddbmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_bitwise_left_shift_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_bitwise_right_shift_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_block_diag_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_block_diag_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_bmm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_bmm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_broadcast_tensors_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_broadcast_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_broadcast_to_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_broadcast_to_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cat_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cat_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cdist_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ceil_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cholesky_inverse_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cholesky_inverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cholesky_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cholesky_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_chunk_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_chunk_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_clamp_scalar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_clamp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_clone_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_clone_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_complex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_conj_physical_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_conj_physical_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_conj_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_conj_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_contiguous_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_contiguous_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_copysign_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_corrcoef_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_corrcoef_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cos_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cos_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cosh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cosh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_count_nonzero_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_count_nonzero_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cov_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cov_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cross_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cross_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cummax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cummin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cumprod_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cumprod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cumsum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cumulative_trapezoid_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_cumulative_trapezoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_deg2rad_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diag_embed_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diag_embed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diag_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diag_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diagonal_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diagonal_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diff_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_diff_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_digamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dist_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dist_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_div_floor_rounding_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_div_floor_rounding_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_div_no_rounding_mode_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_div_no_rounding_mode_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_div_trunc_rounding_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_div_trunc_rounding_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dot_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dsplit_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dstack_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_dstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_eig_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_eig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_einsum_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_einsum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_eq_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_eq_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_erf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_erfc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_erfinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_exp2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_exp_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_exp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_expand_as_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_expand_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_expand_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_expand_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_expm1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_fft_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_fft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_fftn_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_fftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_hfft_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_hfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_ifft_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_ifft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_ifftn_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_ifftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_ihfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_irfft_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_irfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_irfftn_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_irfftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_rfft_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fft_rfftn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fill__xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fill__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_flip_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_flip_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fliplr_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fliplr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_flipud_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_flipud_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_float_power_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_float_power_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_floor_divide_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_floor_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fmin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fmod_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_fmod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_frac_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_frexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_gather_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_gather_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ge_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_geqrf_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_geqrf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_gradient_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_gradient_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_gt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_hsplit_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_hsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_hstack_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_hstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_hypot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_i0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_igamma_grad_other_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_igamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_igammac_grad_other_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_igammac_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_imag_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_add_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_copy_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_copy_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_fill_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_fill_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_put_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_put_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_select_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_index_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_inner_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_inner_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_inverse_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_inverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_isin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_kron_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_kron_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_kthvalue_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_le_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lerp_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lerp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lgamma_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_cholesky_ex_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_cholesky_ex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_cholesky_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_cholesky_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_cond_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_cond_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_det_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_det_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eig_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eigh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eigh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eigvals_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eigvals_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eigvalsh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_eigvalsh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_householder_product_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_householder_product_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_inv_ex_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_inv_ex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_inv_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_inv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_lstsq_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_lstsq_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_norm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_power_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_power_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_rank_hermitian_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_rank_hermitian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_rank_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_matrix_rank_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_multi_dot_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_multi_dot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_norm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_pinv_hermitian_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_pinv_hermitian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_pinv_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_pinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_qr_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_qr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_slogdet_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_slogdet_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_solve_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_svd_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_svd_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_svdvals_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_svdvals_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_tensorinv_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_tensorinv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_vector_norm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_linalg_vector_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log10_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log10_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log1p_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log2_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log_softmax_dtype_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log_softmax_dtype_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log_softmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_log_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logaddexp2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logaddexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logcumsumexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logdet_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logical_not_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logical_not_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_logsumexp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lu_solve_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lu_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lu_unpack_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lu_unpack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lu_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_lu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_masked_fill_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_masked_fill_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_masked_scatter_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_masked_scatter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_masked_select_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_masked_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_matmul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_matrix_exp_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_max_binary_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_max_reduction_no_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_max_reduction_with_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_maximum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mean_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_median_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_meshgrid_list_of_tensors_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_meshgrid_list_of_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_meshgrid_variadic_tensors_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_meshgrid_variadic_tensors_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_min_binary_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_min_reduction_no_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_min_reduction_with_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_minimum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mode_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_movedim_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_movedim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_msort_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mul_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mul_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mv_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mv_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mvlgamma_mvlgamma_p_1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mvlgamma_mvlgamma_p_3_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_mvlgamma_mvlgamma_p_5_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nan_to_num_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nanmean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nanmedian_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nanquantile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nansum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_narrow_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_narrow_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ne_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ne_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_neg_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_neg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nextafter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_adaptive_avg_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_avg_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_conv2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_conv_transpose2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_cosine_similarity_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_dropout_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_gelu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_grid_sample_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_hardshrink_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_hardswish_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_hardtanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_interpolate_area_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_interpolate_bicubic_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_interpolate_bilinear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_interpolate_linear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_interpolate_nearest_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_interpolate_trilinear_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_layer_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_leaky_relu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_logsigmoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_max_pool2d_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_mse_loss_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_nll_loss_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_normalize_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_circular_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_circular_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_constant_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_constant_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_reflect_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_reflect_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_replicate_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_pad_replicate_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_relu6_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_relu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_softplus_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_nn_functional_unfold_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_fro_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_fro_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_inf_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_inf_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_nuc_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_nuc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_norm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ormqr_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ormqr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_outer_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_outer_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_permute_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_permute_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_pinverse_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_pinverse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_polar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_polygamma_polygamma_n_0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_polygamma_polygamma_n_1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_polygamma_polygamma_n_2_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_polygamma_polygamma_n_3_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_polygamma_polygamma_n_4_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_positive_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_positive_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_pow_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_pow_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_prod_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_prod_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_put_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_put_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_qr_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_qr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_quantile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rad2deg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ravel_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_ravel_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_real_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_reciprocal_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_reciprocal_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_remainder_autodiffed_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_remainder_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_renorm_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_renorm_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_repeat_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_repeat_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_reshape_as_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_reshape_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_reshape_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_reshape_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resize__xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resize__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resize_as__xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resize_as__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resolve_conj_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resolve_conj_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resolve_neg_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_resolve_neg_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_roll_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_roll_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rot90_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rot90_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_round_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rsqrt_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rsqrt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rsub_rsub_scalar_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rsub_rsub_scalar_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rsub_rsub_tensor_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_rsub_rsub_tensor_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_scatter_add_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_scatter_add_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_scatter_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_scatter_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_select_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_select_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sgn_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sgn_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sigmoid_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sigmoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sign_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_signbit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sin_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sin_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sinc_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sinc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sinh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sinh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_softmax_with_dtype_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_softmax_with_dtype_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_softmax_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_solve_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sort_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_entr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_erfcx_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_i0e_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_i1_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_i1e_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_ndtr_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_ndtri_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_polygamma_special_polygamma_n_0_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_xlog1py_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_zeta_grad_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_special_zeta_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_split_list_args_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_split_list_args_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_split_with_sizes_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_split_with_sizes_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_split_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_split_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sqrt_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sqrt_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_square_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_square_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_squeeze_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_squeeze_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_stack_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_stack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_std_mean_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_std_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_std_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_std_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sub_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sub_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sum_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_sum_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_svd_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_svd_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_symeig_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_symeig_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_t_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_t_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_take_along_dim_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_take_along_dim_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_take_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_take_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tan_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tan_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tanh_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tanh_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tensor_split_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tensor_split_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tensordot_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tensordot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tile_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tile_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_to_sparse_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_to_sparse_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_topk_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trace_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trace_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_transpose_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_transpose_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trapezoid_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trapezoid_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trapz_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trapz_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_triangular_solve_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_triangular_solve_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tril_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_tril_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_triu_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_triu_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_true_divide_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_true_divide_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_trunc_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_unfold_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_unfold_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_unsqueeze_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_unsqueeze_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_var_mean_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_var_mean_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_var_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_var_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_vdot_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_vdot_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_view_as_complex_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_view_as_real_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_view_as_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_view_as_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_view_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_view_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_vsplit_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_vsplit_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_vstack_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_vstack_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_where_xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_where_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_xlogy_xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_zero__xpu_complex64 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'
test_variant_consistency_eager_zero__xpu_float32 (__main__.TestCommonXPU) ... skipped 'not ready on XPU'

======================================================================
ERROR: test_reference_testing_addbmm_xpu_float32 (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_reference_testing_fmod_xpu_float32 (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1023, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 201, in test_reference_testing
    self.compare_with_reference(op, op.ref, sample_input)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1618, in compare_with_reference
    actual = torch_fn(t_inp, *t_args, **t_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 626, in __call__
    return self.op(*args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_reference_testing_fmod_xpu_int64 (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1023, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 201, in test_reference_testing
    self.compare_with_reference(op, op.ref, sample_input)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1618, in compare_with_reference
    actual = torch_fn(t_inp, *t_args, **t_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 626, in __call__
    return self.op(*args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_reference_testing_nn_functional_layer_norm_xpu_float32 (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_reference_testing_remainder_xpu_float32 (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1023, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 201, in test_reference_testing
    self.compare_with_reference(op, op.ref, sample_input)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1618, in compare_with_reference
    actual = torch_fn(t_inp, *t_args, **t_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 626, in __call__
    return self.op(*args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
FAIL: test_dtypes___rmod___xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for __rmod__ on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_abs_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for abs on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bool}. 

======================================================================
FAIL: test_dtypes_acos_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for acos on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_acosh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for acosh on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_addbmm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int8
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.bfloat16, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float64, torch.float32}.
The supported dtypes for addbmm on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.bfloat16, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.int8, torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_addcdiv_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for addcdiv on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_addcmul_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for addcmul on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_addmm_decomposed_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for addmm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. 

======================================================================
FAIL: test_dtypes_addmm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8
Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.bfloat16, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for addmm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.bfloat16, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_addmv_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.complex64
torch.bfloat16
torch.complex128
torch.float16
torch.int8 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float64, torch.float32}.
The supported dtypes for addmv on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.int8}. 

======================================================================
FAIL: test_dtypes_addr_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for addr on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be removed from the OpInfo: {torch.bool}.

======================================================================
FAIL: test_dtypes_argmax_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for argmax on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bool}. 

======================================================================
FAIL: test_dtypes_argmin_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for argmin on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bool}. 

======================================================================
FAIL: test_dtypes_asin_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for asin on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_asinh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for asinh on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_atan2_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for atan2 on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_atan_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for atan on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_atanh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for atanh on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_baddbmm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8
Items in the second set but not the first:
torch.float64 : Attempted to compare [set] types: Expected: {torch.bfloat16, torch.float32, torch.int8, torch.float16}; Actual: {torch.float64, torch.float32, torch.float16}.
The supported dtypes for baddbmm on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.float16}, but the detected supported dtypes are {torch.bfloat16, torch.float32, torch.int8, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. The following dtypes should be removed from the OpInfo: {torch.float64}.

======================================================================
FAIL: test_dtypes_bitwise_left_shift_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for bitwise_left_shift on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.int8, torch.int16}. 

======================================================================
FAIL: test_dtypes_bitwise_right_shift_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for bitwise_right_shift on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.int8, torch.int16}. 

======================================================================
FAIL: test_dtypes_bmm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.uint8
torch.int16
torch.int64 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for bmm on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.uint8, torch.int16, torch.int64}.

======================================================================
FAIL: test_dtypes_cdist_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for cdist on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_ceil_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for ceil on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_cholesky_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for cholesky on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_clamp_scalar_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for clamp on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_corrcoef_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for corrcoef on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_cos_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for cos on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_cosh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for cosh on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_count_nonzero_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for count_nonzero on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_cross_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16
Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for cross on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_cummax_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for cummax on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_cummin_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for cummin on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_cumprod_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for cumprod on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_cumsum_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.complex64
torch.int64
torch.complex128
torch.float16
torch.uint8
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for cumsum on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.int8, torch.int16}. 

======================================================================
FAIL: test_dtypes_cumulative_trapezoid_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for cumulative_trapezoid on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_diag_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for diag on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_digamma_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for digamma on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_dot_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for dot on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.int8, torch.int16}.

======================================================================
FAIL: test_dtypes_einsum_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.uint8
torch.int16
torch.int64 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for einsum on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.uint8, torch.int16, torch.int64}.

======================================================================
FAIL: test_dtypes_erf_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for erf on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_erfc_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for erfc on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_erfinv_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for erfinv on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_exp2_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for exp2 on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16}. 

======================================================================
FAIL: test_dtypes_exp_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for exp on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_expm1_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for expm1 on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_fft_fftn_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.complex128
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for fft.fftn on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_fft_ifftn_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.complex128
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for fft.ifftn on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_fft_irfftn_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.complex128
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for fft.irfftn on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_fft_rfftn_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported backward dtypes for fft.rfftn on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported backward dtypes are set().
        The following backward dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_floor_divide_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for floor_divide on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.complex128, torch.complex64}. 

======================================================================
FAIL: test_dtypes_floor_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for floor on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_fmod_autodiffed_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
Items in the second set but not the first:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for fmod on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16}. The following dtypes should be removed from the OpInfo: {torch.bool}.

======================================================================
FAIL: test_dtypes_fmod_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.float16
torch.uint8
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for fmod on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_frexp_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.float16}.
The supported dtypes for frexp on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.float16}.

======================================================================
FAIL: test_dtypes_gather_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for gather on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be removed from the OpInfo: {torch.float16}.

======================================================================
FAIL: test_dtypes_geqrf_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for geqrf on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_hypot_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for hypot on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_i0_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for i0 on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_igamma_grad_other_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
The supported dtypes for igamma on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.

======================================================================
FAIL: test_dtypes_igamma_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
The supported dtypes for igamma on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.

======================================================================
FAIL: test_dtypes_igammac_grad_other_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
The supported dtypes for igammac on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.

======================================================================
FAIL: test_dtypes_igammac_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
The supported dtypes for igammac on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.bfloat16, torch.float16}.

======================================================================
FAIL: test_dtypes_index_add_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for index_add on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_index_copy_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float32
torch.complex64
torch.bfloat16
torch.float64
torch.complex128
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.
The supported backward dtypes for index_copy on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}, but the detected supported backward dtypes are set().
        The following backward dtypes should be removed from the OpInfo: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.

======================================================================
FAIL: test_dtypes_index_select_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for index_select on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_inner_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for inner on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. 

======================================================================
FAIL: test_dtypes_isin_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for isin on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_kthvalue_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.
The supported dtypes for kthvalue on xpu according to its OpInfo are
        {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}, but the detected supported dtypes are {torch.int32, torch.int64, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_lgamma_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.uint8
torch.bool
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for lgamma on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.int8, torch.int16}.

======================================================================
FAIL: test_dtypes_linalg_cholesky_ex_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.cholesky_ex on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_cholesky_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.cholesky on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_det_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.det on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_eigh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.eigh on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_eigvalsh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.eigvalsh on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_householder_product_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.householder_product on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_inv_ex_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.inv_ex on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_inv_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.inv on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_lstsq_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.lstsq on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_matrix_power_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.matrix_power on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_matrix_rank_hermitian_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.matrix_rank on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_multi_dot_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for linalg.multi_dot on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. 

======================================================================
FAIL: test_dtypes_linalg_norm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float32
torch.complex64
torch.bfloat16
torch.float64
torch.complex128
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for linalg.norm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.

======================================================================
FAIL: test_dtypes_linalg_pinv_hermitian_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.pinv on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_qr_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.qr on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_slogdet_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.slogdet on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_svdvals_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.complex128
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.complex128, torch.float32, torch.complex64}.
The supported backward dtypes for linalg.svdvals on xpu according to its OpInfo are
        {torch.float64, torch.complex128, torch.float32, torch.complex64}, but the detected supported backward dtypes are set().
        The following backward dtypes should be removed from the OpInfo: {torch.float64, torch.complex128, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_tensorinv_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for linalg.tensorinv on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_linalg_vector_norm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float32
torch.complex64
torch.bfloat16
torch.float64
torch.complex128
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for linalg.vector_norm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.

======================================================================
FAIL: test_dtypes_log10_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for log10 on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_log1p_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for log1p on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_log2_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for log2 on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_log_softmax_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for log_softmax on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_log_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for log on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_logaddexp2_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for logaddexp2 on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_logaddexp_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for logaddexp on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_logcumsumexp_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for logcumsumexp on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_logit_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for logit on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_logsumexp_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for logsumexp on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_lu_solve_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for lu_solve on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_lu_unpack_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for lu_unpack on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_lu_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for lu on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_matrix_exp_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for matrix_exp on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_max_reduction_with_dim_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16}; Actual: {torch.bfloat16, torch.float64, torch.float32, torch.float16}.
The supported backward dtypes for max on xpu according to its OpInfo are
        {torch.bfloat16, torch.float64, torch.float32, torch.float16}, but the detected supported backward dtypes are {torch.float64, torch.float32, torch.bfloat16}.
        The following backward dtypes should be removed from the OpInfo: {torch.float16}.

======================================================================
FAIL: test_dtypes_median_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.float32
torch.int8
torch.int64
torch.float64
torch.int16
torch.uint8 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.
The supported dtypes for median on xpu according to its OpInfo are
        {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.

======================================================================
FAIL: test_dtypes_min_reduction_with_dim_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16}; Actual: {torch.bfloat16, torch.float64, torch.float32, torch.float16}.
The supported backward dtypes for min on xpu according to its OpInfo are
        {torch.bfloat16, torch.float64, torch.float32, torch.float16}, but the detected supported backward dtypes are {torch.float64, torch.float32, torch.bfloat16}.
        The following backward dtypes should be removed from the OpInfo: {torch.float16}.

======================================================================
FAIL: test_dtypes_mm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for mm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. 

======================================================================
FAIL: test_dtypes_mode_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for mode on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_msort_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for msort on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be removed from the OpInfo: {torch.bool}.

======================================================================
FAIL: test_dtypes_mv_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16
Items in the second set but not the first:
torch.int32
torch.uint8
torch.int16
torch.int64 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for mv on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.float16}. The following dtypes should be removed from the OpInfo: {torch.int32, torch.uint8, torch.int16, torch.int64}.

======================================================================
FAIL: test_dtypes_mvlgamma_mvlgamma_p_1_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.
The supported dtypes for mvlgamma on xpu according to its OpInfo are
        {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.int8, torch.int16}.

======================================================================
FAIL: test_dtypes_nan_to_num_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for nan_to_num on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_nanmedian_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.float32
torch.int8
torch.int64
torch.float64
torch.int16
torch.uint8 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.
The supported dtypes for nanmedian on xpu according to its OpInfo are
        {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.

======================================================================
FAIL: test_dtypes_nextafter_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.bfloat16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for nextafter on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.bfloat16}.

======================================================================
FAIL: test_dtypes_nn_functional_adaptive_avg_pool2d_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.adaptive_avg_pool2d on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_nn_functional_avg_pool2d_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.avg_pool2d on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_nn_functional_conv2d_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8
torch.float16
Items in the second set but not the first:
torch.float64
torch.int64 : Attempted to compare [set] types: Expected: {torch.bfloat16, torch.float32, torch.int8, torch.float16}; Actual: {torch.float64, torch.float32, torch.int64}.
The supported dtypes for nn.functional.conv2d on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.int64}, but the detected supported dtypes are {torch.bfloat16, torch.float32, torch.int8, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8, torch.float16}. The following dtypes should be removed from the OpInfo: {torch.float64, torch.int64}.

======================================================================
FAIL: test_dtypes_nn_functional_conv_transpose2d_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.conv_transpose2d on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_nn_functional_cosine_similarity_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for nn.functional.cosine_similarity on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_dropout_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for nn.functional.dropout on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_gelu_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.gelu on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_grid_sample_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.grid_sample on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_hardshrink_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.hardshrink on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_hardswish_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.hardswish on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_nn_functional_hardtanh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.hardtanh on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.int8, torch.int16}. 

======================================================================
FAIL: test_dtypes_nn_functional_interpolate_area_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.interpolate on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_nn_functional_interpolate_bilinear_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.bfloat16
torch.float16
torch.uint8
torch.int8
Items in the second set but not the first:
torch.float64 : Attempted to compare [set] types: Expected: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.interpolate on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.int8}. The following dtypes should be removed from the OpInfo: {torch.float64}.

======================================================================
FAIL: test_dtypes_nn_functional_interpolate_linear_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.bfloat16
torch.float16
torch.uint8
torch.int8
Items in the second set but not the first:
torch.float64 : Attempted to compare [set] types: Expected: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.interpolate on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.int8}. The following dtypes should be removed from the OpInfo: {torch.float64}.

======================================================================
FAIL: test_dtypes_nn_functional_interpolate_nearest_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.bfloat16
torch.float16
torch.uint8
torch.int8
Items in the second set but not the first:
torch.float64 : Attempted to compare [set] types: Expected: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.interpolate on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.int8}. The following dtypes should be removed from the OpInfo: {torch.float64}.

======================================================================
FAIL: test_dtypes_nn_functional_interpolate_trilinear_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.bfloat16
torch.float16
torch.uint8
torch.int8
Items in the second set but not the first:
torch.float64 : Attempted to compare [set] types: Expected: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.interpolate on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.bfloat16, torch.float16, torch.uint8, torch.int8}. The following dtypes should be removed from the OpInfo: {torch.float64}.

======================================================================
FAIL: test_dtypes_nn_functional_layer_norm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 969, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16
Items in the second set but not the first:
torch.float64 : Attempted to compare [set] types: Expected: {torch.bfloat16, torch.float32, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for nn.functional.layer_norm on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.bfloat16, torch.float32, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. The following dtypes should be removed from the OpInfo: {torch.float64}.

======================================================================
FAIL: test_dtypes_nn_functional_leaky_relu_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.leaky_relu on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_logsigmoid_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.logsigmoid on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_mse_loss_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.mse_loss on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_nll_loss_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.nll_loss on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16}. 

======================================================================
FAIL: test_dtypes_nn_functional_normalize_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.complex128
torch.complex64
torch.float16 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.normalize on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.complex128, torch.complex64, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_pad_reflect_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for nn.functional.pad on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_nn_functional_pad_replicate_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.float64
torch.float32
torch.complex64 : Attempted to compare [set] types: Expected: set(); Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for nn.functional.pad on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.float64, torch.float32, torch.complex64}.

======================================================================
FAIL: test_dtypes_nn_functional_relu6_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.
The supported dtypes for nn.functional.relu6 on xpu according to its OpInfo are
        {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_relu_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.relu on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_softplus_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for nn.functional.softplus on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_nn_functional_unfold_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32, torch.float16}.
The supported dtypes for nn.functional.unfold on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32, torch.float16}.

======================================================================
FAIL: test_dtypes_norm_fro_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}; Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128}.
The supported dtypes for norm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_norm_inf_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float32
torch.complex64
torch.bfloat16
torch.float64
torch.complex128
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.
The supported backward dtypes for norm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}, but the detected supported backward dtypes are set().
        The following backward dtypes should be removed from the OpInfo: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.

======================================================================
FAIL: test_dtypes_ormqr_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for ormqr on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_polar_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for polar on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_polygamma_polygamma_n_0_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for polygamma on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_pow_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}; Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128}.
The supported backward dtypes for pow on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128}, but the detected supported backward dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.float64}.
        The following backward dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_prod_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16
Items in the second set but not the first:
torch.bool : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for prod on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. The following dtypes should be removed from the OpInfo: {torch.bool}.

======================================================================
FAIL: test_dtypes_qr_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for qr on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_remainder_autodiffed_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for remainder on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.int8, torch.int16}. 

======================================================================
FAIL: test_dtypes_remainder_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float64
torch.float32 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float64, torch.float32}.
The supported dtypes for remainder on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float64, torch.float32}.

======================================================================
FAIL: test_dtypes_renorm_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float32
torch.complex64
torch.bfloat16
torch.float64
torch.complex128
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for renorm on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.

======================================================================
FAIL: test_dtypes_round_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for round on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_rsqrt_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for rsqrt on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_rsub_rsub_scalar_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for rsub on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_rsub_rsub_tensor_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for rsub on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_scatter_add_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex64
torch.complex128
torch.float16
torch.uint8
torch.bool
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float32, torch.float64}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for scatter_add on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float32, torch.float64}.
        The following dtypes should be removed from the OpInfo: {torch.complex64, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.int8, torch.int16}.

======================================================================
FAIL: test_dtypes_scatter_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.complex128
torch.float16
torch.uint8
torch.bool
torch.int8
torch.int16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for scatter on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16}.
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.int8, torch.int16}.

======================================================================
FAIL: test_dtypes_signbit_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for signbit on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_sin_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for sin on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_sinc_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for sinc on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_sinh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for sinh on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_softmax_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32}.
The supported dtypes for softmax on xpu according to its OpInfo are
        {torch.float64, torch.float32}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_solve_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for solve on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_sort_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for sort on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_entr_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.entr on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_erfcx_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.erfcx on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_i0e_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.i0e on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_i1_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.i1 on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_i1e_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.i1e on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_ndtr_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.ndtr on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_special_ndtri_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.ndtri on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_polygamma_special_polygamma_n_0_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.polygamma on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_special_xlog1py_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.xlog1py on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_zeta_grad_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.zeta on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_special_zeta_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for special.zeta on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_sqrt_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for sqrt on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_std_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for std on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16}. The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_symeig_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for symeig on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_take_along_dim_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for take_along_dim on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_tan_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for tan on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_tanh_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for tanh on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_tensor_split_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for tensor_split on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.float16}. 

======================================================================
FAIL: test_dtypes_tensordot_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.int8 : Attempted to compare [set] types: Expected: {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for tensordot on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.complex64, torch.bfloat16, torch.complex128, torch.float16, torch.float32, torch.int8, torch.float64}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.int8}. 

======================================================================
FAIL: test_dtypes_topk_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.float32
torch.int8
torch.int64
torch.float64
torch.int16
torch.uint8 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.
The supported dtypes for topk on xpu according to its OpInfo are
        {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.float32, torch.int8, torch.int64, torch.float64, torch.int16, torch.uint8}.

======================================================================
FAIL: test_dtypes_trace_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
torch.bool
torch.float16 : Attempted to compare [set] types: Expected: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}; Actual: {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for trace on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.complex128, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16, torch.bool, torch.float16}. 

======================================================================
FAIL: test_dtypes_triangular_solve_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32}; Actual: {torch.complex128, torch.float64, torch.float32, torch.complex64}.
The supported dtypes for triangular_solve on xpu according to its OpInfo are
        {torch.complex128, torch.float64, torch.float32, torch.complex64}, but the detected supported dtypes are {torch.float64, torch.float32}.
        The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_trunc_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.float16 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float64, torch.float32, torch.bfloat16}.
The supported dtypes for trunc on xpu according to its OpInfo are
        {torch.float64, torch.float32, torch.bfloat16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.float16}. 

======================================================================
FAIL: test_dtypes_unfold_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 172, in test_dtypes
    self.assertEqual(supported_backward_dtypes, claimed_backward_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.float32
torch.complex64
torch.bfloat16
torch.float64
torch.complex128
torch.float16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.
The supported backward dtypes for unfold on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}, but the detected supported backward dtypes are set().
        The following backward dtypes should be removed from the OpInfo: {torch.float32, torch.complex64, torch.bfloat16, torch.float64, torch.complex128, torch.float16}.

======================================================================
FAIL: test_dtypes_var_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the first set but not the second:
torch.bfloat16
Items in the second set but not the first:
torch.complex128
torch.complex64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.bfloat16, torch.float16}; Actual: {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}.
The supported dtypes for var on xpu according to its OpInfo are
        {torch.float32, torch.complex64, torch.float64, torch.complex128, torch.float16}, but the detected supported dtypes are {torch.float64, torch.float32, torch.bfloat16, torch.float16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16}. The following dtypes should be removed from the OpInfo: {torch.complex128, torch.complex64}.

======================================================================
FAIL: test_dtypes_vdot_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.complex64
torch.int64
torch.bfloat16
torch.complex128
torch.float16
torch.uint8
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for vdot on xpu according to its OpInfo are
        {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.complex64, torch.int64, torch.bfloat16, torch.complex128, torch.float16, torch.uint8, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_dtypes_xlogy_xpu (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 153, in test_dtypes
    self.assertEqual(supported_dtypes, claimed_supported, msg=msg)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1920, in assertEqual
    super().assertEqual(x, y, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: Items in the second set but not the first:
torch.int32
torch.int64
torch.bfloat16
torch.float16
torch.uint8
torch.bool
torch.float32
torch.int8
torch.float64
torch.int16 : Attempted to compare [set] types: Expected: set(); Actual: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.
The supported dtypes for xlogy on xpu according to its OpInfo are
        {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}, but the detected supported dtypes are set().
        The following dtypes should be removed from the OpInfo: {torch.int32, torch.int64, torch.bfloat16, torch.float16, torch.uint8, torch.bool, torch.float32, torch.int8, torch.float64, torch.int16}.

======================================================================
FAIL: test_reference_testing_fmod_autodiffed_xpu_int64 (__main__.TestCommonXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1023, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 201, in test_reference_testing
    self.compare_with_reference(op, op.ref, sample_input)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1621, in compare_with_reference
    self.assertEqual(actual, expected, exact_device=False)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1899, in assertEqual
    self.assertEqual(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!Attempted to compare equality of tensors with different dtypes. Got dtypes torch.int64 and torch.float64.

----------------------------------------------------------------------
Ran 1930 tests in 1602.485s

FAILED (failures=194, errors=5, skipped=1541)
Raised CalledProcessError: return code 1.