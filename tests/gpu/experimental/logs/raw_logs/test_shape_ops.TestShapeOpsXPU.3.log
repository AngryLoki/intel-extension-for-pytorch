test_clamp_propagates_nans_xpu (__main__.TestShapeOpsXPU) ... ok
test_clamp_raises_arg_errors_xpu (__main__.TestShapeOpsXPU) ... ok
test_clamp_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_clamp_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_complex_rot90_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_complex_rot90_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_diag_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_diag_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_diagonal_multidim_xpu_float32 (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cpu'
test_diagonal_xpu (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_errors_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_errors_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_flip_large_tensor_xpu (__main__.TestShapeOpsXPU) ... skipped 'Insufficient xpu:0 memory'
test_flip_numpy_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_numpy_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_numpy_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_float32 (__main__.TestShapeOpsXPU) ... skipped "not implemented: Could not run 'aten::flip' with arguments from the 'QuantizedXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::flip' is only available for these backends: [CPU, XPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/csrc/aten/generated/ATen/RegisterXPU.cpp:11184 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_flip_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_invalid_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_fliplr_invalid_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_invalid_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_fliplr_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_invalid_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flipud_invalid_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_invalid_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flipud_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_movedim_invalid_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_movedim_invalid_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_movedim_invalid_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_movedim_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_movedim_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_movedim_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_astuple_out_xpu (__main__.TestShapeOpsXPU) ... ok
test_nonzero_discontiguous_xpu (__main__.TestShapeOpsXPU) ... FAIL
test_nonzero_no_warning_xpu (__main__.TestShapeOpsXPU) ... ok
test_nonzero_non_diff_xpu (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_rot90_xpu (__main__.TestShapeOpsXPU) ... ok
test_tolist_xpu (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cpu'
test_trace_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_trace_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_unbind_xpu (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cpu'

======================================================================
FAIL: test_nonzero_discontiguous_xpu (__main__.TestShapeOpsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 575, in only_fn
    return fn(self, device, *args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_shape_ops.py", line 668, in test_nonzero_discontiguous
    self.assertEqual(dst1, dst4, atol=0, rtol=0)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1877, in assertEqual
    super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
AssertionError: False is not true : Tensors failed to compare as equal!Found 6 different element(s) (out of 8), with the greatest difference of 72340172838076673 (0 vs. 72340172838076673) occuring at index (0, 0).

----------------------------------------------------------------------
Ran 90 tests in 35.414s

FAILED (failures=1, skipped=19)
Raised CalledProcessError: return code 1.