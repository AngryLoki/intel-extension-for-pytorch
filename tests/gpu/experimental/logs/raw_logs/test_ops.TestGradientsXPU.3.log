test_fn_fail_gradgrad___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad___getitem___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad___radd___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad___rmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad___rpow___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad___rsub___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_abs_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_acos_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_acosh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_addmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addmv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_addr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_amax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_amin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_fn_fail_gradgrad_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_angle_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_asin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_asinh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_atan2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_atan_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_atanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_bmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cdist_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: the derivative for '_cdist_backward' is not implemented."
test_fn_fail_gradgrad_ceil_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_chunk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_clamp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_clone_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_copysign_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cos_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cosh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cov_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cross_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_diff_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_digamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_dist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_dstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_einsum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_erf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_erfc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_exp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_expand_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_expm1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fill__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_flip_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_flipud_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_float_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_floor_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_fmod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_frac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_gather_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_gradient_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_hstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_index_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_index_put_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_index_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_inner_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_kron_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_lerp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_log10_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_log1p_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_log2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_log_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_logdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_lu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_matmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_maximum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_minimum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_mm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mode_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_movedim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_msort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_mul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_mv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_fail_gradgrad_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_fail_gradgrad_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nansum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_narrow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_fail_gradgrad_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::hardswish' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hardswish' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_fail_gradgrad_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_outer_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_permute_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_fail_gradgrad_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_fail_gradgrad_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_fail_gradgrad_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_fail_gradgrad_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_positive_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_pow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_prod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_put_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_quantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_ravel_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_remainder_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_repeat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_reshape_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_roll_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_rot90_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_round_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_scatter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sgn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sign_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sinh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_fail_gradgrad_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_fail_gradgrad_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_square_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_stack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_std_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sub_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_sum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_symeig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_t_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_take_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_tan_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_tanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_tile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_topk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_trace_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_transpose_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_trapz_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_tril_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_triu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_trunc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_var_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_view_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_view_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_vstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_where_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_fail_gradgrad_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_fail_gradgrad_zero__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does support gradgrad'
test_fn_grad___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad___getitem___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad___radd___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad___rmul___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad___rpow___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad___rsub___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_abs_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_acos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_acosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_add_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_addmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addmv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_addr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_amax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_amin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_fn_grad_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_angle_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_asin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_asinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_atan2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_atan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_atanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_bmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cat_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cdist_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_ceil_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_chunk_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_clamp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_clone_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_complex_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_conj_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_copysign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cov_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:496: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at  ../aten/src/ATen/native/Correlation.cpp:99.)
  gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
ERROR
test_fn_grad_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cross_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_cummax_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummax_helper' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_cummin_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummin_helper' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_diag_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_diff_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_digamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_dist_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_dot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_dstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_eig_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:496: UserWarning: torch.eig is deprecated in favor of torch.linalg.eig and will be removed in a future PyTorch release.
torch.linalg.eig returns complex tensors of dtype cfloat or cdouble rather than real tensors mimicking complex tensors.
L, _ = torch.eig(A)
should be replaced with
L_complex = torch.linalg.eigvals(A)
and
L, V = torch.eig(A, eigenvectors=True)
should be replaced with
L_complex, V_complex = torch.linalg.eig(A) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2882.)
  gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
ok
test_fn_grad_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_einsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_erf_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_erfc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_exp2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_exp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_expand_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_expm1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fill__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_flip_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_flipud_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_float_power_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_floor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_fmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_fmin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_fmod_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_frac_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::frexp.Tensor_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::frexp.Tensor_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_gather_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_gradient_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_hstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hypot.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hypot.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::igamma.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::igamma.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::igammac.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::igammac.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_index_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_index_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_index_select_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_inner_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_inverse_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_kron_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_lerp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::lu_unpack' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::lu_unpack' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_log10_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_log1p_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_log2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_log_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logaddexp2.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logaddexp2.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logaddexp.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logaddexp.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_logcumsumexp' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_logcumsumexp' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_logdet_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logit' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logit' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::lu_unpack' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::lu_unpack' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_lu_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_matmul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::matrix_exp' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::matrix_exp' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_maximum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_mean_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::median.dim_values' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::median.dim_values' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_minimum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_mm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mode_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_movedim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_msort_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_mul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_mv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nan_to_num.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nan_to_num.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nansum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_narrow_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hardswish' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hardswish' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
ok
test_fn_grad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::replication_pad1d.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::replication_pad1d.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_norm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_outer_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_permute_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::polar.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::polar.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_positive_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_pow_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_prod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_qr_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:496: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1931.)
  gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
ERROR
test_fn_grad_quantile_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_ravel_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_remainder_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_repeat_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_reshape_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_roll_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_rot90_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_round_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_select_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sgn_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_sign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sinc.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sinc.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_softmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_solve_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:496: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release.
torch.linalg.solve has its arguments reversed and does not return the LU factorization.
To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack.
X = torch.solve(B, A).solution
should be replaced with
X = torch.linalg.solve(A, B) (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:758.)
  gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
ok
test_fn_grad_sort_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sort.stable' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sort.stable' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_entr.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_entr.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_i0e.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i0e.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_i1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_i1e.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i1e.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_ndtri.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_ndtri.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_xlog1py.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_xlog1py.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_zeta.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_zeta.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_grad_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_split_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_square_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_stack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_std_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sub_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_sum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_svd_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_symeig_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:5878: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2487.)
  return op(input + input.conj().transpose(-2, -1), *args, **kwargs)
ok
test_fn_grad_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_t_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_take_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_tan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_tanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_tile_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_topk_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_trace_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_transpose_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_trapz_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_tril_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_triu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_trunc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::unfold_backward' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::unfold_backward' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_grad_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_var_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_grad_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::vdot' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::vdot' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_view_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_view_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_vstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_where_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_grad_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::xlogy.OutTensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::xlogy.OutTensor' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_grad_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_grad_zero__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad___getitem___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad___radd___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad___rmul___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad___rpow___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad___rsub___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_abs_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_acos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_acosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_add_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_addmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addmv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_addr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_amax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_amin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_fn_gradgrad_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_angle_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_asin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_asinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_atan2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_atan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_atanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_bmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cat_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cdist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support gradgrad'
test_fn_gradgrad_ceil_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_chunk_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_clamp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_clone_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_complex_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_conj_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_copysign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cov_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cross_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_cummax_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummax_helper' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_cummin_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummin_helper' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_diag_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_diff_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_digamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_dist_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_dot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_dstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_eig_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_einsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_erf_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_erfc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_exp2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_exp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_expand_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_expm1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fill__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_flip_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_flipud_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_float_power_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_floor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_fmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_fmin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_fmod_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_frac_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::frexp.Tensor_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::frexp.Tensor_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_gather_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_gradient_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_hstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hypot.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hypot.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::igamma.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::igamma.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::igammac.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::igammac.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_index_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_index_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_index_select_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_inner_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_inverse_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_kron_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_lerp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_log10_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_log1p_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_log2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_log_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logaddexp2.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logaddexp2.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logaddexp.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logaddexp.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_logcumsumexp' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_logcumsumexp' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_logdet_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logit' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logit' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::lu_unpack' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::lu_unpack' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_lu_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_matmul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::matrix_exp' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::matrix_exp' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_maximum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_mean_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::median.dim_values' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::median.dim_values' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_minimum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_mm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_mode_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_movedim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_msort_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_mul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_mv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nan_to_num.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nan_to_num.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nansum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_narrow_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support gradgrad'
test_fn_gradgrad_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support gradgrad'
test_fn_gradgrad_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
ok
test_fn_gradgrad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::replication_pad1d.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::replication_pad1d.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_norm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_outer_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_permute_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::polar.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::polar.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_positive_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_pow_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_prod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_qr_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_quantile_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_ravel_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_remainder_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_repeat_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_reshape_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_roll_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_rot90_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_round_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_select_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sgn_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_sign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sinc.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sinc.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_softmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_solve_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sort_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sort.stable' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sort.stable' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1068 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_entr.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_entr.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_i0e.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i0e.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_i1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_i1e.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i1e.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_ndtri.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_ndtri.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_xlog1py.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_xlog1py.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_zeta.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_zeta.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_fn_gradgrad_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_split_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_square_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_stack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_std_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sub_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_sum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_svd_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_symeig_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_t_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_take_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_tan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_tanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_tile_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_topk_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_trace_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_transpose_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_trapz_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_fn_gradgrad_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_tril_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_triu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_trunc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::unfold_backward' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::unfold_backward' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_fn_gradgrad_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_var_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::vdot' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::vdot' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_view_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_view_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_vstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_where_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_fn_gradgrad_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::xlogy.OutTensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::xlogy.OutTensor' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_fn_gradgrad_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_fn_gradgrad_zero__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD___getitem___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with index that does not support it.'
test_forward_mode_AD___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD___radd___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with mv that does not support it.'
test_forward_mode_AD___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD___rmul___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD___rpow___xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD___rsub___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with rsub that does not support it.'
test_forward_mode_AD_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_abs_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_acos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_acosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_add_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_addmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addmv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_addr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_amax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with amax that does not support it.'
test_forward_mode_AD_amin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with amin that does not support it.'
test_forward_mode_AD_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_forward_mode_AD_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_angle_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_asin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_asinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_atan2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with atan2 that does not support it.'
test_forward_mode_AD_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_atan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_atanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_bmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cat_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cdist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _cdist_forward that does not support it.'
test_forward_mode_AD_ceil_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with cholesky that does not support it.'
test_forward_mode_AD_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_chunk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with split that does not support it.'
test_forward_mode_AD_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_clamp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with clamp that does not support it.'
test_forward_mode_AD_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_clone_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_complex_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_conj_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_copysign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with diag that does not support it.'
test_forward_mode_AD_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cov_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cross_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_cummax_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummax_helper' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_cummin_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummin_helper' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with diag that does not support it.'
test_forward_mode_AD_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_diff_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_digamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_dist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with dist that does not support it.'
test_forward_mode_AD_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_dot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_dstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with eig that does not support it.'
test_forward_mode_AD_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_einsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_erf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erf that does not support it.'
test_forward_mode_AD_erfc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erfc that does not support it.'
test_forward_mode_AD_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erfinv that does not support it.'
test_forward_mode_AD_exp2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_exp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_expand_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_expm1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with constant_pad_nd that does not support it.'
test_forward_mode_AD_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with constant_pad_nd that does not support it.'
test_forward_mode_AD_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with constant_pad_nd that does not support it.'
test_forward_mode_AD_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with constant_pad_nd that does not support it.'
test_forward_mode_AD_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with constant_pad_nd that does not support it.'
test_forward_mode_AD_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fill__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_flip_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_flipud_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_float_power_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_floor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_fmax_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_fmin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with fmod that does not support it.'
test_forward_mode_AD_fmod_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_frac_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::frexp.Tensor_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::frexp.Tensor_out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_gather_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_gradient_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_hstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hypot.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hypot.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::igamma.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::igamma.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::igammac.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::igammac.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_index_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_index_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_index_select_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_inner_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with inverse that does not support it.'
test_forward_mode_AD_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_kron_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_lerp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::lu_unpack' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::lu_unpack' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with linalg_eig that does not support it.'
test_forward_mode_AD_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_eigh' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_eigh' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with linalg_eig that does not support it.'
test_forward_mode_AD_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with linalg_householder_product that does not support it.'
test_forward_mode_AD_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with linalg_qr that does not support it.'
test_forward_mode_AD_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with linalg_slogdet that does not support it.'
test_forward_mode_AD_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::_linalg_inv_out_helper_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_linalg_inv_out_helper_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_log10_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_log1p_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_log2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _log_softmax that does not support it.'
test_forward_mode_AD_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _log_softmax that does not support it.'
test_forward_mode_AD_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_log_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logaddexp2.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logaddexp2.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logaddexp.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logaddexp.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::_logcumsumexp' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_logcumsumexp' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_logdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with logdet that does not support it.'
test_forward_mode_AD_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logit' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logit' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with logsumexp that does not support it.'
test_forward_mode_AD_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::lu_unpack' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::lu_unpack' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_lu_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_matmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with mv that does not support it.'
test_forward_mode_AD_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::matrix_exp' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::matrix_exp' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_maximum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_mean_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with median that does not support it.'
test_forward_mode_AD_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_minimum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_mm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_mode_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_movedim_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_msort_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_mul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_mv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with mv that does not support it.'
test_forward_mode_AD_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nan_to_num.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nan_to_num.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with nansum that does not support it.'
test_forward_mode_AD_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::nanmedian' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nanmedian' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with sort that does not support it.'
test_forward_mode_AD_nansum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with nansum that does not support it.'
test_forward_mode_AD_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_narrow_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with neg that does not support it.'
test_forward_mode_AD_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with gelu that does not support it.'
test_forward_mode_AD_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hardswish' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hardswish' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with upsample_bicubic2d that does not support it.'
test_forward_mode_AD_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with log_sigmoid_forward that does not support it.'
test_forward_mode_AD_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with mse_loss that does not support it.'
test_forward_mode_AD_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with nll_loss2d_forward that does not support it.'
test_forward_mode_AD_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with norm that does not support it.'
test_forward_mode_AD_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with constant_pad_nd that does not support it.'
test_forward_mode_AD_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with reflection_pad1d that does not support it.'
test_forward_mode_AD_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::replication_pad1d.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::replication_pad1d.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with relu that does not support it.'
test_forward_mode_AD_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with softplus that does not support it.'
test_forward_mode_AD_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with norm that does not support it.'
test_forward_mode_AD_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::count_nonzero.dim_IntList' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::count_nonzero.dim_IntList' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with norm that does not support it.'
test_forward_mode_AD_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_outer_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_permute_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::polar.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::polar.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_positive_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_pow_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_prod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with prod that does not support it.'
test_forward_mode_AD_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_put_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with put_ that does not support it.'
test_forward_mode_AD_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with linalg_qr that does not support it.'
test_forward_mode_AD_quantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with sort that does not support it.'
test_forward_mode_AD_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with rad2deg that does not support it.'
test_forward_mode_AD_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_ravel_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_remainder_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_repeat_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_reshape_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_roll_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_rot90_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_round_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with rsub that does not support it.'
test_forward_mode_AD_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with rsub that does not support it.'
test_forward_mode_AD_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with scatter_add that does not support it.'
test_forward_mode_AD_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_scatter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with scatter that does not support it.'
test_forward_mode_AD_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_select_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sgn_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_sign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sinc.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sinc.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _softmax that does not support it.'
test_forward_mode_AD_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _softmax that does not support it.'
test_forward_mode_AD_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with solve that does not support it.'
test_forward_mode_AD_sort_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_entr.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_entr.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::special_i0e.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i0e.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::special_i1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::special_i1e.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_i1e.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erf that does not support it.'
test_forward_mode_AD_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::special_ndtri.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_ndtri.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::special_xlog1py.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_xlog1py.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::special_zeta.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_zeta.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_forward_mode_AD_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with split_with_sizes that does not support it.'
test_forward_mode_AD_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with split_with_sizes that does not support it.'
test_forward_mode_AD_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with split that does not support it.'
test_forward_mode_AD_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_square_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_stack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with stack that does not support it.'
test_forward_mode_AD_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_std_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with std that does not support it.'
test_forward_mode_AD_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sub_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_sum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _svd_helper that does not support it.'
test_forward_mode_AD_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_symeig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with symeig that does not support it.'
test_forward_mode_AD_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_t_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_forward_mode_AD_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_take_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_tan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_tanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_tile_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::to_sparse' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::to_sparse' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_topk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with topk that does not support it.'
test_forward_mode_AD_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_trace_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_transpose_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_trapz_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with triangular_solve that does not support it.'
test_forward_mode_AD_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_tril_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_triu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_trunc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::unfold_backward' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::unfold_backward' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_forward_mode_AD_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_var_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with var that does not support it.'
test_forward_mode_AD_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::vdot' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::vdot' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../aten/src/ATen/autocast_mode.cpp:470 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_view_as_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_view_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_vstack_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_forward_mode_AD_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_where_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with _s_where that does not support it.'
test_forward_mode_AD_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::xlogy.OutTensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::xlogy.OutTensor' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_forward_mode_AD_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_forward_mode_AD_zero__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD___getitem___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD___radd___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD___rmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD___rpow___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD___rsub___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_abs_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_acos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_acosh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_addmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_addmv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_addr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_amax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_amin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_inplace_forward_mode_AD_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_angle_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_asin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_asinh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_atan2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with atan2_ that does not support it.'
test_inplace_forward_mode_AD_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_atan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_atanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_bmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cdist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_ceil_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_chunk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_clamp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with clamp_ that does not support it.'
test_inplace_forward_mode_AD_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_clone_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_copysign_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cov_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cross_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_diff_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_digamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_dist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_dstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_einsum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_erf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erf_ that does not support it.'
test_inplace_forward_mode_AD_erfc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erfc_ that does not support it.'
test_inplace_forward_mode_AD_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with erfinv_ that does not support it.'
test_inplace_forward_mode_AD_exp2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_exp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_expand_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_expm1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fill__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_flip_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_flipud_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_float_power_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_floor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_fmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with fmod_ that does not support it.'
test_inplace_forward_mode_AD_fmod_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_forward_mode_AD_frac_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_gather_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_gradient_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_hstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hypot.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hypot.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_index_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_forward_mode_AD_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_index_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_index_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_inner_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_kron_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_lerp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_log10_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_log1p_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_log2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_log_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_logdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logit_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logit_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_lu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_matmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_maximum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_minimum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_mm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_mode_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_movedim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_msort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_mul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_mv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_forward_mode_AD_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_forward_mode_AD_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nan_to_num.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nan_to_num.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nansum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_narrow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with neg_ that does not support it.'
test_inplace_forward_mode_AD_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_forward_mode_AD_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_outer_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_permute_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_forward_mode_AD_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_forward_mode_AD_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_forward_mode_AD_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_forward_mode_AD_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_positive_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_pow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_prod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_put_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with put_ that does not support it.'
test_inplace_forward_mode_AD_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_quantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with rad2deg_ that does not support it.'
test_inplace_forward_mode_AD_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_ravel_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_remainder_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_forward_mode_AD_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not_implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_repeat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_reshape_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_roll_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_rot90_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_round_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with scatter_add_ that does not support it.'
test_inplace_forward_mode_AD_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_scatter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'not_implemented: Trying to use forward AD with scatter_ that does not support it.'
test_inplace_forward_mode_AD_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sgn_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_forward_mode_AD_sign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sinc.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sinc.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_sort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_forward_mode_AD_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_forward_mode_AD_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_square_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_stack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_std_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sub_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_sum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_symeig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_t_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_take_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_tan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_tanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_tile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_topk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_trace_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_transpose_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_trapz_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_tril_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_triu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_trunc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_forward_mode_AD_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_var_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_view_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_view_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_vstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_where_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_forward_mode_AD_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::xlogy.OutTensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::xlogy.OutTensor' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_forward_mode_AD_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_forward_mode_AD_zero__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad___getitem___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad___radd___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad___rmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad___rpow___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad___rsub___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_abs_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_acos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_acosh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_addmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_addmv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_addr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_amax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_amin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_inplace_grad_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_angle_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_asin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_asinh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_atan2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_atan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_atanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_bmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cdist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_ceil_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_chunk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_clamp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_clone_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_copysign_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cov_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cross_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_diff_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_digamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_dist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_dstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_einsum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_erf_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_erfc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_exp2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_exp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_expand_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_expm1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fill__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_flip_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_flipud_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_float_power_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_floor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_fmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_fmod_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_frac_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_gather_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_gradient_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_hstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hypot.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hypot.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_index_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_index_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_index_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_inner_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_kron_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_lerp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_log10_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_log1p_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_log2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_log_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_logdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logit_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logit_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_lu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_matmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_maximum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_minimum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_mm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_mode_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_movedim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_msort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_mul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_mv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_grad_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_grad_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nan_to_num.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nan_to_num.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nansum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_narrow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_outer_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_permute_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_grad_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_grad_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_grad_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_grad_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_positive_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_pow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_prod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_quantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_ravel_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_remainder_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_repeat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_reshape_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_roll_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_rot90_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_round_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sgn_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_sign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sinc.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sinc.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_sort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_grad_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_grad_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_square_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_stack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_std_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sub_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_sum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_symeig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_t_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_take_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_tan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_tanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_tile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_topk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_trace_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_transpose_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_trapz_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_tril_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_triu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_trunc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_grad_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_var_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_view_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_view_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_vstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_where_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_grad_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::xlogy.OutTensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::xlogy.OutTensor' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_grad_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_grad_zero__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad___getitem___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad___getitem___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad___radd___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad___radd___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad___rdiv___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad___rdiv___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad___rmatmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad___rmod___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad___rmul___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad___rmul___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad___rpow___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad___rpow___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad___rsub___xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad___rsub___xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_abs_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_abs_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_acos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_acos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_acosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_acosh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_add_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_addbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_addcdiv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_addcdiv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_addcmul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_addcmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_addmm_decomposed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_addmm_decomposed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_addmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_addmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_addmv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_addr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_addr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_all_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_all_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_amax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_amin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_aminmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped "onlyOnCPUAndCUDA: doesn't run on xpu"
test_inplace_gradgrad_angle_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_angle_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_any_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_any_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_argmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_argmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_asin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_asin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_asinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_asinh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_atan2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_atan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_atan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_atanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_atanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_baddbmm_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_bitwise_left_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_bitwise_right_shift_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_block_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_block_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_bmm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_bmm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_broadcast_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_broadcast_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_broadcast_to_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_broadcast_to_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cdist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_ceil_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_cholesky_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cholesky_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_chunk_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_chunk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_clamp_scalar_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_clamp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_clone_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_clone_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_conj_physical_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_conj_physical_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_contiguous_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_contiguous_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_copysign_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_corrcoef_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_corrcoef_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cos_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cos_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_cosh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cosh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_count_nonzero_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_count_nonzero_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_cov_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cov_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cross_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cross_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cummax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cummin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_cumprod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cumprod_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_cumsum_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_cumulative_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_cumulative_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_deg2rad_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_diag_embed_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_diag_embed_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_diag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_diag_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_diagonal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_diagonal_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_diff_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_diff_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_digamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_dist_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_dist_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_div_floor_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_div_floor_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_div_no_rounding_mode_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_div_no_rounding_mode_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_div_trunc_rounding_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_div_trunc_rounding_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_dsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_dsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_dstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_dstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_einsum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_einsum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_eq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_eq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_erf_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_erfc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_erfinv_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_exp2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_exp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_exp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_expand_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_expand_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_expand_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_expand_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_expm1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_fft_fft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_fft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_fftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_hfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_ifft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_ifftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_irfft_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_irfftn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fill__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fill__xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_flip_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_flip_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fliplr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_fliplr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_flipud_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_flipud_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_float_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_float_power_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_floor_divide_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_floor_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_fmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fmin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_fmod_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_fmod_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_frac_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_frexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_gather_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_gather_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_ge_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_geqrf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_geqrf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_gradient_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_gradient_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_gt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_hsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_hsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_hstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_hstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_hypot_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::hypot.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::hypot.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_i0_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:9548 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:10664 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_igamma_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_igamma_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_igammac_grad_other_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_igammac_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_imag_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_index_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_index_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_index_copy_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_index_copy_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_index_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_index_fill_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_index_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_index_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_index_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_index_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_inner_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_inner_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_inverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_inverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_isin_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_kron_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_kron_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_kthvalue_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_le_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_lerp_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_lerp_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_lgamma_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_linalg_cholesky_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_cholesky_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_cholesky_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_cholesky_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_cond_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_cond_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_det_singular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_det_singular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_det_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_det_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_eig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_eig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_eigh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_eigh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_eigvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_eigvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_eigvalsh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_eigvalsh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_householder_product_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_inv_ex_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_inv_ex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_inv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_inv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_lstsq_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_lstsq_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_linalg_matrix_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_matrix_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_matrix_power_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_matrix_power_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_linalg_matrix_rank_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_matrix_rank_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_linalg_multi_dot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_multi_dot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_pinv_hermitian_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_pinv_hermitian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_pinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_pinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_slogdet_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_slogdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_svdvals_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_tensorinv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_tensorinv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_linalg_vector_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_linalg_vector_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_log10_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_log10_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_log1p_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_log2_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_log2_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_log_softmax_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_log_softmax_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_log_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_log_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_log_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_logaddexp2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_logaddexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_logcumsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_logdet_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_logical_not_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_logical_not_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_logit_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::logit_' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::logit_' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8932 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9308 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_logsumexp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_lt_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_lu_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_lu_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_lu_unpack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_lu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_lu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_masked_fill_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_masked_fill_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_masked_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_masked_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_masked_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_masked_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_matmul_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_matrix_exp_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_max_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_max_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_max_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_maximum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_median_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_meshgrid_list_of_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_meshgrid_variadic_tensors_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_min_binary_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_min_reduction_no_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_min_reduction_with_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_minimum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_mm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_mm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_mode_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_movedim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_movedim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_msort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_mul_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_mul_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_mv_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_mv_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_gradgrad_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_gradgrad_nan_to_num_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::nan_to_num.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::nan_to_num.out' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:2399 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_nanmean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nanmedian_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nanquantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nansum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_narrow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_narrow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_ne_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_ne_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_neg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_nextafter_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_cosine_similarity_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_nn_functional_gelu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_hardshrink_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_hardswish_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_hardtanh_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_leaky_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_logsigmoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_mse_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_nll_loss_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_normalize_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_pad_circular_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_nn_functional_pad_circular_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_pad_constant_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_nn_functional_pad_constant_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_pad_reflect_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_pad_replicate_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_nn_functional_pad_replicate_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_relu6_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_relu_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_norm_fro_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_norm_fro_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_norm_inf_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_norm_inf_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_norm_nuc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_norm_nuc_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_norm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_norm_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_ormqr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_ormqr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_outer_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_outer_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_permute_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_permute_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_pinverse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_pinverse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_polar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_polygamma_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_polygamma_polygamma_n_1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_gradgrad_polygamma_polygamma_n_2_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_gradgrad_polygamma_polygamma_n_3_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_gradgrad_polygamma_polygamma_n_4_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped!'
test_inplace_gradgrad_positive_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_positive_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_pow_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_pow_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_prod_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_prod_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_put_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_put_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_qr_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_qr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_quantile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_rad2deg_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_ravel_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_ravel_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_reciprocal_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_reciprocal_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_remainder_autodiffed_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_remainder_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_renorm_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_renorm_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::linalg_vector_norm' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_vector_norm' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:10141 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11560 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_repeat_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_repeat_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_reshape_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_reshape_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_reshape_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_reshape_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_resize__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_resize__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_resize_as__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_resize_as__xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_resolve_conj_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_resolve_conj_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_resolve_neg_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_resolve_neg_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_roll_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_roll_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_rot90_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_rot90_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_round_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_rsqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_rsqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_rsub_rsub_scalar_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_rsub_rsub_tensor_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_rsub_rsub_tensor_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_scatter_add_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_scatter_add_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_scatter_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_scatter_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_select_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_select_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_sgn_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sgn_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_sigmoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sigmoid_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_sign_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_signbit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_sin_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sin_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_sinc_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sinc_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::sinc.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sinc.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:10491 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:11425 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_sinh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sinh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_softmax_with_dtype_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_softmax_with_dtype_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_softmax_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_sort_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_entr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU) ... ERROR
test_inplace_gradgrad_special_i0e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_i1_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_i1e_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_ndtr_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_ndtri_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_xlog1py_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_zeta_grad_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_special_zeta_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! autograd not supported.'
test_inplace_gradgrad_split_list_args_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_split_list_args_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_split_with_sizes_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_split_with_sizes_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_sqrt_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sqrt_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_square_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_square_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_squeeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_squeeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_stack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_stack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_std_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_std_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_std_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_std_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_sub_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sub_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_sum_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_sum_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_svd_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_svd_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_symeig_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_symeig_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_t_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_t_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_take_along_dim_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_take_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_take_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_tan_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_tan_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_tanh_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_tanh_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_tensor_split_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_tensor_split_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_tensordot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_tensordot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_tile_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_tile_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_to_sparse_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_to_sparse_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_topk_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_trace_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_trace_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_transpose_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_transpose_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_trapezoid_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_trapezoid_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_trapz_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_trapz_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_triangular_solve_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_tril_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_tril_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_triu_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_triu_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_true_divide_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_true_divide_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_trunc_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_unfold_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_unfold_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_unsqueeze_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_unsqueeze_xpu_float64 (__main__.TestGradientsXPU) ... ok
test_inplace_gradgrad_var_mean_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_var_mean_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_var_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_var_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_vdot_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_vdot_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_view_as_complex_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_view_as_real_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_view_as_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_view_as_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_view_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_view_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_vsplit_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_vsplit_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_vstack_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_vstack_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_where_xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_where_xpu_float64 (__main__.TestGradientsXPU) ... skipped 'Skipped! Operation does not support inplace autograd.'
test_inplace_gradgrad_xlogy_xpu_float64 (__main__.TestGradientsXPU) ... skipped "not implemented: Could not run 'aten::xlogy.OutTensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::xlogy.OutTensor' is only available for these backends: [CPU, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:18433 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:12703 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2505 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8878 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10257 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
test_inplace_gradgrad_zero__xpu_complex128 (__main__.TestGradientsXPU) ... skipped 'dtype not support on XPU'
test_inplace_gradgrad_zero__xpu_float64 (__main__.TestGradientsXPU) ... ok

======================================================================
ERROR: test_fn_fail_gradgrad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_fn_grad_baddbmm_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: self dim 0 must match batch1 dim 0

======================================================================
ERROR: test_fn_grad_cdist_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Function CdistBackward0 returned an invalid gradient at index 0 - got [2, 3, 2] but expected shape compatible with [2, 2, 3, 2]

======================================================================
ERROR: test_fn_grad_cholesky_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 5878, in gradcheck_wrapper_hermitian_input
    return op(input + input.conj().transpose(-2, -1), *args, **kwargs)
RuntimeError: input must be 2-d matrix, input shape=[0, 5, 5]

======================================================================
ERROR: test_fn_grad_cov_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(nan, device='xpu:0')
analytical:tensor(nan, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]], device='xpu:0')
Analytical:
tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]], device='xpu:0')

The max per-element difference (slow mode) is: nan.


======================================================================
ERROR: test_fn_grad_fft_fft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(nan, device='xpu:0')
analytical:tensor(-0.0286, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.3302e+00,
         -1.2399e+01, -1.4588e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -8.3157e+00,
          2.6482e-02,  3.2956e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.1101e+00,
          2.8041e+01,  3.2974e+01],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1892e+02,
          4.3809e+02, -1.5326e+03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.4217e+02,
          9.1426e+01, -3.6406e+02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  9.5510e+01,
         -7.8380e+02, -1.2057e+02]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: nan.


======================================================================
ERROR: test_fn_grad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.0309, device='xpu:0')
analytical:tensor(0.0571, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.9511,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.9511,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.9511]],
       device='xpu:0')
Analytical:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.5878, -0.5878, -0.9511]],
       device='xpu:0')

The max per-element difference (slow mode) is: 1.9021130325904991.


======================================================================
ERROR: test_fn_grad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-173325.7617, device='xpu:0')
analytical:tensor(0.2596, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.3162, 0.0000, 0.0000,  ..., -0.0000, -0.0000, -0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1954, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 0.3162,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.3162,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.3162,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.3162,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.3162,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3162]],
       device='xpu:0')

The max per-element difference (slow mode) is: 3317267236.706397.


======================================================================
ERROR: test_fn_grad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(7.0517e+259, device='xpu:0')
analytical:tensor(0.0286, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.7909e+01,
          2.8778e+01,  2.2053e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.1979e+01,
          1.8924e+01,  3.8828e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.2125e+01,
          1.6113e+03, -1.0232e+03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.7995e+00,
          1.2969e+06,  5.0067e+05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.2969e+06,
         -1.7105e+06,  5.7409e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.7105e+06,
          8.6736e-16,  3.3718e+05]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 4.2376406522801225e+198.


======================================================================
ERROR: test_fn_grad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.0078, device='xpu:0')
analytical:tensor(0.0062, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1176],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.1176],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.1902]],
       device='xpu:0')
Analytical:
tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1902, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1902, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1902]],
       device='xpu:0')

The max per-element difference (slow mode) is: 0.3804226065181664.


======================================================================
ERROR: test_fn_grad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.0022, device='xpu:0')
analytical:tensor(-0.0108, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.1176,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1176,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1176]],
       device='xpu:0')
Analytical:
tensor([[-0., -0., -0.,  ..., -0., -0., -0.],
        [-0., -0., -0.,  ..., -0., -0., -0.],
        [-0., -0., -0.,  ..., -0., -0., -0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 0.3804226065176113.


======================================================================
ERROR: test_fn_grad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(223107.3061, device='xpu:0')
analytical:tensor(0.2596, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 3.1623e-01,  0.0000e+00,  0.0000e+00,  ..., -1.9374e+01,
          3.1372e-02,  3.2721e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  9.6870e+03,
         -1.5686e+01, -1.6360e+04],
        [ 0.0000e+00,  1.9544e-01,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='xpu:0')
Analytical:
tensor([[ 0.3162,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.3162,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.3162,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.3162,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.3162,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3162]],
       device='xpu:0')

The max per-element difference (slow mode) is: nan.


======================================================================
ERROR: test_fn_grad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-2.3604e+266, device='xpu:0')
analytical:tensor(0.0460, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.1826,  0.1826,  0.1826,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.2954,  0.1128,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.1128, -0.2954,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 0.1826,  0.1826,  0.1826,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.2954,  0.1128,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.1128, -0.2954,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')

The max per-element difference (slow mode) is: 9.470678299388537e+264.


======================================================================
ERROR: test_fn_grad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.0110, device='xpu:0')
analytical:tensor(0.0538, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.5878, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5878, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5878]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.902113032590277.


======================================================================
ERROR: test_fn_grad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.0110, device='xpu:0')
analytical:tensor(0.0538, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.5878, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5878, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5878]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.902113032589611.


======================================================================
ERROR: test_fn_grad_fmod_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_fn_grad_index_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_fn_grad_index_copy_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: src tensor dim should be > 0 and < 12

======================================================================
ERROR: test_fn_grad_index_select_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src tensor dim should be > 0 and < 12

======================================================================
ERROR: test_fn_grad_linalg_det_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-3004.0794, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 2024.5043],
        [-2952.1040],
        [-4767.5594],
        [   91.3089],
        [-3193.3413],
        [ 2532.3503],
        [-4908.4608],
        [-4141.4415],
        [ 1380.4624],
        [-3094.4731],
        [-2822.6365],
        [ 2931.9615],
        [ 2705.7345],
        [  373.5639],
        [ 1799.1591],
        [ 1440.1771],
        [-1551.0118],
        [-2570.5254],
        [-1148.6267],
        [-2424.4582],
        [ -234.9747],
        [ 1637.4449],
        [  615.0078],
        [ -561.3978],
        [ -119.9137]], device='xpu:0')
Analytical:
tensor([[0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.],
        [0.]], device='xpu:0')

The max per-element difference (slow mode) is: 4908.460837365965.


======================================================================
ERROR: test_fn_grad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.4633, device='xpu:0')
analytical:tensor(-0.9201, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.0031, -0.0752, -0.2523],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.2939, -2.6828],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.5120,  1.1476,  1.5513],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.9628,  3.6720],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')

The max per-element difference (slow mode) is: 33.0927168142661.


======================================================================
ERROR: test_fn_grad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.1449, device='xpu:0')
analytical:tensor(0.1306, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.4859, -0.4339, -0.2932,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -0.6798,  0.1056,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.8253,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.4738,  1.5542, -1.2115],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.5251, -0.2071],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 4.8585e-01, -4.3389e-01, -2.9324e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-0.0000e+00, -6.7980e-01,  1.0556e-01,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -8.2533e-01,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.1831e-01,
          1.8038e-01,  4.5738e-01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          3.6201e-01, -1.4277e-01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -4.4409e-16]], device='xpu:0')

The max per-element difference (slow mode) is: 2.342955954234798.


======================================================================
ERROR: test_fn_grad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: mat1 and mat2 shapes cannot be multiplied (0x0 and 5x5)

======================================================================
ERROR: test_fn_grad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 528, in _check_helper
    samples = op.sample_inputs(device, dtype, requires_grad=True, include_conjugated_inputs=include_conjugated_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 691, in sample_inputs
    samples = self.sample_inputs_func(self, device, dtype, requires_grad, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 3833, in sample_inputs_lu_unpack
    return list(generate_samples())
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 3815, in generate_samples
    lu_data, pivots = lu_sample.input.lu()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 450, in lu
    LU, pivots, infos = torch._lu_with_info(self, pivot=pivot, check_errors=(not get_infos))
RuntimeError: A must be batches of square matrices, but they are 5 by 3 matrices

======================================================================
ERROR: test_fn_grad_lu_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 420, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1614, in _lu_with_infos
    result = _lu_impl(A, pivot, get_infos, out)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1595, in _lu_impl
    return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
RuntimeError: A must be batches of square matrices, but they are 5 by 3 matrices

======================================================================
ERROR: test_fn_grad_mode_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Input Tensor's dimension must be within (0, 2]

======================================================================
ERROR: test_fn_grad_nanquantile_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.4236, device='xpu:0')
analytical:tensor(0.0661, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.000000000000334.


======================================================================
ERROR: test_fn_grad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1131, in adaptive_avg_pool2d
    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: dpcpp_avg_pool2d operator does not support divisor

======================================================================
ERROR: test_fn_grad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(4576327.5805, device='xpu:0')
analytical:tensor(0.6753, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 8.7295e+03,  0.0000e+00,  4.7328e+03,  5.1740e+03,  0.0000e+00],
        [ 0.0000e+00, -5.4717e+03, -4.7328e+03,  5.1740e+03,  0.0000e+00],
        [ 0.0000e+00,  5.4727e+03,  4.7338e+03,  5.1740e+03, -3.8064e+03],
        [ 0.0000e+00,  0.0000e+00,  4.7328e+03, -5.1730e+03,  0.0000e+00],
        [ 8.7285e+03, -5.4727e+03, -4.7328e+03,  0.0000e+00,  2.0000e+00]],
       device='xpu:0')
Analytical:
tensor([[2., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 2.]], device='xpu:0')

The max per-element difference (slow mode) is: 8728.545429740003.


======================================================================
ERROR: test_fn_grad_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 544, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-15.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=<tol>` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=<tol>`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=<tol>`


======================================================================
ERROR: test_fn_grad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3718, in interpolate
    return adaptive_avg_pool1d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.6994, device='xpu:0')
analytical:tensor(-0.1465, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_fn_grad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.6333, device='xpu:0')
analytical:tensor(2.0000e-06, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='xpu:0')
Analytical:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0000000000000009.


======================================================================
ERROR: test_fn_grad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3710, in interpolate
    return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.7666, device='xpu:0')
analytical:tensor(-0.1877, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_fn_grad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 969, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2347, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 420, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 694, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_grad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Native API failed. Native API returns: -50 (CL_INVALID_ARG_VALUE) -50 (CL_INVALID_ARG_VALUE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: Native API failed. Native API returns: -50 (CL_INVALID_ARG_VALUE) -50 (CL_INVALID_ARG_VALUE)

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_fn_grad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 4491, in unfold
    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))
RuntimeError: Expected non-empty 3D or 4D input tensor, but got input of size [0, 1, 5, 5]

======================================================================
ERROR: test_fn_grad_qr_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.1168, device='xpu:0')
analytical:tensor(0.3782, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-0.3105,  0.1936, -0.0008,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.2233,  0.2456,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0235,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.0407, -0.0724,  0.0318],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0345,  0.0222],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')
Analytical:
tensor([[-3.1051e-01,  1.9357e-01, -8.1138e-04,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  2.2326e-01,  2.4559e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.3533e-02,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0680e-01,
          9.3596e-02, -1.4532e-01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          7.5485e-02,  4.8618e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  1.1102e-16]], device='xpu:0')

The max per-element difference (slow mode) is: 1.5607671153099965.


======================================================================
ERROR: test_fn_grad_quantile_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.3948, device='xpu:0')
analytical:tensor(0.2240, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1.],
        [0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.000000000000334.


======================================================================
ERROR: test_fn_grad_remainder_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_fn_grad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: For integral input tensors, argument alpha must not be a floating point number.

======================================================================
ERROR: test_fn_grad_scatter_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_fn_grad_scatter_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_fn_grad_sigmoid_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_fn_grad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_fn_grad_std_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.0236, device='xpu:0')
analytical:tensor(0.0314, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-0.3129,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0081,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1152,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.1061,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.3581,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.2988]],
       device='xpu:0')
Analytical:
tensor([[-0.4172, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],
        [ 0.0000,  0.0108,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1537,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.1415,  0.0000,  0.0000],
        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.4775, -0.0000],
        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.3985]],
       device='xpu:0')

The max per-element difference (slow mode) is: 0.14325367562639546.


======================================================================
ERROR: test_fn_grad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.

======================================================================
ERROR: test_fn_grad_topk_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_fn_grad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(191.8567, device='xpu:0')
analytical:tensor(190.8740, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-1.3494e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-6.6690e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-5.5380e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 1.3649e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-1.5724e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 1.1464e+01,  2.3277e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 9.5193e+00,  1.9328e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.3462e+01, -4.7637e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 2.7028e+00,  5.4879e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-3.1661e+00, -1.0866e+02, -5.8359e+01,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 7.8034e+00,  2.6781e+02,  1.4384e+02,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-8.9896e-01, -3.0852e+01, -1.6570e+01,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 7.4217e+03,  1.7287e+03,  3.0526e+03, -7.7719e+03,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-8.4294e+02, -1.9635e+02, -3.4670e+02,  8.8272e+02,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 4.9037e+02,  2.6920e+02,  2.1914e+02, -5.0665e+02,  3.3137e+01,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],
       device='xpu:0')
Analytical:
tensor([[-1.3494e+02, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-6.6690e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-5.5380e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 1.3649e+02, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-1.5724e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 1.1463e+01,  2.3275e+02, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 9.5193e+00,  1.9328e+02, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-2.3462e+01, -4.7637e+02, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 2.7028e+00,  5.4879e+01, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-3.1661e+00, -1.0866e+02, -5.8359e+01, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 7.8034e+00,  2.6781e+02,  1.4384e+02, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-8.9896e-01, -3.0852e+01, -1.6570e+01, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 7.3171e+03,  1.7044e+03,  3.0096e+03, -7.6624e+03, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-8.4294e+02, -1.9635e+02, -3.4670e+02,  8.8272e+02, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 4.9036e+02,  2.6919e+02,  2.1914e+02, -5.0664e+02,  3.3136e+01,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -5.3268e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -1.8936e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -1.3442e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -9.1704e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -2.2393e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -1.8936e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -1.3442e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -9.1704e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -2.2393e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -1.3442e+00, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -9.1704e-01, -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -2.2393e+00, -0.0000e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -9.1704e-01, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -2.2393e+00, -0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,
         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -2.2393e+00]],
       device='xpu:0')

The max per-element difference (slow mode) is: 109.50205982639545.


======================================================================
ERROR: test_fn_grad_var_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 593, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.1582, device='xpu:0')
analytical:tensor(-0.2109, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-4.3871,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000, -2.3702,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1319,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  2.6664,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.1328,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.1391]],
       device='xpu:0')
Analytical:
tensor([[-5.8495, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],
        [-0.0000, -3.1602, -0.0000,  ..., -0.0000, -0.0000, -0.0000],
        [ 0.0000,  0.0000,  0.1759,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  3.5552,  0.0000,  0.0000],
        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -1.5104, -0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.5188]],
       device='xpu:0')

The max per-element difference (slow mode) is: 1.7644409540779424.


======================================================================
ERROR: test_fn_gradgrad_baddbmm_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: self dim 0 must match batch1 dim 0

======================================================================
ERROR: test_fn_gradgrad_cholesky_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 5878, in gradcheck_wrapper_hermitian_input
    return op(input + input.conj().transpose(-2, -1), *args, **kwargs)
RuntimeError: input must be 2-d matrix, input shape=[0, 5, 5]

======================================================================
ERROR: test_fn_gradgrad_fft_fft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 568, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(255307.5581, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-1.5705e-01, -5.7814e-01, -1.0265e+00,  ...,  1.3516e-01,
          1.0990e+02, -1.5138e+02],
        [ 1.1625e+00,  6.7772e-01,  2.4519e-01,  ..., -1.0513e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0323e+00, -6.6461e-01,  4.6936e-01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 834.420450304306.


======================================================================
ERROR: test_fn_gradgrad_fft_fftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 568, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: FFT_INVALID_DESCRIPTOR

======================================================================
ERROR: test_fn_gradgrad_fft_hfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.7711, device='xpu:0')
analytical:tensor(1.0107, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.0000]],
       device='xpu:0')
Analytical:
tensor([[1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.7071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='xpu:0')

The max per-element difference (slow mode) is: 3.9999999999997797.


======================================================================
ERROR: test_fn_gradgrad_fft_ifft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.1300-0.0034j, device='xpu:0')
analytical:tensor(0.1619+0.0007j, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.2000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.2000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.2000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        ...,
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j, -0.1618-0.1176j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j, -0.1618+0.1176j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0618+0.1902j]], device='xpu:0')
Analytical:
tensor([[0.2000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,  ...,
         0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j],
        [0.0000-0.0000j, 0.2000-0.0000j, 0.0000-0.0000j,  ...,
         0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j],
        [0.0000-0.0000j, 0.0000-0.0000j, 0.2000-0.0000j,  ...,
         0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j],
        ...,
        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,  ...,
         0.0618+0.1902j, 0.0000-0.0000j, 0.0000-0.0000j],
        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,  ...,
         0.0000-0.0000j, 0.0618+0.1902j, 0.0000-0.0000j],
        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,  ...,
         0.0000-0.0000j, 0.0000-0.0000j, 0.0618+0.1902j]], device='xpu:0')

The max per-element difference (slow mode) is: 0.3804226065180669.


======================================================================
ERROR: test_fn_gradgrad_fft_ifftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 568, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: FFT_INVALID_DESCRIPTOR

======================================================================
ERROR: test_fn_gradgrad_fft_ihfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.1121+0.0022j, device='xpu:0')
analytical:tensor(0.1852+0.0037j, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.2000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.2000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.2000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        ...,
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j]],
       device='xpu:0')
Analytical:
tensor([[ 0.2000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.2000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.2000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        ...,
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
         -0.1618-0.1176j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j, -0.1618-0.1176j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j, -0.1618-0.1176j]], device='xpu:0')

The max per-element difference (slow mode) is: 0.38042260651808796.


======================================================================
ERROR: test_fn_gradgrad_fft_irfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.0964, device='xpu:0')
analytical:tensor(0.1263, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.1250,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.1250,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1250,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.1250,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1250,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1250]],
       device='xpu:0')
Analytical:
tensor([[0.1250, 0.1250, 0.1250,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0884, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='xpu:0')

The max per-element difference (slow mode) is: 0.5000000000000002.


======================================================================
ERROR: test_fn_gradgrad_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 568, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(2286290.8508, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 0.210818510678501.


======================================================================
ERROR: test_fn_gradgrad_fft_rfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.5606-0.0110j, device='xpu:0')
analytical:tensor(0.9258-0.0183j, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[1.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [1.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [1.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        ...,
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j]],
       device='xpu:0')
Analytical:
tensor([[ 1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        ...,
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
         -0.8090+0.5878j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j, -0.8090+0.5878j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j, -0.8090+0.5878j]], device='xpu:0')

The max per-element difference (slow mode) is: 1.9021130325903326.


======================================================================
ERROR: test_fn_gradgrad_fft_rfftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.5606-0.0110j, device='xpu:0')
analytical:tensor(0.9258-0.0183j, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[1.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [1.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [1.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        ...,
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j],
        [0.0000+0.j, 0.0000+0.j, 0.0000+0.j,  ..., 0.0000+0.j, 0.0000+0.j, 0.0000+0.j]],
       device='xpu:0')
Analytical:
tensor([[ 1.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  1.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  1.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        ...,
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
         -0.8090+0.5878j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j, -0.8090+0.5878j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j, -0.8090+0.5878j]], device='xpu:0')

The max per-element difference (slow mode) is: 1.9021130325904825.


======================================================================
ERROR: test_fn_gradgrad_fmod_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_fn_gradgrad_index_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_fn_gradgrad_index_copy_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: src tensor dim should be > 0 and < 12

======================================================================
ERROR: test_fn_gradgrad_index_select_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src tensor dim should be > 0 and < 12

======================================================================
ERROR: test_fn_gradgrad_linalg_det_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(409.9559, device='xpu:0')
analytical:tensor(1248.6727, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 4.5475e-10,  7.9581e-10,  4.5475e-10, -4.5475e-10,  0.0000e+00,
          9.0949e-10, -4.2631e+01, -6.6525e+02, -1.4008e+03, -3.6319e+02,
         -2.2737e-10, -5.4093e+02,  3.1998e+01, -1.3429e+02,  2.4427e+02,
          2.2737e-10, -3.6113e+02, -5.5654e+02, -3.1526e+02, -3.3616e+02,
         -9.0949e-10,  3.3487e+02, -2.3760e+02,  1.0379e+03,  7.4180e+02],
        [-1.3642e-09, -9.0949e-10, -6.8212e-10,  6.8212e-10,  6.8212e-10,
          4.2631e+01, -1.5916e-09, -1.6857e+02, -3.6851e+02, -1.1948e+02,
          5.4093e+02, -5.6843e-10,  2.2603e+02,  2.4762e+02, -1.6158e+02,
          3.6113e+02, -9.0949e-10, -1.0406e+01,  1.0234e+02, -2.4722e+02,
         -3.3487e+02,  1.3642e-09, -2.0072e+02,  1.1321e+02,  3.4929e+02],
        [ 0.0000e+00,  2.2737e-10,  0.0000e+00, -4.5475e-10,  2.2737e-10,
          6.6525e+02,  1.6857e+02,  0.0000e+00, -2.1155e+02, -4.2833e+02,
         -3.1998e+01, -2.2603e+02,  0.0000e+00, -7.0759e+01,  1.1162e+02,
          5.5654e+02,  1.0406e+01,  0.0000e+00,  1.6680e+02, -3.7130e+02,
          2.3760e+02,  2.0072e+02,  4.5475e-10,  5.4180e+02,  1.9679e+02],
        [ 0.0000e+00,  3.4106e-10,  2.2737e-10, -9.0949e-10,  4.5475e-10,
          1.4008e+03,  3.6851e+02,  2.1155e+02, -1.8190e-09, -7.8640e+02,
          1.3429e+02, -2.4762e+02,  7.0759e+01,  3.9790e-10,  7.1706e+01,
          3.1526e+02, -1.0234e+02, -1.6680e+02,  2.2737e-10, -3.1108e+02,
         -1.0379e+03, -1.1321e+02, -5.4180e+02,  9.0949e-10,  8.3185e+02],
        [ 2.2737e-09,  5.6843e-10,  0.0000e+00,  1.3642e-09, -4.5475e-10,
          3.6319e+02,  1.1948e+02,  4.2833e+02,  7.8640e+02, -4.5475e-10,
         -2.4427e+02,  1.6158e+02, -1.1162e+02, -7.1706e+01,  3.3751e-11,
          3.3616e+02,  2.4722e+02,  3.7130e+02,  3.1108e+02, -7.1054e-12,
         -7.4180e+02, -3.4929e+02, -1.9679e+02, -8.3185e+02,  1.7053e-10],
        [ 0.0000e+00,  4.2631e+01,  6.6525e+02,  1.4008e+03,  3.6319e+02,
          0.0000e+00, -4.5475e-10, -9.0949e-10, -9.0949e-10,  0.0000e+00,
          0.0000e+00, -1.0019e+03, -1.9431e+02, -7.7662e+02,  3.0721e+02,
          0.0000e+00, -6.3681e+02, -6.8225e+02,  1.2440e+02, -4.3137e+02,
          0.0000e+00,  5.5221e+02, -1.3385e+03, -1.5141e+01,  8.5744e+02],
        [-4.2631e+01, -2.2737e-10,  1.6857e+02,  3.6851e+02,  1.1948e+02,
         -1.8190e-09,  0.0000e+00,  0.0000e+00,  9.0949e-10,  0.0000e+00,
          1.0019e+03, -1.1369e-10,  5.8103e+02,  7.9669e+02, -2.0629e+02,
          6.3681e+02,  2.2737e-10,  2.0490e+02,  6.8820e+02, -3.1548e+02,
         -5.5221e+02,  4.5475e-10, -7.4555e+02, -5.6949e+02,  4.1579e+02],
        [-6.6525e+02, -1.6857e+02, -2.2737e-10,  2.1155e+02,  4.2833e+02,
          1.8190e-09, -6.8212e-10, -9.0949e-10, -1.8190e-09,  0.0000e+00,
          1.9431e+02, -5.8103e+02,  1.1369e-10, -2.9587e+02,  1.3815e+02,
          6.8225e+02, -2.0490e+02, -4.5475e-10,  7.7734e+02, -4.7679e+02,
          1.3385e+03,  7.4555e+02,  2.2737e-10,  1.3599e+03,  1.4984e+02],
        [-1.4008e+03, -3.6851e+02, -2.1155e+02, -4.5475e-10,  7.8640e+02,
         -9.0949e-10,  9.0949e-10, -9.0949e-10, -9.0949e-10,  2.2737e-10,
          7.7662e+02, -7.9669e+02,  2.9587e+02,  1.1369e-10,  8.4383e+01,
         -1.2440e+02, -6.8820e+02, -7.7734e+02,  0.0000e+00, -4.0455e+02,
          1.5141e+01,  5.6949e+02, -1.3599e+03,  9.0949e-10,  8.7287e+02],
        [-3.6319e+02, -1.1948e+02, -4.2833e+02, -7.8640e+02,  0.0000e+00,
          0.0000e+00, -1.1369e-09,  9.0949e-10,  0.0000e+00,  0.0000e+00,
         -3.0721e+02,  2.0629e+02, -1.3815e+02, -8.4383e+01, -3.0198e-11,
          4.3137e+02,  3.1548e+02,  4.7679e+02,  4.0455e+02,  3.1974e-11,
         -8.5744e+02, -4.1579e+02, -1.4984e+02, -8.7287e+02, -1.1369e-10],
        [ 2.7285e-09,  5.4093e+02, -3.1998e+01,  1.3429e+02, -2.4427e+02,
          3.6380e-09,  1.0019e+03,  1.9431e+02,  7.7662e+02, -3.0721e+02,
         -4.5475e-10,  3.4106e-10, -3.4106e-10,  0.0000e+00,  8.8818e-12,
          6.8212e-10,  4.0702e+02,  1.9537e+02,  1.8672e+02,  5.7747e+00,
         -1.8190e-09, -8.6343e+02,  1.3378e+02, -5.7692e+02,  5.0788e+01],
        [-5.4093e+02, -4.5475e-10, -2.2603e+02, -2.4762e+02,  1.6158e+02,
         -1.0019e+03, -4.5475e-10, -5.8103e+02, -7.9669e+02,  2.0629e+02,
          2.2737e-10, -1.1369e-10,  0.0000e+00,  5.6843e-11, -2.3093e-11,
         -4.0702e+02, -2.2737e-10, -3.1060e+02, -2.4118e+02,  1.8670e-01,
          8.6343e+02,  4.5475e-10,  3.0782e+02,  6.2743e+02, -4.0763e+01],
        [ 3.1998e+01,  2.2603e+02,  6.8212e-10,  7.0759e+01, -1.1162e+02,
         -1.9431e+02,  5.8103e+02,  9.0949e-10,  2.9587e+02, -1.3815e+02,
          2.2737e-10,  0.0000e+00,  0.0000e+00, -1.7053e-10,  0.0000e+00,
         -1.9537e+02,  3.1060e+02,  2.2737e-10,  2.6715e+01,  4.4963e+00,
         -1.3378e+02, -3.0782e+02, -4.5475e-10, -3.0289e+02,  2.4422e+01],
        [-1.3429e+02,  2.4762e+02, -7.0759e+01, -4.5475e-10, -7.1706e+01,
         -7.7662e+02,  7.9669e+02, -2.9587e+02,  9.0949e-10, -8.4383e+01,
          0.0000e+00,  1.1369e-10,  0.0000e+00,  0.0000e+00, -5.5067e-11,
         -1.8672e+02,  2.4118e+02, -2.6715e+01,  0.0000e+00,  3.5075e+00,
          5.7692e+02, -6.2743e+02,  3.0289e+02, -9.0949e-10,  9.6695e+00],
        [ 2.4427e+02, -1.6158e+02,  1.1162e+02,  7.1706e+01,  0.0000e+00,
          3.0721e+02, -2.0629e+02,  1.3815e+02,  8.4383e+01,  0.0000e+00,
          2.2737e-10,  1.1369e-10, -1.1369e-10,  1.7053e-10, -3.9080e-11,
         -5.7747e+00, -1.8670e-01, -4.4963e+00, -3.5075e+00,  6.7502e-11,
         -5.0788e+01,  4.0763e+01, -2.4422e+01, -9.6695e+00, -5.6843e-11],
        [ 2.7285e-09,  3.6113e+02,  5.5654e+02,  3.1526e+02,  3.3616e+02,
          3.6380e-09,  6.3681e+02,  6.8225e+02, -1.2440e+02,  4.3137e+02,
         -4.5475e-10, -4.0702e+02, -1.9537e+02, -1.8672e+02, -5.7747e+00,
          6.8212e-10,  4.5475e-10,  6.8212e-10,  6.8212e-10, -3.9080e-11,
         -1.8190e-09, -3.2445e+02, -8.7606e+02,  8.8774e+01, -8.7431e+01],
        [-3.6113e+02,  0.0000e+00,  1.0406e+01, -1.0234e+02,  2.4722e+02,
         -6.3681e+02, -2.2737e-10, -2.0490e+02, -6.8820e+02,  3.1548e+02,
          4.0702e+02,  2.2737e-10,  3.1060e+02,  2.4118e+02, -1.8670e-01,
          2.2737e-10,  0.0000e+00,  6.8212e-10,  6.8212e-10, -1.0658e-11,
          3.2445e+02,  4.5475e-10, -2.8999e+02,  3.6958e+02, -6.1963e+01],
        [-5.5654e+02, -1.0406e+01, -4.5475e-10, -1.6680e+02,  3.7130e+02,
         -6.8225e+02,  2.0490e+02,  0.0000e+00, -7.7734e+02,  4.7679e+02,
          1.9537e+02, -3.1060e+02, -1.1369e-10, -2.6715e+01, -4.4963e+00,
          2.2737e-10,  0.0000e+00,  2.2737e-10,  9.0949e-10,  0.0000e+00,
          8.7606e+02,  2.8999e+02, -2.2737e-10,  9.1857e+02, -8.9164e+01],
        [-3.1526e+02,  1.0234e+02,  1.6680e+02, -2.5011e-09,  3.1108e+02,
          1.2440e+02,  6.8820e+02,  7.7734e+02, -3.6380e-09,  4.0455e+02,
          1.8672e+02, -2.4118e+02,  2.6715e+01,  6.8212e-10, -3.5075e+00,
         -2.2737e-10, -2.2737e-10, -2.2737e-10, -9.0949e-10,  2.4869e-11,
         -8.8774e+01, -3.6958e+02, -9.1857e+02,  1.8190e-09, -8.2639e+01],
        [-3.3616e+02, -2.4722e+02, -3.7130e+02, -3.1108e+02,  0.0000e+00,
         -4.3137e+02, -3.1548e+02, -4.7679e+02, -4.0455e+02,  0.0000e+00,
          5.7747e+00,  1.8670e-01,  4.4963e+00,  3.5075e+00,  0.0000e+00,
          6.8212e-10,  4.5475e-10,  9.0949e-10,  6.8212e-10, -3.5527e-12,
          8.7431e+01,  6.1963e+01,  8.9164e+01,  8.2639e+01,  5.6843e-11],
        [ 4.5475e-10, -3.3487e+02,  2.3760e+02, -1.0379e+03, -7.4180e+02,
          9.0949e-10, -5.5221e+02,  1.3385e+03,  1.5141e+01, -8.5744e+02,
         -2.2737e-10,  8.6343e+02, -1.3378e+02,  5.7692e+02, -5.0788e+01,
          2.2737e-10,  3.2445e+02,  8.7606e+02, -8.8774e+01,  8.7431e+01,
         -9.0949e-10, -9.0949e-10, -4.5475e-10, -2.7285e-09,  5.6843e-11],
        [ 3.3487e+02, -1.1369e-10,  2.0072e+02, -1.1321e+02, -3.4929e+02,
          5.5221e+02, -4.5475e-10,  7.4555e+02,  5.6949e+02, -4.1579e+02,
         -8.6343e+02, -3.4106e-10, -3.0782e+02, -6.2743e+02,  4.0763e+01,
         -3.2445e+02, -4.5475e-10,  2.8999e+02, -3.6958e+02,  6.1963e+01,
          9.0949e-10,  4.5475e-10,  2.2737e-10,  9.0949e-10, -1.1369e-10],
        [-2.3760e+02, -2.0072e+02, -4.5475e-10, -5.4180e+02, -1.9679e+02,
         -1.3385e+03, -7.4555e+02,  0.0000e+00, -1.3599e+03, -1.4984e+02,
          1.3378e+02,  3.0782e+02,  0.0000e+00,  3.0289e+02, -2.4422e+01,
         -8.7606e+02, -2.8999e+02,  0.0000e+00, -9.1857e+02,  8.9164e+01,
          0.0000e+00,  4.5475e-10,  2.2737e-10,  9.0949e-10, -5.6843e-11],
        [ 1.0379e+03,  1.1321e+02,  5.4180e+02,  1.1369e-09, -8.3185e+02,
         -1.5141e+01, -5.6949e+02,  1.3599e+03,  1.8190e-09, -8.7287e+02,
         -5.7692e+02,  6.2743e+02, -3.0289e+02, -5.1159e-10, -9.6695e+00,
          8.8774e+01,  3.6958e+02,  9.1857e+02,  4.5475e-10,  8.2639e+01,
         -1.8190e-09, -4.5475e-10, -2.2737e-10, -9.0949e-10,  5.6843e-11],
        [ 7.4180e+02,  3.4929e+02,  1.9679e+02,  8.3185e+02, -2.2737e-10,
          8.5744e+02,  4.1579e+02,  1.4984e+02,  8.7287e+02, -4.5475e-10,
          5.0788e+01, -4.0763e+01,  2.4422e+01,  9.6695e+00, -8.8818e-12,
         -8.7431e+01, -6.1963e+01, -8.9164e+01, -8.2639e+01,  2.1316e-11,
         -9.0949e-10, -4.5475e-10, -2.2737e-10, -1.8190e-09,  0.0000e+00]],
       device='xpu:0')
Analytical:
tensor([[        nan,  4.3243e+02,  6.2178e+02,  1.1995e+03, -6.5805e+02,
          2.9874e+03,  1.0241e+03,  1.5160e+03,  1.2381e+03, -1.2710e+02,
         -2.7288e+02,  2.1874e+02,  8.6084e+02, -6.2751e+02,  6.3283e+02,
          6.7119e+02,  1.4901e+02,  8.6836e+01,  4.0955e+02, -3.4801e+01,
         -1.8896e+03, -8.1198e+02, -8.4417e+02, -1.1084e+03,  9.4936e+01],
        [        nan,  1.2070e+02,  1.7355e+02, -1.7309e+01, -1.8367e+02,
          7.2549e+02,  8.0653e+01,  8.8143e+01,  5.9781e+02, -1.0393e+03,
          1.7474e+02, -1.4007e+02, -5.5125e+02,  4.0183e+02, -4.0524e+02,
          6.4208e+02,  2.0376e+02,  3.7458e+02,  3.0269e+02, -4.3300e+02,
         -1.0637e+03, -1.1470e+02, -3.7198e+02, -4.8862e+02,  9.0500e+02],
        [        nan,  1.7355e+02,  2.4954e+02,  5.0223e+02, -2.6410e+02,
          1.8731e+03,  6.0954e+02,  8.9625e+02,  8.3394e+02, -2.7527e+02,
         -1.2436e+02,  9.9687e+01,  3.9231e+02, -2.8598e+02,  2.8840e+02,
          8.2037e+02,  2.1151e+02,  2.4606e+02,  4.5781e+02, -2.3439e+02,
         -5.0887e+02, -2.7171e+02, -2.4332e+02, -3.1947e+02, -1.0637e+02],
        [        nan,  2.2792e+02,  4.4214e+02,  5.4480e+02, -3.4683e+02,
          2.9378e+03,  8.7686e+02,  1.2520e+03,  1.4479e+03, -9.0668e+02,
         -8.1537e+01,  4.3585e+01,  2.5722e+02, -1.8750e+02,  1.1741e+02,
          6.9226e+02,  1.8177e+02,  2.2329e+02,  3.8153e+02, -2.1925e+02,
         -2.0839e+03, -5.0158e+02, -7.8062e+02, -1.1320e+03,  6.7368e+02],
        [        nan, -1.8367e+02, -2.6776e+02, -3.5049e+02,  2.7951e+02,
         -8.3753e+02, -2.3559e+02, -3.3846e+02, -4.3822e+02,  3.4485e+02,
          2.6043e+00, -1.3921e+00, -8.2157e+00,  5.9889e+00, -3.7501e+00,
          8.8016e+00,  2.8226e+00,  5.2745e+00,  4.1065e+00, -6.1272e+00,
          1.5178e+02,  3.9808e+01,  5.9136e+01,  8.1072e+01, -5.7716e+01],
        [        nan,  8.3064e+02,  7.8984e+02,  2.8888e+03, -8.3597e+02,
          5.1447e+03,  5.0520e+02,  2.4605e+03,  5.2642e+03, -1.5234e+03,
         -2.0805e+03, -2.1137e+03, -6.8287e+02, -1.0970e+03, -5.9282e+01,
          1.4086e+03,  4.1444e+02, -1.1122e+02,  1.0646e+03, -4.1710e+02,
         -3.8427e+03, -1.9974e+03, -3.1704e+03, -3.9319e+03,  1.1378e+03],
        [        nan,  2.3185e+02,  2.2046e+02,  8.0631e+02, -2.3333e+02,
          1.5137e+03,  1.6387e+02,  7.3625e+02,  1.5488e+03, -4.4821e+02,
          4.0502e+02, -2.9996e+02,  4.3728e+02,  7.0244e+02, -3.0843e+02,
          1.0512e+03,  3.0930e+02,  3.8814e+02,  9.7051e+02, -3.1128e+02,
         -1.6828e+03, -7.3704e+02, -1.2736e+03, -1.7219e+03,  4.9829e+02],
        [        nan,  3.3337e+02,  3.1699e+02,  1.1594e+03, -3.3551e+02,
          3.2770e+03,  5.5943e+02,  1.7597e+03,  3.3531e+03, -9.7035e+02,
         -8.9331e+02, -8.6548e+02, -3.1120e+02, -4.9991e+02, -6.5261e+00,
          1.5795e+03,  4.6472e+02,  6.0137e+02,  1.4650e+03, -4.6770e+02,
         -1.1092e+03, -6.7422e+02, -9.9659e+02, -1.1350e+03,  3.2845e+02],
        [        nan,  4.3780e+02,  9.4782e+02,  1.5226e+03, -4.4061e+02,
          5.2642e+03,  1.5488e+03,  3.3531e+03,  5.3864e+03, -1.5588e+03,
         -3.2033e+02, -1.0421e+03, -2.0404e+02, -3.2777e+02, -8.5297e+02,
          1.3169e+03,  3.8746e+02,  8.3885e+02,  3.9969e+02, -3.8995e+02,
         -3.9168e+03, -1.1524e+03, -3.4427e+03, -4.0077e+03,  1.1598e+03],
        [        nan, -3.5282e+02, -7.6384e+02, -1.2270e+03,  3.5508e+02,
         -1.5234e+03, -4.4821e+02, -9.7035e+02, -1.5588e+03,  4.5108e+02,
          1.0232e+01,  7.6685e+02,  6.5172e+00,  1.0469e+01,  7.6081e+02,
          1.4269e+01,  4.1981e+00,  9.0887e+00,  7.7843e+02, -4.2250e+00,
          2.8040e+02,  8.2499e+01,  9.4244e+02,  2.8691e+02, -8.3028e+01],
        [-5.8830e+02,  3.7672e+02, -2.6811e+02, -1.7578e+02,  5.6146e+00,
         -1.0721e+03,  6.8650e+02, -4.8857e+02, -3.2033e+02,  1.0232e+01,
          2.2340e+02, -1.4305e+02,  1.0181e+02,  6.6751e+01, -2.1321e+00,
         -2.9353e+02,  1.8796e+02, -1.3377e+02, -8.7707e+01,  2.8014e+00,
          8.0074e+02, -5.1276e+02,  3.6492e+02,  2.3926e+02, -7.6421e+00],
        [-1.6421e+02,  1.0515e+02, -7.4833e+01, -4.9065e+01,  1.5672e+00,
         -3.1542e+02,  2.0198e+02, -1.4375e+02, -9.4248e+01,  3.0103e+00,
         -1.4305e+02,  9.1606e+01, -6.5194e+01, -4.2745e+01,  1.3653e+00,
         -2.1906e+02,  1.4028e+02, -9.9832e+01, -6.5455e+01,  2.0907e+00,
          3.5066e+02, -2.2455e+02,  1.5981e+02,  1.0478e+02, -3.3467e+00],
        [-2.3611e+02,  1.5119e+02, -1.0760e+02, -7.0549e+01,  2.2534e+00,
         -6.8287e+02,  4.3728e+02, -3.1120e+02, -2.0404e+02,  6.5172e+00,
          1.0181e+02, -6.5194e+01,  4.6397e+01,  3.0420e+01, -9.7164e-01,
         -3.2914e+02,  2.1077e+02, -1.5000e+02, -9.8347e+01,  3.1412e+00,
          2.3114e+02, -1.4801e+02,  1.0534e+02,  6.9066e+01, -2.2060e+00],
        [-3.1007e+02,  1.9856e+02, -1.4131e+02, -9.2649e+01,  2.9593e+00,
         -1.0970e+03,  7.0244e+02, -4.9991e+02, -3.2777e+02,  1.0469e+01,
          6.6751e+01, -4.2745e+01,  3.0420e+01,  1.9945e+01, -6.3706e-01,
         -2.7442e+02,  1.7573e+02, -1.2506e+02, -8.1998e+01,  2.6190e+00,
          8.1618e+02, -5.2265e+02,  3.7196e+02,  2.4388e+02, -7.7895e+00],
        [ 2.4988e+02, -1.6001e+02,  1.1388e+02,  7.4665e+01, -2.3848e+00,
          3.1744e+02, -2.0328e+02,  1.4467e+02,  9.4852e+01, -3.0296e+00,
         -2.1321e+00,  1.3653e+00, -9.7164e-01, -6.3706e-01,  2.0348e-02,
         -2.9733e+00,  1.9040e+00, -1.3550e+00, -8.8842e-01,  2.8377e-02,
         -5.8430e+01,  3.7416e+01, -2.6628e+01, -1.7459e+01,  5.5765e-01],
        [ 7.7299e+02,  5.7688e+02,  8.6677e+02,  7.2268e+02,  7.8300e+00,
          1.4086e+03,  1.0512e+03,  1.5795e+03,  1.3169e+03,  1.4269e+01,
         -2.9353e+02, -2.1906e+02, -3.2914e+02, -2.7442e+02, -2.9733e+00,
          3.8568e+02,  2.8783e+02,  4.3247e+02,  3.6058e+02,  3.9067e+00,
         -1.0521e+03, -7.8520e+02, -1.1798e+03, -9.8364e+02, -1.0657e+01],
        [ 2.1576e+02,  1.6102e+02,  2.4193e+02,  2.0171e+02,  2.1855e+00,
          4.1444e+02,  3.0930e+02,  4.6472e+02,  3.8746e+02,  4.1981e+00,
          1.8796e+02,  1.4028e+02,  2.1077e+02,  1.7573e+02,  1.9040e+00,
          2.8783e+02,  2.1481e+02,  3.2275e+02,  2.6910e+02,  2.9156e+00,
         -4.6075e+02, -3.4386e+02, -5.1665e+02, -4.3076e+02, -4.6672e+00],
        [ 3.1023e+02,  2.3152e+02,  3.4787e+02,  2.9004e+02,  3.1425e+00,
          8.9726e+02,  6.6962e+02,  1.0061e+03,  8.3885e+02,  9.0887e+00,
         -1.3377e+02, -9.9832e+01, -1.5000e+02, -1.2506e+02, -1.3550e+00,
          4.3247e+02,  3.2275e+02,  4.8493e+02,  4.0432e+02,  4.3807e+00,
         -3.0371e+02, -2.2666e+02, -3.4055e+02, -2.8394e+02, -3.0764e+00],
        [ 4.0741e+02,  3.0405e+02,  4.5684e+02,  3.8089e+02,  4.1269e+00,
          1.4413e+03,  1.0757e+03,  1.6162e+03,  1.3475e+03,  1.4600e+01,
         -8.7707e+01, -6.5455e+01, -9.8347e+01, -8.1998e+01, -8.8842e-01,
          3.6058e+02,  2.6910e+02,  4.0432e+02,  3.3710e+02,  3.6524e+00,
         -1.0724e+03, -8.0034e+02, -1.2025e+03, -1.0026e+03, -1.0863e+01],
        [-3.2833e+02, -2.4503e+02, -3.6816e+02, -3.0696e+02, -3.3258e+00,
         -4.1710e+02, -3.1128e+02, -4.6770e+02, -3.8995e+02, -4.2250e+00,
          2.8014e+00,  2.0907e+00,  3.1412e+00,  2.6190e+00,  2.8377e-02,
          3.9067e+00,  2.9156e+00,  4.3807e+00,  3.6524e+00,  3.9573e-02,
          7.6774e+01,  5.7296e+01,  8.6088e+01,  7.1776e+01,  7.7768e-01],
        [-2.1087e+03, -9.2345e+02, -6.0870e+02, -2.1494e+03,  1.5387e+02,
         -3.8427e+03, -1.6828e+03, -1.1092e+03, -3.9168e+03,  2.8040e+02,
          8.0074e+02,  3.5066e+02,  2.3114e+02,  8.1618e+02, -5.8430e+01,
         -1.0521e+03, -4.6075e+02, -3.0371e+02, -1.0724e+03,  7.6774e+01,
          2.8702e+03,  1.2569e+03,  8.2851e+02,  2.9255e+03, -2.0944e+02],
        [-5.8858e+02, -2.5775e+02, -1.6990e+02, -5.9993e+02,  4.2949e+01,
         -1.1306e+03, -4.9511e+02, -3.2636e+02, -1.1524e+03,  8.2499e+01,
         -5.1276e+02, -2.2455e+02, -1.4801e+02, -5.2265e+02,  3.7416e+01,
         -7.8520e+02, -3.4386e+02, -2.2666e+02, -8.0034e+02,  5.7296e+01,
          1.2569e+03,  5.5044e+02,  3.6282e+02,  1.2812e+03, -9.1718e+01],
        [-8.4630e+02, -3.7062e+02, -2.4429e+02, -8.6262e+02,  6.1755e+01,
         -2.4477e+03, -1.0719e+03, -7.0655e+02, -2.4949e+03,  1.7861e+02,
          3.6492e+02,  1.5981e+02,  1.0534e+02,  3.7196e+02, -2.6628e+01,
         -1.1798e+03, -5.1665e+02, -3.4055e+02, -1.2025e+03,  8.6088e+01,
          8.2851e+02,  3.6282e+02,  2.3916e+02,  8.4448e+02, -6.0456e+01],
        [-1.1114e+03, -4.8672e+02, -3.2082e+02, -1.1328e+03,  8.1100e+01,
         -3.9319e+03, -1.7219e+03, -1.1350e+03, -4.0077e+03,  2.8691e+02,
          2.3926e+02,  1.0478e+02,  6.9066e+01,  2.4388e+02, -1.7459e+01,
         -9.8364e+02, -4.3076e+02, -2.8394e+02, -1.0026e+03,  7.1776e+01,
          2.9255e+03,  1.2812e+03,  8.4448e+02,  2.9819e+03, -2.1348e+02],
        [ 8.9568e+02,  3.9224e+02,  2.5855e+02,  9.1295e+02, -6.5358e+01,
          1.1378e+03,  4.9829e+02,  3.2845e+02,  1.1598e+03, -8.3028e+01,
         -7.6421e+00, -3.3467e+00, -2.2060e+00, -7.7895e+00,  5.5765e-01,
         -1.0657e+01, -4.6672e+00, -3.0764e+00, -1.0863e+01,  7.7768e-01,
         -2.0944e+02, -9.1718e+01, -6.0456e+01, -2.1348e+02,  1.5283e+01]],
       device='xpu:0')

The max per-element difference (slow mode) is: nan.


======================================================================
ERROR: test_fn_gradgrad_linalg_householder_product_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-1.9568, device='xpu:0')
analytical:tensor(-6.5101, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],
        ...,
        [  0.0000,   0.0000,   0.0000,  ...,   0.3551, -28.4191,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,  -2.2606,  28.7687,   0.0000],
        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.3551, -2.2606,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ..., -2.2606, 28.7687,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')

The max per-element difference (slow mode) is: 235.19306672937117.


======================================================================
ERROR: test_fn_gradgrad_linalg_qr_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.2212, device='xpu:0')
analytical:tensor(1.5390, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.6644,  0.2381, -0.1200,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.2381,  5.1342, -1.7777,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1200, -1.7777,  1.9360,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -1.3711, -0.3020, -0.8012],
        [ 0.0000,  0.0000,  0.0000,  ..., -0.2173,  0.2549,  0.0061],
        [ 0.0000,  0.0000,  0.0000,  ..., -0.5091, -0.0735, -0.0336]],
       device='xpu:0')
Analytical:
tensor([[ 6.6444e-01,  2.3812e-01, -1.1999e-01,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        [ 2.3812e-01,  5.1342e+00, -1.7777e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1999e-01, -1.7777e+00,  1.9360e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.8726e+00,
          5.8086e-01,  1.1080e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.2278e+00,
         -2.1146e+00, -2.6222e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  8.1240e-01,
          4.3928e-01,  1.9848e-01]], device='xpu:0')

The max per-element difference (slow mode) is: 16.830620201851517.


======================================================================
ERROR: test_fn_gradgrad_linalg_svdvals_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: mat1 and mat2 shapes cannot be multiplied (0x0 and 5x5)

======================================================================
ERROR: test_fn_gradgrad_lu_unpack_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 528, in _check_helper
    samples = op.sample_inputs(device, dtype, requires_grad=True, include_conjugated_inputs=include_conjugated_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 691, in sample_inputs
    samples = self.sample_inputs_func(self, device, dtype, requires_grad, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 3833, in sample_inputs_lu_unpack
    return list(generate_samples())
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 3815, in generate_samples
    lu_data, pivots = lu_sample.input.lu()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 450, in lu
    LU, pivots, infos = torch._lu_with_info(self, pivot=pivot, check_errors=(not get_infos))
RuntimeError: A must be batches of square matrices, but they are 5 by 3 matrices

======================================================================
ERROR: test_fn_gradgrad_lu_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 420, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1614, in _lu_with_infos
    result = _lu_impl(A, pivot, get_infos, out)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1595, in _lu_impl
    return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
RuntimeError: A must be batches of square matrices, but they are 5 by 3 matrices

======================================================================
ERROR: test_fn_gradgrad_mode_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Input Tensor's dimension must be within (0, 2]

======================================================================
ERROR: test_fn_gradgrad_msort_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-58311.0111, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-814.2633,    0.0000,    0.0000,  ...,    0.0000,    0.0000,
            0.0000],
        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,
            0.0000],
        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,
            0.0000],
        ...,
        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,
            0.0000],
        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,
            0.0000],
        [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,
         -814.2633]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 2678.0289432694904.


======================================================================
ERROR: test_fn_gradgrad_nanquantile_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.0683, device='xpu:0')
analytical:tensor(0.1237, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000]], device='xpu:0')
Analytical:
tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],
       device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_fn_gradgrad_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1131, in adaptive_avg_pool2d
    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: dpcpp_avg_pool2d operator does not support divisor

======================================================================
ERROR: test_fn_gradgrad_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(269037.7778, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[    0.0000,   185.9164,  -701.2234,   -22.6499, -1133.3963],
        [ 1023.4156,     0.0000,     0.0000,     0.0000,     0.0000],
        [    0.0000,     0.0000,  -701.2234,   -22.6499, -1133.3963],
        [-1023.4156,     0.0000,  -701.2234,    22.6499,     0.0000],
        [-1023.4156,     0.0000,     0.0000,     0.0000,     0.0000]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1133.3963105786852.


======================================================================
ERROR: test_fn_gradgrad_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3718, in interpolate
    return adaptive_avg_pool1d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.0146, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 6.6158e+02,  6.5043e+02, -4.4474e+02,  ..., -5.0000e-06,
         -5.0000e-06, -5.0000e-06],
        [ 6.6158e+02,  6.5043e+02, -4.4474e+02,  ..., -5.0000e-06,
         -5.0000e-06, -5.0000e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.6158e+02, -6.5043e+02,  4.4474e+02,  ...,  5.0000e-06,
          5.0000e-06,  5.0000e-06],
        [-6.6158e+02, -6.5043e+02,  4.4474e+02,  ...,  5.0000e-06,
          5.0000e-06,  5.0000e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1291.0632986393337.


======================================================================
ERROR: test_fn_gradgrad_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-577898.9219, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-2.8866e+08,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.8895e+05, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8866e+08, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 2.8895e+05,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8866e+08,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.8895e+05, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8866e+08, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 2.8895e+05,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8866e+08,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.8895e+05, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8866e+08, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 2.8895e+05,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8866e+08,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01],
        [-2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-2.8895e+05, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8866e+08, -5.8330e+01,  1.1031e+02, -1.1194e+02,  2.2696e+02,
          7.4838e+02, -2.7073e+02, -2.1903e+02,  1.0581e+01,  7.8620e+01,
          4.4172e+01,  3.2731e+02,  1.2439e+02,  6.0569e+02,  7.2727e+01,
          2.2905e+02,  1.8176e+01,  3.9934e+02,  8.4743e+02,  6.6528e+02,
         -7.5912e+02,  7.4331e+01, -1.1291e+02, -7.8395e+01],
        [ 2.8895e+08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 2.8895e+05,  5.8330e+01, -1.1031e+02,  1.1194e+02, -2.2696e+02,
         -7.4838e+02,  2.7073e+02,  2.1903e+02, -1.0581e+01, -7.8620e+01,
         -4.4172e+01, -3.2731e+02, -1.2439e+02, -6.0569e+02, -7.2727e+01,
         -2.2905e+02, -1.8176e+01, -3.9934e+02, -8.4743e+02, -6.6528e+02,
          7.5912e+02, -7.4331e+01,  1.1291e+02,  7.8395e+01]], device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
       device='xpu:0')

The max per-element difference (slow mode) is: 288949460.953911.


======================================================================
ERROR: test_fn_gradgrad_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3710, in interpolate
    return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-1.1711e+13, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[   0.0000,    0.0000,    0.0000,  ..., -151.6712,  -56.6300,
         -471.2080],
        [ 122.0308,  543.3487, -111.0044,  ..., -151.6712,  -56.6300,
         -471.2080],
        [ 122.0308,  543.3487, -111.0044,  ...,    0.0000,    0.0000,
            0.0000],
        ...,
        [   0.0000,    0.0000,    0.0000,  ...,  151.6712,   56.6300,
          471.2080],
        [-122.0308, -543.3487,  111.0044,  ...,  151.6712,   56.6300,
          471.2080],
        [-122.0308, -543.3487,  111.0044,  ...,    0.0000,    0.0000,
            0.0000]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 685279.4341566816.


======================================================================
ERROR: test_fn_gradgrad_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 969, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2347, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 420, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 694, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_fn_gradgrad_nn_functional_pad_reflect_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Native API failed. Native API returns: -50 (CL_INVALID_ARG_VALUE) -50 (CL_INVALID_ARG_VALUE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: Native API failed. Native API returns: -50 (CL_INVALID_ARG_VALUE) -50 (CL_INVALID_ARG_VALUE)

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_fn_gradgrad_nn_functional_softplus_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_fn_gradgrad_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 4491, in unfold
    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))
RuntimeError: Expected non-empty 3D or 4D input tensor, but got input of size [0, 1, 5, 5]

======================================================================
ERROR: test_fn_gradgrad_qr_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.6277, device='xpu:0')
analytical:tensor(-0.7125, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 1.1012e+00,  3.5252e-01,  4.2743e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.5252e-01,  2.2900e-01,  1.6201e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.2743e+00,  1.6201e+00,  1.0946e+01,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.2074e-02,
          4.5626e-01, -1.0205e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.3476e-01,
         -9.1060e-01, -1.1622e-01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0741e-03,
          1.0679e+00, -1.0411e+00]], device='xpu:0')
Analytical:
tensor([[ 1.1012,  0.3525,  4.2743,  ..., -0.0000, -0.0000, -0.0000],
        [ 0.3525,  0.2290,  1.6201,  ...,  0.0000,  0.0000,  0.0000],
        [ 4.2743,  1.6201, 10.9462,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  2.8334, -0.6974, -0.3295],
        [ 0.0000,  0.0000,  0.0000,  ..., -0.7619,  1.6259, -1.3354],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.4896, -1.8649,  3.7093]],
       device='xpu:0')

The max per-element difference (slow mode) is: 8.924157136540964.


======================================================================
ERROR: test_fn_gradgrad_quantile_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.2938, device='xpu:0')
analytical:tensor(0.3632, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000]], device='xpu:0')
Analytical:
tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_fn_gradgrad_remainder_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_fn_gradgrad_rsub_rsub_scalar_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: For integral input tensors, argument alpha must not be a floating point number.

======================================================================
ERROR: test_fn_gradgrad_scatter_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_fn_gradgrad_scatter_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_fn_gradgrad_sigmoid_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_fn_gradgrad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_fn_gradgrad_take_along_dim_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.

======================================================================
ERROR: test_fn_gradgrad_topk_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_fn_gradgrad_triangular_solve_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 615, in test_fn_gradgrad
    self._gradgrad_test_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 568, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(14.6586, device='xpu:0')
analytical:tensor(14.8102, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[40.0658,  6.4755, 29.3061,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 4.0065e+01,  6.4754e+00,  2.9305e+01,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
         -0.0000e+00, -0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  4.9655e-01,
         -3.8220e-01, -3.4306e-02]], device='xpu:0')

The max per-element difference (slow mode) is: 1.2604108737559594.


======================================================================
ERROR: test_forward_mode_AD_baddbmm_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: self dim 0 must match batch1 dim 0

======================================================================
ERROR: test_forward_mode_AD_fft_fft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(1776924.1711, device='xpu:0')
analytical:tensor(-0.0286, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.3302e+00,
         -1.2399e+01, -1.4588e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -8.3157e+00,
          2.6482e-02,  3.2956e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.1101e+00,
          2.8041e+01,  3.2974e+01],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.9310e+02,
         -6.8550e+02,  2.6438e+02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.8141e+02,
         -2.9863e+02,  4.6466e+02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.9455e+02,
         -4.7062e+02, -4.6003e+02]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 310289474.28003305.


======================================================================
ERROR: test_forward_mode_AD_fft_hfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(979902.4758, device='xpu:0')
analytical:tensor(0.2596, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.3162, 0.0000, 0.0000,  ..., -0.0000, -0.0000, -0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1954, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 0.3162,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.3162,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.3162,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.3162,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.3162,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3162]],
       device='xpu:0')

The max per-element difference (slow mode) is: 3390341424.6139207.


======================================================================
ERROR: test_forward_mode_AD_fft_ifft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 937, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.1392, device='xpu:0')
analytical:tensor(0.0286, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.7909e+01,
          2.8778e+01,  2.2053e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.1979e+01,
          1.8924e+01,  3.8828e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.2125e+01,
         -3.1822e+02, -2.5335e+03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.7995e+00,
         -2.1101e+06,  5.6370e+05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.1101e+06,
          2.9737e+06,  5.7409e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.9737e+06,
          8.6736e-16,  5.5494e+05]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1517864379.9272983.


======================================================================
ERROR: test_forward_mode_AD_fft_irfft_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-2397894.3302, device='xpu:0')
analytical:tensor(0.2596, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 3.1623e-01,  0.0000e+00,  0.0000e+00,  ..., -1.9374e+01,
          3.1372e-02,  3.2721e+01],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  9.6870e+03,
         -1.5686e+01, -1.6360e+04],
        [ 0.0000e+00,  1.9544e-01,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='xpu:0')
Analytical:
tensor([[ 0.3162,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.3162,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.3162,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ..., -0.3162,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.3162,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.3162]],
       device='xpu:0')

The max per-element difference (slow mode) is: 519613170940.64294.


======================================================================
ERROR: test_forward_mode_AD_fft_irfftn_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.1175, device='xpu:0')
analytical:tensor(0.0460, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.1826,  0.1826,  0.1826,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.2954,  0.1128,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.1128, -0.2954,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')
Analytical:
tensor([[ 0.1826,  0.1826,  0.1826,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.2954,  0.1128,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3651,  0.1128, -0.2954,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='xpu:0')

The max per-element difference (slow mode) is: 237492467570340.88.


======================================================================
ERROR: test_forward_mode_AD_fmod_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_forward_mode_AD_index_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_forward_mode_AD_index_copy_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: src tensor dim should be > 0 and < 12

======================================================================
ERROR: test_forward_mode_AD_index_select_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src tensor dim should be > 0 and < 12

======================================================================
ERROR: test_forward_mode_AD_linalg_det_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 544, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 0.0.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=<tol>` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=<tol>`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=<tol>`


======================================================================
ERROR: test_forward_mode_AD_lu_unpack_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 528, in _check_helper
    samples = op.sample_inputs(device, dtype, requires_grad=True, include_conjugated_inputs=include_conjugated_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 691, in sample_inputs
    samples = self.sample_inputs_func(self, device, dtype, requires_grad, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 3833, in sample_inputs_lu_unpack
    return list(generate_samples())
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 3815, in generate_samples
    lu_data, pivots = lu_sample.input.lu()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 450, in lu
    LU, pivots, infos = torch._lu_with_info(self, pivot=pivot, check_errors=(not get_infos))
RuntimeError: A must be batches of square matrices, but they are 5 by 3 matrices

======================================================================
ERROR: test_forward_mode_AD_lu_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 769, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 420, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1614, in _lu_with_infos
    result = _lu_impl(A, pivot, get_infos, out)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1595, in _lu_impl
    return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))
RuntimeError: A must be batches of square matrices, but they are 5 by 3 matrices

======================================================================
ERROR: test_forward_mode_AD_mode_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Input Tensor's dimension must be within (0, 2]

======================================================================
ERROR: test_forward_mode_AD_msort_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0., device='xpu:0')
analytical:tensor(0.7533, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_forward_mode_AD_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1131, in adaptive_avg_pool2d
    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_avg_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: dpcpp_avg_pool2d operator does not support divisor

======================================================================
ERROR: test_forward_mode_AD_nn_functional_conv2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-134894.0793, device='xpu:0')
analytical:tensor(0.5746, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 2.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4694e+03],
        [-2.6840e+03, -2.4942e+03,  0.0000e+00,  2.3342e+03,  0.0000e+00],
        [ 0.0000e+00, -2.4952e+03, -6.3293e+03,  0.0000e+00, -2.4694e+03],
        [ 2.6840e+03,  2.4952e+03,  0.0000e+00, -2.3332e+03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e+00]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 2.]], device='xpu:0')

The max per-element difference (slow mode) is: 6329.318465195334.


======================================================================
ERROR: test_forward_mode_AD_nn_functional_grid_sample_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 544, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-15.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=<tol>` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=<tol>`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=<tol>`


======================================================================
ERROR: test_forward_mode_AD_nn_functional_interpolate_area_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3718, in interpolate
    return adaptive_avg_pool1d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.6994, device='xpu:0')
analytical:tensor(-0.2286, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_forward_mode_AD_nn_functional_interpolate_linear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.6333, device='xpu:0')
analytical:tensor(2.0000e-06, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='xpu:0')
Analytical:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
         1., 1., 1., 1., 1., 1.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0000000000000009.


======================================================================
ERROR: test_forward_mode_AD_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3710, in interpolate
    return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.7666, device='xpu:0')
analytical:tensor(-0.1615, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[1., 1., 1.,  ..., 1., 1., 1.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_forward_mode_AD_nn_functional_layer_norm_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 969, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2347, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_max_pool2d_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 420, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 694, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_forward_mode_AD_nn_functional_unfold_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 4491, in unfold
    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))
RuntimeError: Expected non-empty 3D or 4D input tensor, but got input of size [0, 1, 5, 5]

======================================================================
ERROR: test_forward_mode_AD_remainder_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_forward_mode_AD_sigmoid_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_forward_mode_AD_sort_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0., device='xpu:0')
analytical:tensor(0.7504, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')
Analytical:
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1.0.


======================================================================
ERROR: test_forward_mode_AD_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_forward_mode_AD_take_along_dim_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 656, in test_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, op.get_op())
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.

======================================================================
ERROR: test_inplace_forward_mode_AD_fmod_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 665, in test_inplace_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 650, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_inplace_forward_mode_AD_index_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 665, in test_inplace_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_inplace_forward_mode_AD_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 665, in test_inplace_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(3998201.5454, device='xpu:0')
analytical:tensor(0.9649, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 2.0000e+00,  0.0000e+00,  4.0785e+03,  0.0000e+00,  0.0000e+00],
        [-8.0988e+03, -8.9418e+03,  4.0785e+03,  0.0000e+00,  0.0000e+00],
        [ 8.0988e+03,  0.0000e+00, -4.0775e+03, -7.0237e+03, -3.4393e+03],
        [ 0.0000e+00,  8.9428e+03,  0.0000e+00,  7.0247e+03,  0.0000e+00],
        [-8.0988e+03,  8.9428e+03,  4.0785e+03,  7.0237e+03,  2.0000e+00]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 2., 0., 0., 0.],
        [0., 0., 2., 0., 0.],
        [0., 0., 0., 2., 0.],
        [0., 0., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 8943.779267894277.


======================================================================
ERROR: test_inplace_forward_mode_AD_remainder_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 665, in test_inplace_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_inplace_forward_mode_AD_sigmoid_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 665, in test_inplace_forward_mode_AD
    self._forward_grad_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 644, in _forward_grad_helper
    self._grad_test_helper(device, dtype, op, variant, check_forward_ad=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_inplace_forward_mode_AD_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_inplace_grad_fmod_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_inplace_grad_index_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_inplace_grad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-1877434.0522, device='xpu:0')
analytical:tensor(0.5746, device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-8.4217e+03,  0.0000e+00,  0.0000e+00, -5.3300e+03, -8.6291e+03],
        [ 8.4227e+03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 8.4227e+03,  0.0000e+00, -3.2178e+03,  5.3300e+03,  0.0000e+00],
        [ 8.4227e+03,  0.0000e+00, -3.2188e+03,  2.0000e+00,  8.6291e+03],
        [ 0.0000e+00,  0.0000e+00, -3.2188e+03,  5.3300e+03,  2.0000e+00]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 2.]], device='xpu:0')

The max per-element difference (slow mode) is: 8629.142977698062.


======================================================================
ERROR: test_inplace_grad_remainder_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_inplace_grad_scatter_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_inplace_grad_scatter_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1165, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 555, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 528, in _check_analytical_jacobian_attributes
    vjps1 = _get_analytical_vjps_wrt_specific_output(vjp_fn, output.clone(), v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 633, in _get_analytical_vjps_wrt_specific_output
    grad_inputs = vjp_fn(v.reshape(sample_output.shape))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 524, in vjp_fn
    return torch.autograd.grad(output, diff_input_list, grad_output,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_inplace_grad_sigmoid_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 607, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 578, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 554, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2688, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_inplace_grad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

======================================================================
ERROR: test_inplace_gradgrad_fmod_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_inplace_gradgrad_index_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
ERROR: test_inplace_gradgrad_nn_functional_dropout_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1276, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 946, in _gradcheck_real_imag
    gradcheck_fn(func, func_out, tupled_inputs, outputs, eps,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1167, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1147, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-253584.9416, device='xpu:0')
analytical:tensor(0., device='xpu:0')

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-646.5262, -615.2074,  584.4054,    0.0000,   64.2659],
        [   0.0000, -615.2074,  584.4054, 1130.3003,    0.0000],
        [-646.5262,    0.0000,    0.0000,    0.0000,   64.2659],
        [   0.0000,    0.0000, -584.4054,    0.0000,    0.0000],
        [ 646.5262, -615.2074, -584.4054, 1130.3003,   64.2659]],
       device='xpu:0')
Analytical:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], device='xpu:0')

The max per-element difference (slow mode) is: 1130.3002575401263.


======================================================================
ERROR: test_inplace_gradgrad_remainder_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1370, in gradgradcheck
    outputs = _as_tuple(func(*tupled_inputs))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 544, in fn
    output = op.gradcheck_wrapper(variant, *inputs, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 496, in <lambda>
    gradcheck_wrapper=lambda op, *args, **kwargs: op(*args, **kwargs),  # wrapper function for gradcheck
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 512, in _fn
    return inplace_variant(t.clone(), *args, **kwargs)
RuntimeError: src element is not the same as dst

======================================================================
ERROR: test_inplace_gradgrad_scatter_add_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_inplace_gradgrad_scatter_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1271, in _gradcheck_helper
    func_out = func(*tupled_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1383, in new_func
    grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: Input tensor must have same size as output tensor apart from the specified dimension

======================================================================
ERROR: test_inplace_gradgrad_sigmoid_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 772, in _test_batched_grad
    result = vmap(vjp)(torch.stack(grad_outputs))
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_vmap_internals.py", line 263, in wrapped
    batched_outputs = func(*batched_inputs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 755, in vjp
    results = grad(v)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: different elements ...

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 402, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 397, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 734, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 640, in test_inplace_gradgrad
    self._gradgrad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 581, in _gradgrad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradgradcheck')
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/test_ops.py", line 562, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args,
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2708, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1388, in gradgradcheck
    return gradcheck(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1263, in gradcheck
    return _gradcheck_helper(**args)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 1281, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/gradcheck.py", line 778, in _test_batched_grad
    raise GradcheckError(
torch.autograd.gradcheck.GradcheckError: While computing batched gradients, got: different elements ...

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.
- is common_method_invocations-based, then add your test to the denylist
  EXCLUDE_BATCHED_GRAD_TESTS in test_autograd.py

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

======================================================================
ERROR: test_inplace_gradgrad_special_erfcx_xpu_float64 (__main__.TestGradientsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1396, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 396, in instantiated_test
    self._apply_precision_override_for_test(test, param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 356, in _apply_precision_override_for_test
    self.precision, self.rel_tol = self._get_tolerance_override(test, dtype)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/pytorch/common/pytorch_test_base.py", line 349, in _get_tolerance_override
    return test.tolerance_overrides.get(dtype, tol(self.precision, self.rel_tol))
NameError: name 'tol' is not defined

----------------------------------------------------------------------
Ran 4046 tests in 816.700s

FAILED (errors=162, skipped=3097)
Raised CalledProcessError: return code 1.