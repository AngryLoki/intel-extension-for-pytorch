MESA: warning: Driver does not support the 0xbd5 PCI ID.
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
test_clamp_propagates_nans_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_clamp_raises_arg_errors_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_clamp_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_clamp_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_complex_rot90_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_complex_rot90_xpu_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_diag_xpu_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_diag_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_diagonal_multidim_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cpu'
test_diagonal_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_errors_xpu_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_errors_xpu_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_flip_errors_xpu_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_flip_large_tensor_xpu_xpu (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cuda'
test_flip_numpy_xpu_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_numpy_xpu_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_numpy_xpu_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_flip_numpy_xpu_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_xpu_xpu_complex64 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flip_xpu_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... skipped "not implemented: Could not run 'aten::flip' with arguments from the 'QuantizedXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::flip' is only available for these backends: [CPU, XPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nXPU: registered at /home/gta/xunsongh/ipex-gpu/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:10022 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nQuantizedCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/BatchRulesViews.cpp:512 [kernel]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_flip_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_flip_xpu_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_invalid_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_fliplr_invalid_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_invalid_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_fliplr_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_fliplr_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_invalid_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flipud_invalid_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_invalid_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_flipud_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_flipud_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_movedim_invalid_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_movedim_invalid_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_movedim_invalid_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_movedim_xpu_xpu_complex128 (__main__.TestShapeOpsXPU) ... skipped 'dtype not support on XPU'
test_movedim_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_movedim_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_astuple_out_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_nonzero_discontiguous_xpu_xpu (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nonzero_no_warning_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_nonzero_non_diff_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_bfloat16 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_bool (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_float16 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_int16 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_int32 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_int8 (__main__.TestShapeOpsXPU) ... ok
test_nonzero_xpu_xpu_uint8 (__main__.TestShapeOpsXPU) ... ok
test_rot90_xpu_xpu (__main__.TestShapeOpsXPU) ... ok
test_tolist_xpu_xpu (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cpu'
test_trace_xpu_xpu_float16 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_float32 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_float64 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_int16 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_int32 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_int64 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_int8 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_trace_xpu_xpu_uint8 (__main__.TestShapeOpsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_unbind_xpu_xpu (__main__.TestShapeOpsXPU) ... skipped 'Only runs on cpu'

----------------------------------------------------------------------
Ran 90 tests in 3.870s

OK (skipped=28)
