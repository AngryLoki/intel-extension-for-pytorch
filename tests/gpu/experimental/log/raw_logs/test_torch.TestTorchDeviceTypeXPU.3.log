MESA: warning: Driver does not support the 0xbd5 PCI ID.
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
test_addcdiv_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_addcdiv_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_addcdiv_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcdiv_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcdiv_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcdiv_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcdiv_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcdiv_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcdiv_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_addcmul_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_addcmul_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_addcmul_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_addcmul_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_addcmul_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_addcmul_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_addcmul_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_addcmul_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_addcmul_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_assertRaisesRegex_ignore_msg_non_native_device_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_edge_cases_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_edge_cases_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_edge_cases_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bernoulli_p_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_p_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_p_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bernoulli_self_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_bfloat16_float_copy_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_bool_tensor_value_change_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_add_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_addcdiv_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_addcmul_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_atan2_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_copy_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_dist_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_div_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_eq_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_fmod_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_ge_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_gt_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_le_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_lerp_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_lt_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_map2_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_map_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_masked_fill_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_masked_scatter_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_masked_select_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_max_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_min_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_mul_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_ne_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_pow_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_remainder_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_broadcast_fn_sub_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_bytes_to_scalar_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_bytes_to_scalar_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_bytes_to_scalar_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_bytes_to_scalar_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_cauchy_kstest_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_cauchy_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_cauchy_kstest_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_cauchy_kstest_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_cauchy_no_inf_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_cauchy_no_inf_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_cdist_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_euclidean_large_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ERROR
test_cdist_grad_p_lt_1_no_nan_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_cdist_large_batch_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ERROR
test_cdist_large_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_non_contiguous_batch_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_non_contiguous_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_norm_batch_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_norm_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_same_inputs_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cdist_xpu_backward_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_clone_all_dtypes_and_devices_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_clone_not_memory_dense_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_clone_zero_stride_dim_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_complex_half_experimental_warning_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_constants_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_conv_transposed_backward_agnostic_to_memory_format_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ERROR
test_conv_transposed_large_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_copy__xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_copy__xpu_xpu_complex32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_copy__xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_copy__xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy__xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_copy_all_dtypes_and_devices_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_copy_math_view_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_copy_mem_overlap_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_copy_transpose_math_view_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_copy_transpose_math_view_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_copy_transpose_math_view_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_corrcoef_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_corrcoef_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_corrcoef_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_cov_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_cov_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:1354: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Correlation.cpp:100.)
  res = torch.cov(t, correction=correction, fweights=fweights, aweights=aweights)
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.
  avg = a.mean(axis, **keepdims_kw)
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide
  ret = um.true_divide(
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:1358: RuntimeWarning: Degrees of freedom <= 0 for slice
  ref = np.cov(t, ddof=correction, fweights=fweights, aweights=aweights)
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: divide by zero encountered in divide
  c *= np.true_divide(1, fact)
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: invalid value encountered in multiply
  c *= np.true_divide(1, fact)
ok
test_cov_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_cpp_warnings_have_python_context_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_cublas_config_nondeterministic_alert_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_cummax_cummin_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::_cummax_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummax_helper' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_cummax_discontiguous_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::_cummax_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummax_helper' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_cummin_discontiguous_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::_cummin_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummin_helper' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_cumprod_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_cumsum_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_deepcopy_scalar_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_deepcopy_scalar_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_deepcopy_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_deepcopy_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_device_guard_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_diff_noncontig_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_diff_noncontig_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_diff_noncontig_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_noncontig_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_diff_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_diff_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_diff_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_diff_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_dim_function_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_discontiguous_out_cumsum_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_dist_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_errors_index_copy_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_expected_failure_xla_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_kstest_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_kstest_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_kstest_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_no_zero_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_exponential_no_zero_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_exponential_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_exponential_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_gather_backward_deterministic_path_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gather_backward_one_dim_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_geometric_kstest_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_kstest_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_geometric_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_gradient_all_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_gradient_all_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gradient_all_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gradient_extreme_cases_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_gradient_extreme_cases_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gradient_extreme_cases_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gradient_type_promotion_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_hook_remove_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_add_deterministic_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_index_add_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_deterministic_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_index_copy_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_copy_scalars_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_copy_scalars_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_scalars_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_copy_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_copy_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_copy_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_copy_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_fill_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_fill_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_fill_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_fill_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_index_put_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_put_non_accumulate_deterministic_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_index_reduce_reduce_amax_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_deprecated.py:35: FutureWarning: torch.testing.make_non_contiguous() is deprecated since 1.12 and will be removed in 1.14. Depending on the use case there a different replacement options:

- If you are using `make_non_contiguous` in combination with a creation function to create a noncontiguous tensor with random values, use `torch.testing.make_tensor(..., noncontiguous=True)` instead.
- If you are using `make_non_contiguous` with a specific tensor, you can replace this call with `torch.repeat_interleave(input, 2, dim=-1)[..., ::2]`.
- If you are using `make_non_contiguous` in the PyTorch test suite, use `torch.testing._internal.common_utils.noncontiguous_like` instead.
  warnings.warn(msg, FutureWarning)
skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amax_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_amin_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_mean_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_reduce_reduce_prod_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_index_select_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_select_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_index_select_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_index_select_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_invalid_shapes_grid_sampler_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_is_set_to_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_is_signed_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_item_xpu_xpu_complex32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_item_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_item_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_item_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_large_cumprod_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_large_cumsum_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_log_normal_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_log_normal_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_log_normal_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_log_normal_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_logcumsumexp_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_lognormal_kstest_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_lognormal_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_lognormal_kstest_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_lognormal_kstest_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_bool_tensor_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_fill_xpu_xpu_bfloat16_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_bfloat16_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_bool_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_bool_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_complex128_bool (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_fill_xpu_xpu_complex128_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_fill_xpu_xpu_complex64_bool (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_fill_xpu_xpu_complex64_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_fill_xpu_xpu_float16_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_float16_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_float32_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_float32_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_float64_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_float64_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int16_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int16_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int32_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int32_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int64_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int64_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int8_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_int8_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_uint8_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_fill_xpu_xpu_uint8_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_scatter_bool_tensor_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_scatter_large_tensor_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_masked_scatter_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_scatter_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_scatter_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_scatter_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_masked_select_discontiguous_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_masked_select_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_select_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_masked_select_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_masked_select_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py:2647: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a mask with dtype torch.bool instead. (Triggered internally at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/operators/Indexing.cpp:1276.)
  torch.masked_select(src, mask, out=dst3)
ok
test_memory_format_clone_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_consistency_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_cpu_and_xpu_ops_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_empty_like_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_factory_like_functions_preserve_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_operators_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_preserved_after_permute_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_propagation_rules_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_memory_format_to_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_type_shortcuts_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_memory_format_type_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_module_share_memory_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_multinomial_cpu_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_cpu_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_cpu_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_deterministic_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_multinomial_deterministic_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_multinomial_deterministic_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_multinomial_device_constrain_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_multinomial_empty_w_replacement_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_empty_wo_replacement_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_gpu_device_constrain_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_multinomial_rng_state_advance_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_multinomial_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_multinomial_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_narrow_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_AdaptiveAvgPool2d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_AdaptiveAvgPool3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_AdaptiveMaxPool2d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_AvgPool3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_CTCLoss_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_EmbeddingBag_max_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_FractionalMaxPool2d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_FractionalMaxPool3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_MaxPool3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_MaxUnpool1d_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool1d_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool1d_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool2d_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool2d_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool2d_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool3d_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool3d_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_MaxUnpool3d_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nondeterministic_alert_NLLLoss_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_ReflectionPad1d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_ReflectionPad2d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_ReflectionPad3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_ReplicationPad1d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_ReplicationPad2d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_ReplicationPad3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_bincount_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_cumsum_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_cumsum_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_nondeterministic_alert_cumsum_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_nondeterministic_alert_cumsum_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_cumsum_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_cumsum_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_cumsum_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_cumsum_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_cumsum_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_cumsum_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_grid_sample_2d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_grid_sample_3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_histc_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_interpolate_bicubic_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_interpolate_bilinear_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_interpolate_linear_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_interpolate_trilinear_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_kthvalue_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... FAIL
test_nondeterministic_alert_median_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nondeterministic_alert_put_accumulate_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_nondeterministic_alert_put_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_normal_kstest_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_normal_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_normal_kstest_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_normal_kstest_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_nullary_op_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_pairwise_distance_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_pdist_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_pdist_norm_large_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_pickle_gradscaler_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'not ready on XPU'
test_pin_memory_from_constructor_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_put_accumulate_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_put_accumulate_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_put_accumulate_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_accumulate_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_put_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_put_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_put_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_put_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_repeat_interleave_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_scalar_check_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ERROR
test_scatter_add_bool_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_add_non_unique_index_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_add_one_dim_deterministic_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_scatter_add_to_large_input_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_bool_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_scatter_reduce_multiply_unsupported_dtypes_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_multiply_unsupported_dtypes_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_non_unique_index_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_non_unique_index_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_non_unique_index_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_non_unique_index_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_operations_to_large_input_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_operations_to_large_input_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_operations_to_large_input_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_scalar_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_scatter_reduce_scalar_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_reduce_scalar_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_to_large_input_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_scatter_zero_size_index_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_serialization_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_set_storage_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_set_storage_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_set_storage_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_set_storage_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_shift_mem_overlap_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_skip_xla_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_all_devices_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_storage_meta_errors_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_meta_errors_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_meta_errors_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_errors_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_storage_meta_from_tensor_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_meta_from_tensor_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_meta_from_tensor_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_meta_from_tensor_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_setitem_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_setitem_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_qint32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_qint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_quint4x2 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_quint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_setitem_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_storage_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_storage_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_storage_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_strides_propagation_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_sync_warning_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_take_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_take_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_take_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_take_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... ok
test_tensor_from_storage_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_tensor_from_storage_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_tensor_from_storage_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_from_storage_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_set_errors_multigpu_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cuda'
test_tensor_shape_empty_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_tensor_storage_type_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_tensor_storage_type_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_tensor_storage_type_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_tensor_storage_type_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_ternary_op_mem_overlap_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_typed_storage_meta_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_bool (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_complex128 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_typed_storage_meta_xpu_xpu_complex64 (__main__.TestTorchDeviceTypeXPU) ... skipped 'dtype not support on XPU'
test_typed_storage_meta_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_int16 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_int32 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_int64 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_int8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_typed_storage_meta_xpu_xpu_uint8 (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_unfold_all_devices_and_dtypes_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_unfold_scalars_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... ok
test_uniform_kstest_xpu_xpu_bfloat16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_uniform_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU) ... ok
test_uniform_kstest_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU) ... ok
test_uniform_kstest_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU) ... ok
test_untyped_storage_meta_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_warn_always_caught_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped 'Only runs on cpu'
test_where_scalar_handcrafted_values_xpu_xpu (__main__.TestTorchDeviceTypeXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"

======================================================================
ERROR: test_cdist_euclidean_large_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1640, in test_cdist_euclidean_large
    _test_euclidean_large_cdist((2000, 5))
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1639, in _test_euclidean_large_cdist
    loss.backward()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Provided range is out of integer limits. Pass `-fno-sycl-id-queries-fit-in-int' to disable range check. -30 (CL_INVALID_VALUE)

======================================================================
ERROR: test_cdist_large_batch_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1286, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_cuda.py", line 144, in wrapped
    f(**kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1572, in test_cdist_large_batch
    actual = torch.cdist(x, y, p=2, compute_mode=cm)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/functional.py", line 1218, in cdist
    return _VF.cdist(x1, x2, p, 2)  # type: ignore[attr-defined]
RuntimeError: Provided range is out of integer limits. Pass `-fno-sycl-id-queries-fit-in-int' to disable range check. -30 (CL_INVALID_VALUE)

======================================================================
ERROR: test_conv_transposed_backward_agnostic_to_memory_format_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 625, in test_conv_transposed_backward_agnostic_to_memory_format
    input_.sum().backward()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: expected 4D tensor, got tensor with 1 dimensions instead

======================================================================
ERROR: test_scalar_check_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 430, in test_scalar_check
    self.assertEqual([(), ()], [x.shape for x in torch.topk(zero_d, 1, 0, False)])
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
FAIL: test_cdist_grad_p_lt_1_no_nan_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1651, in test_cdist_grad_p_lt_1_no_nan
    self.assertFalse(torch.isnan(x.grad).any())
AssertionError: tensor(True, device='xpu:0') is not false

======================================================================
FAIL: test_cpp_warnings_have_python_context_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 953, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 555, in test_cpp_warnings_have_python_context
    self.assertEqual(frameinfo.lineno - 6, warning.lineno)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Absolute difference: 2 (up to 0.001 allowed)
Relative difference: 0.003656307129798903 (up to 0.0 allowed)

======================================================================
FAIL: test_invalid_shapes_grid_sampler_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1075, in test_invalid_shapes_grid_sampler
    torch.grid_sampler(input, grid, interpolation_mode, padding_mode, align_corners)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised

======================================================================
FAIL: test_lognormal_kstest_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1264, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1405, in test_lognormal_kstest
    self.assertTrue(res.statistic < 0.3)
AssertionError: False is not true

======================================================================
FAIL: test_multinomial_xpu_xpu_float16 (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 3432, in test_multinomial
    self.assertNotEqual(sample_indices[i, j], zero_prob_idx, msg='sampled an index with zero probability')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2510, in assertNotEqual
    self.assertEqual(x, y, msg, atol=atol, rtol=rtol, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: AssertionError not raised : sampled an index with zero probability

======================================================================
FAIL: test_multinomial_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 3432, in test_multinomial
    self.assertNotEqual(sample_indices[i, j], zero_prob_idx, msg='sampled an index with zero probability')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2510, in assertNotEqual
    self.assertEqual(x, y, msg, atol=atol, rtol=rtol, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: AssertionError not raised : sampled an index with zero probability

======================================================================
FAIL: test_nondeterministic_alert_EmbeddingBag_max_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 991, in test_nondeterministic_alert_EmbeddingBag_max
    self.check_nondeterministic_alert(lambda : res.backward(grad, retain_graph=True), 'embedding_bag_backward_xpu_max', torch.device(device).type == 'xpu')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

======================================================================
FAIL: test_nondeterministic_alert_ReflectionPad3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 945, in test_nondeterministic_alert_ReflectionPad3d
    self.check_nondeterministic_alert(lambda : res.backward(grad, retain_graph=True), 'reflection_pad3d_backward_out_xpu', torch.device(device).type == 'xpu')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

======================================================================
FAIL: test_nondeterministic_alert_ReplicationPad1d_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 953, in test_nondeterministic_alert_ReplicationPad1d
    self.check_nondeterministic_alert(lambda : res.backward(grad, retain_graph=True), 'replication_pad1d_backward_xpu', torch.device(device).type == 'xpu')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

======================================================================
FAIL: test_nondeterministic_alert_ReplicationPad3d_xpu_xpu (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1061, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 968, in test_nondeterministic_alert_ReplicationPad3d
    self.check_nondeterministic_alert(lambda : res.backward(grad, retain_graph=True), 'replication_pad3d_backward_xpu', torch.device(device).type == 'xpu')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

======================================================================
FAIL: test_nondeterministic_alert_cumsum_xpu_xpu_float32 (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 998, in test_nondeterministic_alert_cumsum
    self.check_nondeterministic_alert(lambda : op_call(input, 0), 'cumsum_xpu_kernel', should_alert)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

======================================================================
FAIL: test_nondeterministic_alert_cumsum_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 998, in test_nondeterministic_alert_cumsum
    self.check_nondeterministic_alert(lambda : op_call(input, 0), 'cumsum_xpu_kernel', should_alert)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

======================================================================
FAIL: test_nondeterministic_alert_kthvalue_xpu_xpu_float64 (__main__.TestTorchDeviceTypeXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_torch.py", line 1046, in test_nondeterministic_alert_kthvalue
    self.check_nondeterministic_alert(lambda : test_func('function'), 'kthvalue XPU', torch.device(device).type == 'xpu')
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2759, in check_nondeterministic_alert
    fn()
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1889, in __exit__
    return super().__exit__(exc_type, exc_value, tb)
AssertionError: RuntimeError not raised : expected a non-deterministic error, but it was not raised

----------------------------------------------------------------------
Ran 654 tests in 46.827s

FAILED (failures=13, errors=4, skipped=335)
Raised CalledProcessError: return code 1.