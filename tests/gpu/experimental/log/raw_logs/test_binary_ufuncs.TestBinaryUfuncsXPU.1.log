MESA: warning: Driver does not support the 0xbd5 PCI ID.
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
test___add___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___add___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___and___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___eq___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___floordiv___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ge___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___gt___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___iadd___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___iand___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ifloordiv___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ilshift___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___imod___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___imul___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ior___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ipow___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___irshift___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___isub___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___itruediv___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ixor___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___le___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___lshift___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___lt___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___matmul___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___mod___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___mul___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ne___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___or___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___pow___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___radd___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rand___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rfloordiv___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rlshift___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmatmul___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmod___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rmul___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___ror___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rpow___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rrshift___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rshift___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rsub___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rtruediv___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___rxor___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___sub___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___truediv___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test___xor___not_implemented_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_add_broadcast_empty_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_add_with_tail_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_add_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_addcmul_scalars_as_floats_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_addsub_half_tensor_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_atan2_edgecases_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_atan2_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py:863: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(other, dtype=dtype, device=self.device) ** self
ok
test_batch_vs_slicing___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_batch_vs_slicing_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_batch_vs_slicing_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_batch_vs_slicing_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_batch_vs_slicing_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_batch_vs_slicing_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_batch_vs_slicing_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_batch_vs_slicing_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_batch_vs_slicing_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_batch_vs_slicing_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_batch_vs_slicing_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_batch_vs_slicing_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_binary_op_mem_overlap_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_binary_op_scalar_device_unspecified_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_binary_ops_with_scalars_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_bitwise_ops_xpu_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_bitwise_ops_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_bitwise_ops_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_bitwise_ops_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_bitwise_ops_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_bitwise_ops_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_bool_tensor_comparison_ops_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_broadcasting_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_broadcasting_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_broadcasting_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_broadcasting_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_broadcasting_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_cdiv_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_cmul_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex128_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex128_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex128_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex64_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_complex64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_complex_scalar_pow_tensor_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_complex_scalar_pow_tensor_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_complex_scalar_pow_tensor_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_complex_scalar_pow_tensor_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_contig_size1___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_contig_size1_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_large_dim_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_large_dim_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_large_dim_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_contig_size1_large_dim_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_large_dim_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_large_dim_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_size1_large_dim_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_size1_large_dim_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_large_dim_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_large_dim_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_large_dim_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_size1_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_size1_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_size1_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_size1_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_every_other_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_every_other_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_every_other_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_contig_vs_every_other_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_contig_vs_every_other_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_every_other_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_every_other_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_every_other_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_contig_vs_every_other_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_every_other_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_every_other_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_every_other_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_transposed_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_transposed_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_transposed_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_contig_vs_transposed_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_transposed_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_transposed_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_contig_vs_transposed_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_contig_vs_transposed_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_contig_vs_transposed_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_contig_vs_transposed_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_contig_vs_transposed_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_subgradient_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_copysign_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_cpow_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_cpu_tensor_pow_xpu_scalar_tensor_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_cremainder_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_cross_device_binary_ops_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_cross_device_inplace_error_msg_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_csub_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_cumulative_trapezoid_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_div_and_floordiv_script_vs_python_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_div_and_floordiv_vs_python_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_modes_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_nonfinite_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_nonfinite_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_nonfinite_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_nonfinite_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... FAIL
test_div_rounding_numpy_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_rounding_numpy_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_div_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_divide_by_zero_rounding_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:539: RuntimeWarning: divide by zero encountered in divide
  expect = np.divide(an, 0)
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:539: RuntimeWarning: invalid value encountered in divide
  expect = np.divide(an, 0)
ok
test_divide_by_zero_rounding_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_divide_by_zero_rounding_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_divide_by_zero_rounding_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_divmul_scalar_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_float_power_exceptions_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:2363: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Resize.cpp:17.)
  torch.float_power(base, exp, out=out)
ok
test_float_power_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:2341: RuntimeWarning: invalid value encountered in float_power
  expected_scalar_base = torch.from_numpy(np.float_power(i, to_np(exp)))
ok
test_float_power_xpu_xpu_bfloat16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_bfloat16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_complex128_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex128_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_complex64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_uint8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_float_power_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_power_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_float_scalar_pow_float_tensor_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:727: RuntimeWarning: invalid value encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:727: RuntimeWarning: divide by zero encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
ok
test_float_scalar_pow_float_tensor_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_scalar_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_tensor_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_floor_divide_zero_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_floor_divide_zero_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_floor_divide_zero_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_floor_divide_zero_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_floor_divide_zero_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_fmod_remainder_by_zero_float_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_by_zero_float_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_by_zero_float_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_by_zero_integral_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_fmod_remainder_by_zero_integral_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_fmod_remainder_by_zero_integral_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_fmod_remainder_by_zero_integral_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_fmod_remainder_by_zero_integral_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_fmod_remainder_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_fmod_remainder_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_gcd_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gcd_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gcd_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gcd_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_gcd_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_complex_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_heaviside_complex_xpu_xpu_complex128_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_heaviside_complex_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_heaviside_complex_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_heaviside_cross_device_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_heaviside_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_heaviside_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_hypot_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_hypot_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_idiv_and_ifloordiv_vs_python_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_inplace_division_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_inplace_dunders_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_int_and_float_pow_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_int_tensor_pow_neg_ints_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'not ready on XPU'
test_lcm_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_lcm_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_lcm_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_ldexp_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_lerp_lowp_cpu_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_lerp_lowp_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_lerp_lowp_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_lerp_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_lerp_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_lerp_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_lerp_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logaddexp2_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:1971: RuntimeWarning: invalid value encountered in logaddexp2
  ref = ref_func(a.cpu().float().numpy(), b.cpu().float().numpy())
ok
test_logaddexp2_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:1975: RuntimeWarning: invalid value encountered in logaddexp2
  ref = ref_func(a.cpu().numpy(), b.cpu().numpy())
ok
test_logaddexp2_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logaddexp_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:1971: RuntimeWarning: invalid value encountered in logaddexp
  ref = ref_func(a.cpu().float().numpy(), b.cpu().float().numpy())
ok
test_logaddexp_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:1975: RuntimeWarning: invalid value encountered in logaddexp
  ref = ref_func(a.cpu().numpy(), b.cpu().numpy())
ok
test_logaddexp_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_bfloat16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_bool_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_complex128_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex128_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_complex64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_uint8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_and_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_and_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_bfloat16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_bool_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_complex128_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex128_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_complex64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_uint8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_or_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_or_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_with_nontrivial_alignment_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_bfloat16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_bool_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_complex128_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex128_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_complex64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int16_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int32_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_uint8_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_logical_xor_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xor_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_logical_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_long_tensor_pow_floats_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_and_minimum_subgradient_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_complex_xpu_xpu_complex128_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex128_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_complex_xpu_xpu_complex64_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_maximum_minimum_cross_device_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_maximum_minimum_float_nan_and_inf_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_nan_and_inf_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_nan_and_inf_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_nan_and_inf_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_float_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_forward_ad_float32_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_int_and_bool_xpu_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_int_and_bool_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_int_and_bool_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_int_and_bool_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_int_and_bool_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_int_and_bool_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bfloat16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_maximum_minimum_type_promotion_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_min_max_binary_op_nan_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_min_max_binary_op_nan_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_min_max_binary_op_nan_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_mul_chalf_tensor_and_cpu_scalar_xpu_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_mul_intertype_scalar_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_mul_intertype_scalar_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_mul_intertype_scalar_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_mul_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_muldiv_scalar_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_muldiv_scalar_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_muldiv_scalar_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_nextafter_bfloat16_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nextafter_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_nextafter_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_non_contig___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_expand_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_expand_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_expand_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_non_contig_expand_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_expand_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_expand_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_expand_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_expand_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_expand_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_expand_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_expand_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_expand_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_non_contig_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_non_contig_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___radd___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___radd___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___radd___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rand___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rand___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rand___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rand___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rand___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rand___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rdiv___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rdiv___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rdiv___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmod___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmod___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmod___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rmul___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rmul___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rmul___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___ror___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___ror___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___ror___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___ror___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___ror___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___ror___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rpow___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rpow___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rpow___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rsub___xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index___rsub___xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rsub___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rxor___xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rxor___xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rxor___xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rxor___xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rxor___xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index___rxor___xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_atan2_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_left_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_left_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_left_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_left_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_left_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_right_shift_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_right_shift_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_right_shift_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_right_shift_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_right_shift_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_index_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_index_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_index_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_complex_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_copysign_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_floor_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_div_no_rounding_mode_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_div_no_rounding_mode_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_no_rounding_mode_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_div_trunc_rounding_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_non_contig_index_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmax_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmin_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_index_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_hypot_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_hypot_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_igamma_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_index_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_igamma_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_igammac_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_non_contig_index_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_igammac_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_index_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_index_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_index_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_index_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_index_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_non_contig_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_ldexp_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_ldexp_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ldexp_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_mul_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_mul_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_mul_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_mul_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_nextafter_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_nextafter_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_polar_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_non_contig_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_rsub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_rsub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_rsub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_hermite_polynomial_h_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_h_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_hermite_polynomial_he_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_laguerre_polynomial_l_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_non_contig_special_legendre_polynomial_p_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_legendre_polynomial_p_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_t_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_u_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_v_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_shifted_chebyshev_polynomial_w_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_non_contig_special_xlog1py_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_xlog1py_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_special_zeta_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_true_divide_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_non_contig_true_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_true_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_non_contig_xlogy_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable___radd___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable___rdiv___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable___rmod___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable___rmul___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable___rpow___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable___rsub___xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_atan2_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_complex_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_copysign_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_div_floor_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_div_no_rounding_mode_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_div_trunc_rounding_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_fmax_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_fmin_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_hypot_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_igamma_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_igammac_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_not_broadcastable_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_not_broadcastable_ldexp_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_mul_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_nextafter_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_polar_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_rsub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_hermite_polynomial_h_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_hermite_polynomial_he_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_laguerre_polynomial_l_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_legendre_polynomial_p_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_shifted_chebyshev_polynomial_t_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_shifted_chebyshev_polynomial_u_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_shifted_chebyshev_polynomial_v_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_shifted_chebyshev_polynomial_w_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_not_broadcastable_special_xlog1py_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_special_zeta_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_true_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_not_broadcastable_xlogy_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_out_resize_warning_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_pow_inplace_resizing_exception_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_scalar_base_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_scalar_overloads_mem_overlap_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_scalar_type_promotion_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_pow_xpu_complex_extremal_failing_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_pow_xpu_complex_extremal_failing_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_pow_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_pow_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_pow_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_pow_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_pow_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_rdiv_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_rdiv_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_rdiv_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_rdiv_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_rdiv_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_rdiv_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_rdiv_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_rdiv_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_rdiv_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_reference_numerics_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_reference_numerics_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ERROR
test_reference_numerics_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:8264: RuntimeWarning: invalid value encountered in add
  ref=lambda input, other, *, alpha=1: np.add(input, other) if alpha == 1 \
ok
test_reference_numerics_extremal_values_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: divide by zero encountered in float_power
  expected = op.ref(l_numpy, r_numpy)
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: invalid value encountered in float_power
  expected = op.ref(l_numpy, r_numpy)
ok
test_reference_numerics_extremal_values_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:2069: RuntimeWarning: invalid value encountered in floor_divide
  return np.floor_divide(a, b)
ok
test_reference_numerics_extremal_values_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_reference_numerics_extremal_values_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: invalid value encountered in fmod
  expected = op.ref(l_numpy, r_numpy)
ok
test_reference_numerics_extremal_values_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_extremal_values_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_extremal_values_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: divide by zero encountered in power
  expected = op.ref(l_numpy, r_numpy)
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: invalid value encountered in power
  expected = op.ref(l_numpy, r_numpy)
ok
test_reference_numerics_extremal_values_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: overflow encountered in power
  expected = op.ref(l_numpy, r_numpy)
ok
test_reference_numerics_extremal_values_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: invalid value encountered in remainder
  expected = op.ref(l_numpy, r_numpy)
ok
test_reference_numerics_extremal_values_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:8407: RuntimeWarning: invalid value encountered in subtract
  ref=lambda input, other, *, alpha=1: np.subtract(input, np.multiply(alpha, other)),
ok
test_reference_numerics_extremal_values_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_extremal_values_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_extremal_values_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:56: RuntimeWarning: overflow encountered in float_power
  expected = op.ref(l_numpy, r_numpy)
ok
test_reference_numerics_large_values_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_large_values_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_reference_numerics_large_values_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_large_values_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_large_values_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_large_values_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_large_values_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_large_values_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_large_values_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_add_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_add_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_add_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_add_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_bitwise_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_bitwise_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_bitwise_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_bitwise_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_clamp_max_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_max_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_clamp_min_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_clamp_min_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_eq_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_eq_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_eq_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_eq_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_float_power_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_float_power_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_float_power_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_small_values_floor_divide_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_floor_divide_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_small_values_floor_divide_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ERROR
test_reference_numerics_small_values_fmod_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_fmod_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_small_values_gcd_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gcd_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gcd_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_small_values_gcd_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_ge_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ge_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_gt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_gt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_heaviside_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_heaviside_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_isclose_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_isclose_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_isclose_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_return_by_ref_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_jiterator_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_jiterator_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_reference_numerics_small_values_lcm_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lcm_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lcm_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lcm_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_le_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_le_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_logical_and_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_logical_and_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_and_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_logical_or_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_logical_or_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_or_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_logical_xor_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_logical_xor_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_logical_xor_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_lt_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_lt_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_max_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_max_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_maximum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_maximum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_min_binary_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_min_binary_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_minimum_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_minimum_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped "Doesn't support bool!"
test_reference_numerics_small_values_ne_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_ne_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_ne_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_pow_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_pow_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_pow_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_pow_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_pow_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_small_values_pow_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_small_values_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_small_values_pow_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... expected failure
test_reference_numerics_small_values_pow_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_remainder_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_small_values_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_small_values_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_small_values_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_reference_numerics_sub_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_sub_xpu_complex32 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_reference_numerics_sub_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_reference_numerics_sub_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_remainder_fmod_large_dividend_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_remainder_fmod_large_dividend_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_remainder_overflow_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_rpow_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_add_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_add_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_add_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_bitwise_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_bitwise_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_bitwise_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_clamp_max_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_clamp_max_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_clamp_min_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_clamp_min_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_eq_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_eq_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_eq_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_float_power_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_float_power_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_float_power_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_floor_divide_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_floor_divide_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_fmod_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_fmod_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_gcd_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_ge_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_ge_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_gt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_gt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_heaviside_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_heaviside_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_isclose_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_isclose_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_isclose_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_jiterator_binary_return_by_ref_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_scalar_support_jiterator_binary_return_by_ref_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_scalar_support_jiterator_binary_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_jiterator_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_scalar_support_jiterator_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_scalar_support_lcm_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_le_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_le_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_logical_and_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_logical_and_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_logical_and_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_logical_or_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_logical_or_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_logical_or_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_logical_xor_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_logical_xor_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_logical_xor_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_lt_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_lt_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_max_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_max_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_maximum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_maximum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_min_binary_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_min_binary_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_minimum_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_minimum_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_ne_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_ne_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_ne_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_pow_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_pow_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_pow_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_remainder_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_remainder_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_sub_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_scalar_support_sub_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_scalar_support_sub_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_signed_shift_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU)
Ensure that signed integer bit shifting works as expected. ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_signed_shift_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU)
Ensure that signed integer bit shifting works as expected. ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_signed_shift_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU)
Ensure that signed integer bit shifting works as expected. ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_signed_shift_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU)
Ensure that signed integer bit shifting works as expected. ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_sub_typing_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_sub_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_bool (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_complex128 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_sub_xpu_xpu_complex64 (__main__.TestBinaryUfuncsXPU) ... skipped 'dtype not support on XPU'
test_sub_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_int16 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_int32 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_int64 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_int8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_sub_xpu_xpu_uint8 (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cpu'
test_tensor_pow_tensor_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:727: RuntimeWarning: invalid value encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py:727: RuntimeWarning: divide by zero encountered in power
  np_res = np.power(to_np(base), to_np(np_exponent))
ok
test_trapezoid_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_true_divide_out_xpu_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU) ... ok
test_true_divide_out_xpu_xpu_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___radd___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___rand___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___rdiv___xpu (__main__.TestBinaryUfuncsXPU) ... expected failure
test_type_promotion___rmod___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___rmul___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___ror___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___rpow___xpu (__main__.TestBinaryUfuncsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py:863: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(other, dtype=dtype, device=self.device) ** self
ok
test_type_promotion___rsub___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion___rxor___xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_add_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_atan2_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_bitwise_and_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_bitwise_left_shift_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_bitwise_or_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_bitwise_xor_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_clamp_max_xpu (__main__.TestBinaryUfuncsXPU) ... FAIL
test_type_promotion__refs_clamp_min_xpu (__main__.TestBinaryUfuncsXPU) ... FAIL
test_type_promotion__refs_copysign_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion__refs_div_floor_rounding_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_div_no_rounding_mode_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_div_trunc_rounding_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_eq_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_float_power_xpu (__main__.TestBinaryUfuncsXPU) ... expected failure
test_type_promotion__refs_floor_divide_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_fmax_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_fmin_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_fmod_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_gcd_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_ge_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_gt_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_heaviside_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_hypot_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_igamma_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_igammac_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_isclose_xpu (__main__.TestBinaryUfuncsXPU) ... expected failure
test_type_promotion__refs_lcm_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_le_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_logical_and_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_logical_or_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_logical_xor_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_lt_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_maximum_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_minimum_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_mul_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_ne_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_nextafter_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_pow_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_remainder_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_rsub_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_special_zeta_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_sub_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion__refs_true_divide_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_add_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_atan2_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_bitwise_and_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_bitwise_left_shift_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion_bitwise_or_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_bitwise_right_shift_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion_bitwise_xor_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_clamp_max_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_clamp_min_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_complex_xpu (__main__.TestBinaryUfuncsXPU) ... expected failure
test_type_promotion_copysign_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_div_floor_rounding_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_div_no_rounding_mode_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_div_trunc_rounding_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_eq_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_float_power_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion_floor_divide_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_fmax_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion_fmin_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion_fmod_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_gcd_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_ge_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_gt_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_heaviside_xpu (__main__.TestBinaryUfuncsXPU) ... expected failure
test_type_promotion_hypot_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_igamma_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_igammac_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_isclose_xpu (__main__.TestBinaryUfuncsXPU) ... expected failure
test_type_promotion_jiterator_binary_return_by_ref_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_type_promotion_jiterator_binary_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_type_promotion_lcm_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_ldexp_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_le_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_logical_and_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_logical_or_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_logical_xor_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_lt_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_max_binary_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_maximum_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_min_binary_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_minimum_xpu (__main__.TestBinaryUfuncsXPU) ... ERROR
test_type_promotion_mul_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_ne_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_nextafter_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_polar_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipped!'
test_type_promotion_pow_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_remainder_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_rsub_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_special_chebyshev_polynomial_t_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_type_promotion_special_chebyshev_polynomial_u_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_type_promotion_special_chebyshev_polynomial_v_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_chebyshev_polynomial_w_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_hermite_polynomial_h_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_type_promotion_special_hermite_polynomial_he_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_type_promotion_special_laguerre_polynomial_l_xpu (__main__.TestBinaryUfuncsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_type_promotion_special_legendre_polynomial_p_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_shifted_chebyshev_polynomial_t_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_shifted_chebyshev_polynomial_u_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_shifted_chebyshev_polynomial_v_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_shifted_chebyshev_polynomial_w_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_type_promotion_special_xlog1py_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_special_zeta_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_sub_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_true_divide_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_type_promotion_xlogy_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_bfloat16_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_gradients_xpu_xpu_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_scalar_type_promotion_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xlogy_xlog1py_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_xpu_tensor_pow_scalar_tensor_xpu_xpu (__main__.TestBinaryUfuncsXPU) ... skipped 'Only runs on cuda'
test_zeta_xpu_xpu_bool_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_bool_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_float64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int16_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int32_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int64_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_int8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_bool (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_float32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_float64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_int16 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_int32 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_int64 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_int8 (__main__.TestBinaryUfuncsXPU) ... ok
test_zeta_xpu_xpu_uint8_uint8 (__main__.TestBinaryUfuncsXPU) ... ok

======================================================================
ERROR: test_batch_vs_slicing_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 234, in test_batch_vs_slicing
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_batch_vs_slicing_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 234, in test_batch_vs_slicing
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_batch_vs_slicing_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 234, in test_batch_vs_slicing
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_batch_vs_slicing_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 234, in test_batch_vs_slicing
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_batch_vs_slicing_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 234, in test_batch_vs_slicing
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_batch_vs_slicing_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 234, in test_batch_vs_slicing
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_size1_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 206, in test_contig_size1
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_size1_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 206, in test_contig_size1
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_size1_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 206, in test_contig_size1
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_contig_size1_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 206, in test_contig_size1
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_contig_size1_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 206, in test_contig_size1
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_size1_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 206, in test_contig_size1
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_size1_large_dim_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 225, in test_contig_size1_large_dim
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_size1_large_dim_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 225, in test_contig_size1_large_dim
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_size1_large_dim_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 225, in test_contig_size1_large_dim
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_contig_size1_large_dim_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 225, in test_contig_size1_large_dim
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_contig_size1_large_dim_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 225, in test_contig_size1_large_dim
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_size1_large_dim_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 225, in test_contig_size1_large_dim
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_vs_every_other_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 121, in test_contig_vs_every_other
    expected = op(lhs, rhs)[::2]
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_vs_every_other_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 121, in test_contig_vs_every_other
    expected = op(lhs, rhs)[::2]
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_vs_every_other_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 121, in test_contig_vs_every_other
    expected = op(lhs, rhs)[::2]
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_contig_vs_every_other_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 121, in test_contig_vs_every_other
    expected = op(lhs, rhs)[::2]
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_vs_every_other_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 121, in test_contig_vs_every_other
    expected = op(lhs, rhs)[::2]
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_vs_transposed_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 135, in test_contig_vs_transposed
    expected = op(lhs, rhs).T
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_vs_transposed_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 135, in test_contig_vs_transposed
    expected = op(lhs, rhs).T
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_contig_vs_transposed_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 135, in test_contig_vs_transposed
    expected = op(lhs, rhs).T
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_contig_vs_transposed_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 135, in test_contig_vs_transposed
    expected = op(lhs, rhs).T
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_contig_vs_transposed_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 135, in test_contig_vs_transposed
    expected = op(lhs, rhs).T
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_contig_vs_transposed_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 135, in test_contig_vs_transposed
    expected = op(lhs, rhs).T
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_non_contig_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 153, in test_non_contig
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_non_contig_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 153, in test_non_contig
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_non_contig_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 153, in test_non_contig
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_non_contig_expand_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 186, in test_non_contig_expand
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_non_contig_expand_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 186, in test_non_contig_expand
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_non_contig_expand_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 186, in test_non_contig_expand
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_non_contig_expand_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 186, in test_non_contig_expand
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_non_contig_expand_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 186, in test_non_contig_expand
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_non_contig_expand_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 186, in test_non_contig_expand
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_non_contig_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 153, in test_non_contig
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_non_contig_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 153, in test_non_contig
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_non_contig_index_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 170, in test_non_contig_index
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_non_contig_index_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 170, in test_non_contig_index
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_non_contig_index_complex_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 170, in test_non_contig_index
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Expected both inputs to be Float or Double tensors but got Half and Half

======================================================================
ERROR: test_non_contig_index_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 170, in test_non_contig_index
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_non_contig_index_igamma_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 170, in test_non_contig_index
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igamma_xpu" not implemented for 'Half'

======================================================================
ERROR: test_non_contig_index_igammac_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 170, in test_non_contig_index
    expected = op(lhs, rhs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "igammac_xpu" not implemented for 'Half'

======================================================================
ERROR: test_pow_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 706, in test_pow
    self._do_pow_for_exponents(m1, complex_exponents, pow, 0.001)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 670, in _do_pow_for_exponents
    res1 = torch.pow(m1[4], num)
RuntimeError: "copy_" not implemented for 'ComplexHalf'

======================================================================
ERROR: test_reference_numerics_clamp_max_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 77, in test_reference_numerics
    self._test_reference_numerics(dtype, op, gen, equal_nan=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 55, in _test_reference_numerics
    actual = op(l, r)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_max_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_reference_numerics_clamp_min_xpu_bool (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 77, in test_reference_numerics
    self._test_reference_numerics(dtype, op, gen, equal_nan=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 55, in _test_reference_numerics
    actual = op(l, r)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "clamp_min_dpcpp" not implemented for 'Bool'

======================================================================
ERROR: test_reference_numerics_extremal_values_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 97, in test_reference_numerics_extremal_values
    self._test_reference_numerics(dtype, op, gen, equal_nan=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 55, in _test_reference_numerics
    actual = op(l, r)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_reference_numerics_large_values_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 91, in test_reference_numerics_large_values
    self._test_reference_numerics(dtype, op, gen, equal_nan=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 55, in _test_reference_numerics
    actual = op(l, r)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_reference_numerics_small_values_fmod_xpu_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 85, in test_reference_numerics_small_values
    self._test_reference_numerics(dtype, op, gen, equal_nan=True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 55, in _test_reference_numerics
    actual = op(l, r)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "fmod_xpu" not implemented for 'BFloat16'

======================================================================
ERROR: test_type_promotion_atan2_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "atan2" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_bitwise_and_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 286, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.float16)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "bitwise_and_xpu" not implemented for 'Half'

======================================================================
ERROR: test_type_promotion_bitwise_or_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 286, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.float16)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "bitwise_or_xpu" not implemented for 'Half'

======================================================================
ERROR: test_type_promotion_bitwise_xor_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 286, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.float16)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "bitwise_xor_xpu" not implemented for 'Half'

======================================================================
ERROR: test_type_promotion_copysign_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "copysign_out" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_gcd_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 286, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.float16)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "gcd" not implemented for 'Half'

======================================================================
ERROR: test_type_promotion_lcm_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 286, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.float16)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "lcm" not implemented for 'Half'

======================================================================
ERROR: test_type_promotion_logical_and_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "logical_and_kernel" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_logical_or_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "logical_or_kernel" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_logical_xor_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "logical_xor_kernel" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_max_binary_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "max_elementwise_dpcpp" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_maximum_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "max_elementwise_dpcpp" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_min_binary_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "min_elementwise_dpcpp" not implemented for 'ComplexFloat'

======================================================================
ERROR: test_type_promotion_minimum_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 293, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32, out=out).dtype, torch.complex64)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: "min_elementwise_dpcpp" not implemented for 'ComplexFloat'

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 5 / 20 (25.0%)
Greatest absolute difference: 1 at index (0, 4) (up to 0.001 allowed)
Greatest relative difference: inf at index (0, 4) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int16_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 5 / 20 (25.0%)
Greatest absolute difference: 1 at index (0, 4) (up to 0.001 allowed)
Greatest relative difference: inf at index (0, 4) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 20 (5.0%)
Greatest absolute difference: 1 at index (1, 5) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 5) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int32_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 20 (5.0%)
Greatest absolute difference: 1 at index (1, 9) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 9) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 20 (5.0%)
Greatest absolute difference: 1 at index (1, 8) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 8) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int64_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 2 / 20 (10.0%)
Greatest absolute difference: 1 at index (0, 2) (up to 0.001 allowed)
Greatest relative difference: inf at index (0, 2) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 20 (5.0%)
Greatest absolute difference: 1 at index (1, 8) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 8) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_int8_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 20 (5.0%)
Greatest absolute difference: 1 at index (1, 3) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 3) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_bfloat16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 2 / 2 (100.0%)
Greatest absolute difference: 1 at index (0, 0) (up to 0.001 allowed)
Greatest relative difference: inf at index (0, 0) (up to 0.0 allowed)

======================================================================
FAIL: test_comparison_ops_type_promotion_and_broadcasting_xpu_xpu_uint8_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1769, in test_comparison_ops_type_promotion_and_broadcasting
    compare_with_numpy_bin_op(torch_op, numpy_op, a, b)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 1757, in compare_with_numpy_bin_op
    self.compare_with_numpy(lambda inp: torch_fn(inp, y, out=out) if out else torch_fn(inp, y), lambda inp: np_fn(inp, y_np, out=out) if out else np_fn(inp, y_np), x_np)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2404, in compare_with_numpy
    self.assertEqual(np_result, torch_result, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 2 (50.0%)
Greatest absolute difference: 1 at index (1, 0) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 0) (up to 0.0 allowed)

======================================================================
FAIL: test_div_rounding_numpy_xpu_xpu_float16 (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 564, in test_div_rounding_numpy
    self.assertEqual(actual, expect, exact_device=False, exact_dtype=exact_dtype)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 2 / 4096 (0.0%)
Greatest absolute difference: 0.001953125 at index (1820,) (up to 0.001 allowed)
Greatest relative difference: 0.0020757654385054488 at index (3273,) (up to 0.001 allowed)

======================================================================
FAIL: test_type_promotion__refs_clamp_max_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 268, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32).dtype, torch.int32)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Object comparison failed: torch.int16 != torch.int32

======================================================================
FAIL: test_type_promotion__refs_clamp_min_xpu (__main__.TestBinaryUfuncsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_binary_ufuncs.py", line 268, in test_type_promotion
    self.assertEqual(op(lhs_i16, rhs_i32).dtype, torch.int32)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Object comparison failed: torch.int16 != torch.int32

----------------------------------------------------------------------
Ran 8285 tests in 100.843s

FAILED (failures=13, errors=66, skipped=1992, expected failures=14)
Raised CalledProcessError: return code 1.