MESA: warning: Driver does not support the 0xbd5 PCI ID.
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_proxy_tensor.py:123: UserWarning: Couldn't import torchvision. Some of our tests use it, try to install it with commands from pytorch.org, post-fixed with `--no-deps` to avoid overwriting the pytorch installation
  warnings.warn("Couldn't import torchvision. Some of our tests use it, try "
test_conj_view_H_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_T_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___getitem___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___radd___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___rdiv___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___rmatmul___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___rmul___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___rpow___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view___rsub___xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_abs_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_acos_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_acosh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_add_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_addcdiv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_addr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_all_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_allclose_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_any_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_as_strided_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_asin_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_asinh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_atan_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_atanh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_atleast_1d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_atleast_2d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_atleast_3d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_broadcast_tensors_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_broadcast_to_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_cat_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_chunk_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_clone_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_column_stack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_conj_physical_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_conj_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_constant_pad_nd_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_contiguous_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_cos_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_cosh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_cumsum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_diag_embed_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_diagonal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_div_no_rounding_mode_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_dsplit_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_dstack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_empty_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_empty_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_eq_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_exp_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_expand_as_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_expand_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_eye_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_fft_fft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_fft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_fftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_fftshift_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_hfft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_hfft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_hfftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_ifft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_ifft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_ifftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_ifftshift_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_irfft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_irfft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fft_irfftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view__refs_fill_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_flatten_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_flip_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_fliplr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_flipud_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_float_power_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_hsplit_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_hstack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_imag_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_index_add_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_index_copy_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_index_fill_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_index_select_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_isclose_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_isfinite_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_isinf_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_isnan_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_isreal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_linalg_matrix_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_linalg_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_linalg_svd_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_linalg_svdvals_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_linalg_vector_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_linspace_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_log10_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_log2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_log_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_logical_and_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_logical_not_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_logical_or_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_logical_xor_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_logspace_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_masked_fill_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_meshgrid_list_of_tensors_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_meshgrid_variadic_tensors_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_movedim_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_mul_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_narrow_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_ne_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_neg_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_new_empty_strided_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_new_empty_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_new_full_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_new_ones_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_new_zeros_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_nn_functional_l1_loss_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_nn_functional_pairwise_distance_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_nn_functional_tanhshrink_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_ones_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_permute_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_positive_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_pow_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_prod_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_randn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_ravel_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_real_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_reciprocal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_repeat_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_reshape_as_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_reshape_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_roll_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_rot90_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_rsqrt_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_rsub_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sgn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sigmoid_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sin_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sinc_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sinh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sqrt_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_square_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_squeeze_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_stack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_std_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_std_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sub_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sum_to_size_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_sum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_t_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_tan_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_tanh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_tensor_split_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_to_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_trace_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_transpose_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_tril_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_triu_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_true_divide_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_unbind_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_unflatten_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_unfold_copy_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_unsqueeze_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_var_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_var_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_view_as_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_view_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_vsplit_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_vstack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_where_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view__refs_zeros_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_abs_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_acos_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_acosh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_add_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addbmm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addcdiv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addcmul_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addmm_decomposed_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addmm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addmv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_addr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_all_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_allclose_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_angle_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_any_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_argwhere_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_as_strided_scatter_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_as_strided_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_asin_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_asinh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_atan_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_atanh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_atleast_1d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_atleast_2d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_atleast_3d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_baddbmm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_bfloat16_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_block_diag_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_bmm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_bool_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_broadcast_tensors_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_broadcast_to_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_byte_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cartesian_prod_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cat_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_chalf_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_char_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cholesky_inverse_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cholesky_solve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cholesky_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_chunk_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_clone_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_column_stack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_combinations_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_conj_physical_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_conj_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_constant_pad_nd_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_contiguous_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_corrcoef_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cos_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cosh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_count_nonzero_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cov_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cross_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cumprod_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cumsum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_cumulative_trapezoid_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_diag_embed_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_diag_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_diagflat_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_diagonal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_diff_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_dist_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_div_no_rounding_mode_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_dot_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_double_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_dsplit_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_dstack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_einsum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_empty_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_empty_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_eq_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_equal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_exp_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_expand_as_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_expand_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_eye_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_fft_fft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_fft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_fftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_fftshift_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_hfft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_hfft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_hfftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_ifft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_ifft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_ifftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_ifftshift_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_irfft2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_irfft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fft_irfftn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_conj_view_fill_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_flatten_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_flip_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_fliplr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_flipud_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_float_power_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_float_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_full_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_gather_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_geqrf_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_gradient_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_half_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_hsplit_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_hstack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_imag_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_index_add_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_index_copy_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_index_fill_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_index_put_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_index_select_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_inner_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_int_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_isclose_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_isfinite_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_isinf_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_isnan_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_isreal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_istft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_jiterator_2inputs_2outputs_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_jiterator_4inputs_with_extra_args_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_jiterator_binary_return_by_ref_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_jiterator_binary_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_jiterator_unary_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_kron_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_ldexp_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_lerp_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_cholesky_ex_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_cholesky_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_cond_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_cross_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_det_singular_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_det_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_eig_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_eigh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_eigvals_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_eigvalsh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_householder_product_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_inv_ex_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_inv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_ldl_factor_ex_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_ldl_factor_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_ldl_solve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_lstsq_grad_oriented_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_lstsq_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_lu_factor_ex_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_lu_factor_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_lu_solve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_lu_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_matrix_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_matrix_power_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_matrix_rank_hermitian_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_matrix_rank_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_multi_dot_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_norm_subgradients_at_zero_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_pinv_hermitian_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_pinv_singular_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_pinv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_qr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_slogdet_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_solve_ex_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_solve_triangular_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_solve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_svd_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_svdvals_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_tensorinv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_tensorsolve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_vander_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_vecdot_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linalg_vector_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_linspace_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_log10_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_log2_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_log_softmax_dtype_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_log_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_logdet_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_logical_and_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_logical_not_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_logical_or_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_logical_xor_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_logspace_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_long_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_lu_solve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_lu_unpack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_lu_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_mH_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_mT_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_cumprod_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_cumsum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_fill_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_normalize_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_prod_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_scatter_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_select_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_std_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_sum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_masked_var_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_matmul_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_matrix_exp_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_meshgrid_list_of_tensors_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_meshgrid_variadic_tensors_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_mm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_movedim_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_mul_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_mv_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_narrow_copy_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_narrow_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_ne_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_neg_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_new_empty_strided_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_new_empty_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_new_full_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_new_ones_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_new_zeros_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_conv1d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_conv2d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_conv_transpose1d_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_feature_alpha_dropout_without_train_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_l1_loss_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_linear_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_normalize_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pad_circular_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pad_constant_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pad_reflect_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pad_replicate_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pairwise_distance_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pixel_shuffle_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_pixel_unshuffle_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_silu_complex_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_softmin_with_dtype_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_softsign_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_tanhshrink_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_triplet_margin_loss_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_triplet_margin_with_distance_loss_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nn_functional_unfold_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_nonzero_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_norm_fro_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_norm_inf_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_norm_nuc_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_norm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_ones_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_ones_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_ormqr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_outer_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_permute_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_pinverse_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_positive_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_pow_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_prod_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_put_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_qr_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_rand_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_randn_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_randn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_ravel_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_real_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_reciprocal_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_renorm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_repeat_interleave_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_repeat_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_reshape_as_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_reshape_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_resize__xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_resize_as__xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_resolve_conj_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_resolve_neg_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_roll_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_rot90_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_rsqrt_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_rsub_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_scatter_add_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_scatter_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_select_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sgn_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_short_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sigmoid_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sin_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sinc_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sinh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_slice_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_softmax_with_dtype_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sparse_sampled_addmm_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_split_list_args_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_split_with_sizes_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_split_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sqrt_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_square_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_squeeze_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_stack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_std_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_std_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_stft_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sub_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sum_to_size_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_sum_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_svd_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_symeig_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_t_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_take_along_dim_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_take_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_tan_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_tanh_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_tensor_split_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_tensordot_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_tile_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_to_sparse_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_to_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_trace_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_transpose_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_trapezoid_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_trapz_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_triangular_solve_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_tril_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_triu_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_true_divide_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_unbind_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_unflatten_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_unfold_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_uniform_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_unsqueeze_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_var_mean_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_var_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_vdot_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_view_as_real_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_view_as_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_view_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_vsplit_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_vstack_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_where_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_zero__xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_zeros_like_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_conj_view_zeros_xpu_complex64 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_H_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_T_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___getitem___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___radd___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___rdiv___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___rmatmul___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___rmul___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___rpow___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view___rsub___xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_abs_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_acos_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_acosh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_add_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_addcdiv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_addr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_all_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_allclose_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_any_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_as_strided_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_asin_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_asinh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_atan_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_atanh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_atleast_1d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_atleast_2d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_atleast_3d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_broadcast_tensors_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_broadcast_to_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_cat_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_chunk_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_clone_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_column_stack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_conj_physical_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_conj_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_constant_pad_nd_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_contiguous_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_cos_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_cosh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_cumsum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_diag_embed_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_diagonal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_div_no_rounding_mode_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_dsplit_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_dstack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_empty_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_empty_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_eq_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_exp_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_expand_as_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_expand_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_eye_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_fft_fft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_fft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_fftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_fftshift_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_hfft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_hfft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_hfftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_ifft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_ifft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_ifftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_ifftshift_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_irfft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_irfft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fft_irfftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view__refs_fill_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_flatten_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_flip_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_fliplr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_flipud_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_float_power_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_hsplit_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_hstack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_imag_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_index_add_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_index_copy_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_index_fill_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_index_select_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_isclose_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_isfinite_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_isinf_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_isnan_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_isreal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_linalg_matrix_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_linalg_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_linalg_svd_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_linalg_svdvals_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_linalg_vector_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_linspace_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_log10_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_log2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_log_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_logical_and_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_logical_not_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_logical_or_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_logical_xor_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_logspace_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_masked_fill_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_movedim_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_mul_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_narrow_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_ne_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_neg_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_new_empty_strided_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_new_empty_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_new_full_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_new_ones_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_new_zeros_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_nn_functional_l1_loss_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_nn_functional_pairwise_distance_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_nn_functional_tanhshrink_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_ones_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_permute_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_positive_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_pow_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_prod_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_randn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_ravel_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_real_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_reciprocal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_repeat_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_reshape_as_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_reshape_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_roll_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_rot90_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_rsqrt_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_rsub_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sgn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sigmoid_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sin_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sinc_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sinh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sqrt_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_square_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_squeeze_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_stack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_std_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_std_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sub_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sum_to_size_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_sum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_t_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_tan_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_tanh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_tensor_split_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_to_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_trace_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_transpose_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_tril_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_triu_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_true_divide_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_unbind_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_unflatten_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_unfold_copy_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_unsqueeze_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_var_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_var_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_view_as_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_view_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_vsplit_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_vstack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_where_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view__refs_zeros_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_abs_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_acos_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_acosh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_add_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addbmm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addcdiv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addcmul_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addmm_decomposed_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addmm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addmv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_addr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_all_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_allclose_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_angle_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_any_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_argwhere_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_as_strided_scatter_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_as_strided_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_asin_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_asinh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_atan_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_atanh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_atleast_1d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_atleast_2d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_atleast_3d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_baddbmm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_bfloat16_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_block_diag_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_bmm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_bool_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_broadcast_tensors_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_broadcast_to_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_byte_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cartesian_prod_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cat_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_chalf_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_char_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cholesky_inverse_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cholesky_solve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cholesky_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_chunk_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_clone_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_column_stack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_combinations_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_conj_physical_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_conj_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_constant_pad_nd_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_contiguous_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_corrcoef_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cos_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cosh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_count_nonzero_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cov_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cross_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cumprod_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cumsum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_cumulative_trapezoid_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_diag_embed_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_diag_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_diagflat_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_diagonal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_diff_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_dist_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_div_no_rounding_mode_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_dot_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_double_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_dsplit_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_dstack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_einsum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_empty_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_empty_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_eq_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_equal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_exp_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_expand_as_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_expand_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_eye_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_fft_fft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_fft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_fftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_fftshift_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_hfft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_hfft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_hfftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_ifft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_ifft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_ifftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_ifftshift_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_irfft2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_irfft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fft_irfftn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_conj_view_fill_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_flatten_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_flip_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_fliplr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_flipud_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_float_power_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_float_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_full_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_gather_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_geqrf_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_gradient_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_half_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_hsplit_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_hstack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_imag_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_index_add_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_index_copy_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_index_fill_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_index_put_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_index_select_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_inner_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_int_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_isclose_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_isfinite_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_isinf_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_isnan_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_isreal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_istft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_jiterator_2inputs_2outputs_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_jiterator_4inputs_with_extra_args_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_jiterator_binary_return_by_ref_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_jiterator_binary_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_jiterator_unary_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_kron_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_ldexp_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_lerp_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_cholesky_ex_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_cholesky_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_cond_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_cross_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_det_singular_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_det_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_eig_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_eigh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_eigvals_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_eigvalsh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_householder_product_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_inv_ex_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_inv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_ldl_factor_ex_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_ldl_factor_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_ldl_solve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_lstsq_grad_oriented_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_lstsq_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_lu_factor_ex_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_lu_factor_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_lu_solve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_lu_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_matrix_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_matrix_power_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_matrix_rank_hermitian_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_matrix_rank_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_multi_dot_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_norm_subgradients_at_zero_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_pinv_hermitian_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_pinv_singular_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_pinv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_qr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_slogdet_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_solve_ex_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_solve_triangular_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_solve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_svd_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_svdvals_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_tensorinv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_tensorsolve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_vander_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_vecdot_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linalg_vector_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_linspace_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_log10_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_log2_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_log_softmax_dtype_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_log_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_logdet_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_logical_and_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_logical_not_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_logical_or_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_logical_xor_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_logspace_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_long_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_lu_solve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_lu_unpack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_lu_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_mH_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_mT_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_cumprod_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_cumsum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_fill_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_normalize_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_prod_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_scatter_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_select_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_std_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_sum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_masked_var_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_matmul_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_matrix_exp_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_meshgrid_list_of_tensors_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_meshgrid_variadic_tensors_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_mm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_movedim_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_mul_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_mv_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_narrow_copy_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_narrow_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_ne_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_neg_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_new_empty_strided_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_new_empty_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_new_full_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_new_ones_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_new_zeros_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_conv1d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_conv2d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_conv_transpose1d_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_feature_alpha_dropout_without_train_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_l1_loss_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_linear_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_normalize_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pad_circular_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pad_constant_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pad_reflect_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pad_replicate_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pairwise_distance_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pixel_shuffle_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_pixel_unshuffle_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_silu_complex_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_softmin_with_dtype_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_softsign_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_tanhshrink_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_triplet_margin_loss_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_triplet_margin_with_distance_loss_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nn_functional_unfold_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_nonzero_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_norm_fro_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_norm_inf_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_norm_nuc_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_norm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_ones_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_ones_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_ormqr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_outer_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_permute_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_pinverse_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_positive_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_pow_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_prod_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_put_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_qr_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_rand_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_randn_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_randn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_ravel_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_real_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_reciprocal_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_renorm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_repeat_interleave_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_repeat_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_reshape_as_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_reshape_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_resize__xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_resize_as__xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_resolve_conj_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_resolve_neg_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_roll_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_rot90_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_rsqrt_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_rsub_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_scatter_add_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_scatter_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_select_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sgn_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_short_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sigmoid_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sin_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sinc_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sinh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_slice_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_softmax_with_dtype_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sparse_sampled_addmm_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_split_list_args_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_split_with_sizes_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_split_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sqrt_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_square_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_squeeze_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_stack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_std_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_std_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_stft_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sub_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sum_to_size_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_sum_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_svd_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_symeig_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_t_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_take_along_dim_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_take_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_tan_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_tanh_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_tensor_split_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_tensordot_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_tile_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_to_sparse_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_to_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_trace_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_transpose_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_trapezoid_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_trapz_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_triangular_solve_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_tril_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_triu_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_true_divide_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_unbind_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_unflatten_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_unfold_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_uniform_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_unsqueeze_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_var_mean_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_var_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_vdot_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_view_as_real_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_view_as_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_view_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_vsplit_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_vstack_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_where_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_zero__xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_zeros_like_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_conj_view_zeros_xpu_complex128 (__main__.TestMathBitsXPU) ... skipped 'dtype not support on XPU'
test_neg_view_H_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_T_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___getitem___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___radd___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___rdiv___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___rmatmul___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___rmod___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___rmul___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view___rpow___xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py:863: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(other, dtype=dtype, device=self.device) ** self
ok
test_neg_view___rsub___xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_abs_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_acos_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_acosh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_add_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_addcdiv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_addr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_all_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_allclose_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_amax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_amin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_any_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_arange_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view__refs_as_strided_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Errors when storage_offset is included'
test_neg_view__refs_asin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_asinh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_atan2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_atan_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_atanh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_atleast_1d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_atleast_2d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_atleast_3d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_broadcast_tensors_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_broadcast_to_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_cat_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_ceil_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_chunk_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_clamp_max_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_clamp_min_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_clamp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_clone_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_column_stack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_conj_physical_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_conj_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_constant_pad_nd_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_contiguous_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_copysign_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_cos_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_cosh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_cumsum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_diag_embed_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_diagonal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_digamma_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_div_floor_rounding_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_div_no_rounding_mode_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_div_trunc_rounding_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_dsplit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_dstack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_empty_like_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Expected: empty is not comparable'
test_neg_view__refs_empty_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Expected: empty is not comparable'
test_neg_view__refs_eq_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_erf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_erfc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_erfinv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_exp2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_exp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_expand_as_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_expand_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_expm1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_eye_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view__refs_fft_fft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_fft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_fftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_fftshift_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_hfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_hfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_hfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ifft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ifft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ifftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ifftshift_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ihfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ihfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_ihfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_irfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_irfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_irfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_rfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_rfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fft_rfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view__refs_fill_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_flatten_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_flip_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_fliplr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_flipud_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_float_power_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_floor_divide_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_floor_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_fmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_fmin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_fmod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_frac_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_ge_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_gt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_heaviside_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_hsplit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_hstack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_hypot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_i0_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_igamma_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_igammac_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_index_add_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_index_copy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_index_fill_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_index_select_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isclose_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isfinite_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isinf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isnan_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isneginf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isposinf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_isreal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_le_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_lgamma_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_linalg_matrix_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_linalg_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_linalg_svd_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_linalg_svdvals_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_linalg_vector_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_linspace_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view__refs_log10_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_log1p_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_log2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_log_softmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_log_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_logical_and_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_logical_not_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_logical_or_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_logical_xor_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_logspace_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view__refs_logsumexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_lt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_masked_fill_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_maximum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_meshgrid_list_of_tensors_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_meshgrid_variadic_tensors_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_minimum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_movedim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_mul_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nan_to_num_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_narrow_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_native_layer_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_ne_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_neg_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_new_empty_strided_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Expected: empty_strided is not comparable'
test_neg_view__refs_new_empty_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Expected: empty is not comparable'
test_neg_view__refs_new_full_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_new_ones_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_new_zeros_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nextafter_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_celu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_dropout_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Expected: dropout is not comparable'
test_neg_view__refs_nn_functional_elu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_gelu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_glu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_hardshrink_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_hardtanh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_hinge_embedding_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_huber_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_l1_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_layer_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view__refs_nn_functional_leaky_relu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_margin_ranking_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_mish_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_mse_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_pairwise_distance_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_pdist_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_poisson_nll_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_prelu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_relu6_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_relu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_selu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_softplus_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_softshrink_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_tanhshrink_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_nn_functional_threshold_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_ones_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view__refs_permute_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_positive_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_pow_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_prod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_randn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Test expects tensor input'
test_neg_view__refs_ravel_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_real_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_reciprocal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_remainder_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_repeat_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_reshape_as_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_reshape_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_roll_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_rot90_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_round_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_rsqrt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_rsub_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sgn_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sigmoid_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sign_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_signbit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sinc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sinh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_softmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_bessel_j0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_bessel_j0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_bessel_j0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view__refs_special_bessel_j1_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_bessel_j1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_bessel_j1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view__refs_special_i0e_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_i1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_i1e_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_logit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_multigammaln_mvlgamma_p_1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_multigammaln_mvlgamma_p_3_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_multigammaln_mvlgamma_p_5_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_special_spherical_bessel_j0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_spherical_bessel_j0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_spherical_bessel_j0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view__refs_special_zeta_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sqrt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_square_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_squeeze_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_stack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_std_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_std_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sub_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sum_to_size_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_sum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_t_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_tan_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_tanh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_tensor_split_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_to_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_trace_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_transpose_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_tril_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_triu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_true_divide_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_trunc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_unbind_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_unflatten_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_unfold_copy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_unsqueeze_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_var_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_var_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_view_as_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_view_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_vsplit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_vstack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_where_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view__refs_zeros_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_abs_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_acos_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_acosh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_add_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addbmm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addcdiv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addcmul_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addmm_decomposed_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addmm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addmv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_addr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_all_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_allclose_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_amax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_amin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_aminmax_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_neg_view_angle_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_any_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_arange_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_argmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_argmin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_argsort_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_argwhere_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_as_strided_scatter_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Works for float64, fails for everything else'
test_neg_view_as_strided_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Errors when storage_offset is included'
test_neg_view_asin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_asinh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_atan2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_atan_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_atanh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_atleast_1d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_atleast_2d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_atleast_3d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_baddbmm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_bernoulli_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_bfloat16_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_block_diag_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_bmm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_bool_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_broadcast_tensors_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_broadcast_to_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_bucketize_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_byte_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cartesian_prod_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_cat_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cdist_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [2, 2, 3, 2], which does not match the required output shape [2, 3, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Resize.cpp:17.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [2, 2, 4, 2], which does not match the required output shape [2, 4, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Resize.cpp:17.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
ERROR
test_neg_view_ceil_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_chalf_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_char_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cholesky_inverse_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex.L' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex.L' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_cholesky_solve_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex.L' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex.L' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_cholesky_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_solve_triangular' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_solve_triangular' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:592 [kernel]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_chunk_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_clamp_max_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_clamp_min_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_clamp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_clone_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_column_stack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_combinations_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_complex_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_conj_physical_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_conj_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_constant_pad_nd_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_contiguous_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_copysign_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_corrcoef_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cos_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cosh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_count_nonzero_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cov_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py:1068: UserWarning: cov(): degrees of freedom is <= 0 (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Correlation.cpp:100.)
  return self.op(*args, **kwargs)
ok
test_neg_view_cross_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_cummax_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::_cummax_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummax_helper' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_cummin_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::_cummin_helper' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_cummin_helper' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_cumprod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cumsum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_cumulative_trapezoid_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_deg2rad_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_diag_embed_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_diag_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_diagflat_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_diagonal_scatter_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_diagonal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_diff_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_digamma_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_dist_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_div_floor_rounding_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_div_no_rounding_mode_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_div_trunc_rounding_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_dot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_double_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_dsplit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_dstack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_einsum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_empty_like_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_empty_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_eq_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_equal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_erf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_erfc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_erfinv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_exp2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_exp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_expand_as_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_expand_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_expm1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_eye_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_fft_fft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_fft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_fftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_fftshift_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_hfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_hfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_hfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ifft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ifft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ifftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ifftshift_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ihfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ihfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_ihfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_irfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_irfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_irfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_rfft2_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_rfft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fft_rfftn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "case has skip key and won't run on XPU"
test_neg_view_fill_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_flatten_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_flip_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_fliplr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_flipud_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_float_power_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_float_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_floor_divide_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_floor_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_fmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_fmin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_fmod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_frac_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_frexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_full_like_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_gather_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_ge_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_geqrf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_gradient_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_gt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_half_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_heaviside_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_histc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_histogram_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::histogram.bin_ct' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::histogram.bin_ct' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_histogramdd_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::_histogramdd_bin_edges' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_histogramdd_bin_edges' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_hsplit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_hstack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_hypot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_i0_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_igamma_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_igammac_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_index_add_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_index_copy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_index_fill_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_index_put_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Operation not tested with tensors with negative bit.'
test_neg_view_index_reduce_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::index_reduce.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_reduce.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_index_select_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_inner_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_int_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isclose_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isfinite_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isinf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isnan_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isneginf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isposinf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_isreal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_istft_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py:1068: UserWarning: istft will require a complex-valued input tensor in a future PyTorch release. Matching the output from stft with return_complex=True.  (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/SpectralOps.cpp:978.)
  return self.op(*args, **kwargs)
ERROR
test_neg_view_jiterator_2inputs_2outputs_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Only runs on cuda'
test_neg_view_jiterator_4inputs_with_extra_args_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Only runs on cuda'
test_neg_view_jiterator_binary_return_by_ref_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Only runs on cuda'
test_neg_view_jiterator_binary_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Only runs on cuda'
test_neg_view_jiterator_unary_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Only runs on cuda'
test_neg_view_kron_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_kthvalue_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_ldexp_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_le_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_lerp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_lgamma_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_cholesky_ex_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex.L' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex.L' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_cholesky_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cholesky_ex.L' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cholesky_ex.L' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_cond_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_cross_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_cross.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_cross.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_det_singular_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_det_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_eig_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_eigh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_eigvals_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_eigvalsh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_householder_product_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_inv_ex_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_inv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_ldl_factor_ex_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_ldl_factor_ex.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_ldl_factor_ex.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_ldl_factor_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_ldl_factor_ex.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_ldl_factor_ex.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_ldl_solve_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_ldl_factor_ex.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_ldl_factor_ex.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_lstsq_grad_oriented_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_lstsq.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_lstsq.out' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_lstsq_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_lstsq.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_lstsq.out' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_lu_factor_ex_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_lu_factor_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_lu_solve_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_solve_triangular' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_solve_triangular' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:592 [kernel]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_lu_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_lu.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_lu.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_matrix_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_matrix_power_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_matrix_rank_hermitian_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_matrix_rank_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_multi_dot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_norm_subgradients_at_zero_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_pinv_hermitian_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_pinv_singular_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/definitions/linalg.py:729: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2349.)
  torch.rand(*batch, m, k, device=device, dtype=dtype)
FAIL
test_neg_view_linalg_pinv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_qr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_slogdet_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_solve_ex_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_solve_triangular_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::linalg_solve_triangular' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::linalg_solve_triangular' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:22 [kernel]\nNegative: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:592 [kernel]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_linalg_solve_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_svd_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_svdvals_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_tensorinv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_tensorsolve_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_vander_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_vecdot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linalg_vector_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_linspace_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_log10_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_log1p_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_log2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_log_softmax_dtype_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_log_softmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_log_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logaddexp2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logaddexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logcumsumexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logdet_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logical_and_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logical_not_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logical_or_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logical_xor_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_logspace_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_logsumexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_long_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_lt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_lu_solve_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_lu_unpack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_lu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mH_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mT_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_amax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_amin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_argmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_argmin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_cumprod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_cumsum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_fill_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_log_softmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_logaddexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_logsumexp_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_median_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_normalize_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_prod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_scatter_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_select_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_softmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_softmin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_std_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_sum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_masked_var_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_matmul_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_matrix_exp_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_max_binary_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_max_reduction_no_dim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_max_reduction_with_dim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_maximum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_median_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_meshgrid_list_of_tensors_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_meshgrid_variadic_tensors_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_min_binary_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_min_reduction_no_dim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_min_reduction_with_dim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_minimum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mode_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_movedim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_msort_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mul_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_multinomial_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_mv_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mvlgamma_mvlgamma_p_1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mvlgamma_mvlgamma_p_3_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_mvlgamma_mvlgamma_p_5_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nan_to_num_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nanmean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nanmedian_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nanquantile_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nansum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_narrow_copy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_narrow_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_native_batch_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_native_layer_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_ne_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_neg_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_new_empty_strided_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Expected: new_empty_strided is not comparable'
test_neg_view_new_empty_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_new_full_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_new_ones_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_new_zeros_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nextafter_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional__scaled_dot_product_attention_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_nn_functional_adaptive_avg_pool1d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_adaptive_avg_pool3d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_adaptive_max_pool1d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_adaptive_max_pool2d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_adaptive_max_pool3d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_avg_pool1d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_avg_pool2d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_avg_pool3d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_batch_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_bilinear_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_binary_cross_entropy_with_logits_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_binary_cross_entropy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_celu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_conv1d_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_conv2d_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py:1068: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Convolution.cpp:895.)
  return self.op(*args, **kwargs)
ok
test_neg_view_nn_functional_conv_transpose1d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_conv_transpose3d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_cosine_embedding_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_cosine_similarity_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_cross_entropy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_ctc_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_dropout2d_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
FAIL
test_neg_view_nn_functional_dropout3d_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:1384: UserWarning: An output with one or more elements was resized since it had shape [1, 5, 5, 5, 5], which does not match the required output shape [5, 5, 5, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Resize.cpp:17.)
  result = result.squeeze_(0) if inplace else result.squeeze(0)
FAIL
test_neg_view_nn_functional_dropout_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_nn_functional_elu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_embedding_bag_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_embedding_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_feature_alpha_dropout_with_train_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_nn_functional_feature_alpha_dropout_without_train_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_fractional_max_pool2d_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Operation not tested with tensors with negative bit.'
test_neg_view_nn_functional_fractional_max_pool3d_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Operation not tested with tensors with negative bit.'
test_neg_view_nn_functional_gaussian_nll_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_gelu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_glu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_grid_sample_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_nn_functional_group_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_hardshrink_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_hardsigmoid_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_hardswish_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_hardtanh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_hinge_embedding_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_huber_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_instance_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_interpolate_area_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_interpolate_bicubic_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_interpolate_linear_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_kl_div_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
ok
test_neg_view_nn_functional_l1_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_layer_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_leaky_relu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_linear_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_local_response_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_logsigmoid_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_margin_ranking_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_max_pool1d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_pool2d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_pool3d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_unpool1d_grad_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_unpool1d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_unpool2d_grad_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_unpool2d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_unpool3d_grad_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_max_unpool3d_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_mish_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_mse_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_multi_margin_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_multilabel_margin_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_multilabel_soft_margin_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_nll_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_normalize_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_pad_circular_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_pad_constant_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_pad_reflect_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_pad_replicate_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_nn_functional_pairwise_distance_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_pdist_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_nn_functional_pixel_shuffle_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_pixel_unshuffle_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_poisson_nll_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_prelu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_relu6_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_relu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_rrelu_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_nn_functional_selu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_silu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_smooth_l1_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_soft_margin_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_softmin_with_dtype_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_softmin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_softplus_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_softshrink_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_softsign_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_tanhshrink_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_threshold_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_triplet_margin_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_triplet_margin_with_distance_loss_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_unfold_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_nn_functional_upsample_bilinear_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:4070: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
ok
test_neg_view_nn_functional_upsample_nearest_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py:4014: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
ERROR
test_neg_view_nonzero_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_norm_fro_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_norm_inf_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_norm_nuc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_normal_number_mean_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_normal_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_ones_like_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_ones_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_ops_nvprims_native_batch_norm_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_ops_nvprims_var_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_ormqr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_outer_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_pca_lowrank_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_permute_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_pinverse_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_polar_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_polygamma_polygamma_n_0_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_polygamma_polygamma_n_1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_polygamma_polygamma_n_2_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_polygamma_polygamma_n_3_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_polygamma_polygamma_n_4_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_positive_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_pow_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_prod_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_put_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_qr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_quantile_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_rad2deg_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_rand_like_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_randint_like_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_randn_like_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_randn_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_ravel_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_real_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_reciprocal_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_remainder_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_renorm_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_repeat_interleave_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_repeat_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_reshape_as_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_reshape_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_resize__xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Operation not tested with tensors with negative bit.'
test_neg_view_resize_as__xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_resolve_conj_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_resolve_neg_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_roll_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "onlyNativeDeviceTypes: doesn't run on xpu"
test_neg_view_rot90_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_round_decimals_0_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_round_decimals_3_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_round_decimals_neg_3_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_round_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_rsqrt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_rsub_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_scatter_add_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_scatter_reduce_amax_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::scatter_reduce.two_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::scatter_reduce.two_out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_scatter_reduce_amin_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::scatter_reduce.two_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::scatter_reduce.two_out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_scatter_reduce_mean_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::scatter_reduce.two_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::scatter_reduce.two_out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_scatter_reduce_prod_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::scatter_reduce.two_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::scatter_reduce.two_out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_scatter_reduce_sum_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::scatter_reduce.two_out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::scatter_reduce.two_out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_scatter_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_searchsorted_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py:1068: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/include/ATen/native/BucketizationUtils.h:35.)
  return self.op(*args, **kwargs)
/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py:1068: UserWarning: torch.searchsorted(): boundary tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous boundary tensor if possible. This message will only appear once per program. (Triggered internally at /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/include/ATen/native/BucketizationUtils.h:41.)
  return self.op(*args, **kwargs)
ok
test_neg_view_segment_reduce_lengths_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::segment_reduce' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::segment_reduce' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_segment_reduce_offsets_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::segment_reduce' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::segment_reduce' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_select_scatter_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_select_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sgn_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::_efficientzerotensor' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_efficientzerotensor' is only available for these backends: [CPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nBackendSelect: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:726 [kernel]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_short_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sigmoid_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sign_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_signbit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sin_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sinc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sinh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_slice_scatter_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_slice_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_softmax_with_dtype_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_softmax_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sort_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sparse_sampled_addmm_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipped!'
test_neg_view_special_airy_ai_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_airy_ai.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_airy_ai.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_bessel_j0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_bessel_j0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_bessel_j0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_bessel_j1_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_bessel_j1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_bessel_j1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14484 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_bessel_y0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_bessel_y0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_bessel_y0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_bessel_y1_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_bessel_y1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_bessel_y1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_chebyshev_polynomial_t_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_t.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_t.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:15910 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_chebyshev_polynomial_u_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_chebyshev_polynomial_u.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_chebyshev_polynomial_u.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_chebyshev_polynomial_v_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_chebyshev_polynomial_w_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_entr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_erfcx_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_hermite_polynomial_h_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_h.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_h.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_hermite_polynomial_he_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_hermite_polynomial_he.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_hermite_polynomial_he.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_i0e_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_i1_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_i1e_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_laguerre_polynomial_l_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_laguerre_polynomial_l.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_laguerre_polynomial_l.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_3.cpp:22330 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_legendre_polynomial_p_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_log_ndtr_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_log_ndtr.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_log_ndtr.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_modified_bessel_i0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_modified_bessel_i0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_modified_bessel_i0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:19962 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14624 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_modified_bessel_i1_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_modified_bessel_i1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_modified_bessel_i1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_modified_bessel_k0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_modified_bessel_k0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_modified_bessel_k0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_2.cpp:20548 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4822 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_modified_bessel_k1_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_modified_bessel_k1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_modified_bessel_k1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_1.cpp:14538 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_1.cpp:15586 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_ndtr_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_ndtri_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_polygamma_special_polygamma_n_0_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_scaled_modified_bessel_k0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_scaled_modified_bessel_k0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_scaled_modified_bessel_k0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:14883 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_scaled_modified_bessel_k1_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_scaled_modified_bessel_k1.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_scaled_modified_bessel_k1.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_shifted_chebyshev_polynomial_t_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_shifted_chebyshev_polynomial_u_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_shifted_chebyshev_polynomial_v_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_shifted_chebyshev_polynomial_w_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Skipping - testing takes an unreasonably long time, #79528'
test_neg_view_special_spherical_bessel_j0_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::special_spherical_bessel_j0.out' with arguments from the 'XPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::special_spherical_bessel_j0.out' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30798 [kernel]\nMeta: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMeta.cpp:26815 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterFunctionalization_1.cpp:21945 [kernel]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5089 [kernel]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:16458 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_special_xlog1py_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_special_zeta_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_split_list_args_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_split_with_sizes_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_split_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sqrt_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_square_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_squeeze_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py:709: UserWarning: An output with one or more elements was resized since it had shape [5, 1, 5, 1], which does not match the required output shape [5, 5, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Resize.cpp:17.)
  inplace_forward = inplace_variant(cloned2, *sample.args, **sample.kwargs)
/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py:709: UserWarning: An output with one or more elements was resized since it had shape [5, 1, 5, 1], which does not match the required output shape [5, 1, 5]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/Resize.cpp:17.)
  inplace_forward = inplace_variant(cloned2, *sample.args, **sample.kwargs)
ok
test_neg_view_stack_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_std_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_std_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_stft_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'not ready on XPU'
test_neg_view_sub_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_sum_to_size_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_sum_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_svd_lowrank_xpu_float64 (__main__.TestMathBitsXPU) ... FAIL
test_neg_view_svd_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_symeig_xpu_float64 (__main__.TestMathBitsXPU) ... /home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py:1068: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.
The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.
L, _ = torch.symeig(A, upper=upper)
should be replaced with
L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')
and
L, V = torch.symeig(A, eigenvectors=True)
should be replaced with
L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at /home/gta/xunsongh/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2794.)
  return self.op(*args, **kwargs)
ok
test_neg_view_t_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_take_along_dim_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_take_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_tan_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_tanh_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_tensor_split_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_tensordot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_tile_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_to_sparse_xpu_float64 (__main__.TestMathBitsXPU) ... skipped "not implemented: Could not run 'aten::_to_dense' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_to_dense' is only available for these backends: [MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nMkldnnCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterMkldnnCPU.cpp:492 [kernel]\nSparseCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1261 [kernel]\nSparseCsrCPU: registered at /home/gta/xunsongh/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1030 [kernel]\nBackendSelect: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradCUDA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHIP: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXLA: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMPS: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradIPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradXPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradHPU: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradVE: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradLazy: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradMeta: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse1: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse2: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradPrivateUse3: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nAutogradNestedTensor: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:16899 [autograd kernel]\nTracer: registered at /home/gta/xunsongh/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:16890 [kernel]\nAutocastCPU: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastXPU: fallthrough registered at /home/gta/xunsongh/ipex-gpu/csrc/gpu/aten/amp/autocast_mode.cpp:233 [backend fallback]\nAutocastCUDA: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /home/gta/xunsongh/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /home/gta/xunsongh/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
test_neg_view_to_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_topk_xpu_float64 (__main__.TestMathBitsXPU) ... ERROR
test_neg_view_trace_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_transpose_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_trapezoid_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_trapz_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_triangular_solve_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_tril_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_triu_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_true_divide_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_trunc_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_unbind_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_unflatten_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_unfold_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_uniform_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure
test_neg_view_unique_consecutive_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_unique_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_unsqueeze_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_var_mean_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_var_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_vdot_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_view_as_complex_xpu_float64 (__main__.TestMathBitsXPU) ... skipped 'Operation not tested with tensors with negative bit.'
test_neg_view_view_as_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_view_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_vsplit_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_vstack_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_where_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_xlogy_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_zero__xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_zeros_like_xpu_float64 (__main__.TestMathBitsXPU) ... ok
test_neg_view_zeros_xpu_float64 (__main__.TestMathBitsXPU) ... expected failure

======================================================================
ERROR: test_neg_view__refs_nn_functional_layer_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_refs/nn/functional/__init__.py", line 178, in layer_norm
    return torch.native_layer_norm(input, normalized_shape, weight, bias, eps)[0]
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_baddbmm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: self dim 0 must match batch1 dim 0

======================================================================
ERROR: test_neg_view_cdist_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 717, in _test_math_view
    expected_forward.sum().backward(retain_graph=True)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function CdistBackward0 returned an invalid gradient at index 0 - got [2, 3, 2] but expected shape compatible with [2, 2, 3, 2]

======================================================================
ERROR: test_neg_view_chalf_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 14122, in <lambda>
    op=lambda x, *args, **kwargs: x.chalf(*args, **kwargs),
RuntimeError: "copy_" not implemented for 'ComplexHalf'

======================================================================
ERROR: test_neg_view_combinations_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 725, in _test_math_view
    expected_forward.backward(grad)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Native API failed. Native API returns: -50 (CL_INVALID_ARG_VALUE) -50 (CL_INVALID_ARG_VALUE)

======================================================================
ERROR: test_neg_view_cross_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: inconsistent tensors sizes input: [1, 3] other: [5, 3]

======================================================================
ERROR: test_neg_view_istft_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Level-Zero error:78000011

======================================================================
ERROR: test_neg_view_matrix_exp_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: bmm(): functions with out=... arguments don't support automatic differentiation, but one of the arguments requires grad.

======================================================================
ERROR: test_neg_view_native_batch_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: DPCPP batch_norm backend got unsupported type=Double

======================================================================
ERROR: test_neg_view_native_layer_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_adaptive_avg_pool1d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: adaptive_average_pool2d_dpcpp(): expected input to have non-empty spatial dimensions, but input has sizes [0, 8, 1, 8] with dimension 0 being empty

======================================================================
ERROR: test_neg_view_nn_functional_adaptive_avg_pool2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1214, in adaptive_avg_pool2d
    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_adaptive_avg_pool3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1231, in adaptive_avg_pool3d
    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)
RuntimeError: adaptive_avg_pool3d(): expected input to have non-empty spatial dimensions, but input has sizes [0, 8, 8, 8, 8] with dimension 0 being empty

======================================================================
ERROR: test_neg_view_nn_functional_adaptive_max_pool1d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1080, in adaptive_max_pool1d_with_indices
    return torch.adaptive_max_pool1d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_adaptive_max_pool2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1121, in adaptive_max_pool2d_with_indices
    return torch._C._nn.adaptive_max_pool2d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_adaptive_max_pool3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 1162, in adaptive_max_pool3d_with_indices
    return torch._C._nn.adaptive_max_pool3d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_avg_pool1d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_avg_pool2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: dpcpp_avg_pool2d operator does not support divisor

======================================================================
ERROR: test_neg_view_nn_functional_avg_pool3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_batch_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
RuntimeError: DPCPP batch_norm backend got unsupported type=Double

======================================================================
ERROR: test_neg_view_nn_functional_conv_transpose1d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_conv_transpose2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_conv_transpose3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_instance_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2495, in instance_norm
    return torch.instance_norm(
RuntimeError: DPCPP batch_norm backend got unsupported type=Double

======================================================================
ERROR: test_neg_view_nn_functional_interpolate_area_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3935, in interpolate
    return adaptive_avg_pool1d(input, output_size)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_interpolate_bilinear_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3950, in interpolate
    return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_interpolate_linear_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3945, in interpolate
    return torch._C._nn.upsample_linear1d(input, output_size, align_corners, scale_factors)
RuntimeError: We don't support align_corners currently as oneDNN don't support this algorithm!


======================================================================
ERROR: test_neg_view_nn_functional_interpolate_nearest_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3920, in interpolate
    return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_interpolate_trilinear_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3953, in interpolate
    return torch._C._nn.upsample_trilinear3d(input, output_size, align_corners, scale_factors)
RuntimeError: we don't support align_cornser path by currently as oneDNN don't support this algorithm!


======================================================================
ERROR: test_neg_view_nn_functional_layer_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_local_response_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2555, in local_response_norm
    div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_pool1d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 671, in max_pool1d_with_indices
    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_pool2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 757, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_pool3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 843, in max_pool3d_with_indices
    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_unpool1d_grad_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 700, in _test_math_view
    for sample in samples:
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 43, in generator_context
    response = gen.send(None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7646, in sample_inputs_max_unpool_grad
    for sample in sample_inputs_max_unpool(op_info, device, dtype, requires_grad, **kwargs):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7630, in sample_inputs_max_unpool
    pool, indices = pool_method(sample.input, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 671, in max_pool1d_with_indices
    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_unpool1d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 700, in _test_math_view
    for sample in samples:
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 43, in generator_context
    response = gen.send(None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7630, in sample_inputs_max_unpool
    pool, indices = pool_method(sample.input, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 671, in max_pool1d_with_indices
    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_unpool2d_grad_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 700, in _test_math_view
    for sample in samples:
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 43, in generator_context
    response = gen.send(None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7646, in sample_inputs_max_unpool_grad
    for sample in sample_inputs_max_unpool(op_info, device, dtype, requires_grad, **kwargs):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7630, in sample_inputs_max_unpool
    pool, indices = pool_method(sample.input, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 757, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_unpool2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 700, in _test_math_view
    for sample in samples:
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 43, in generator_context
    response = gen.send(None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7630, in sample_inputs_max_unpool
    pool, indices = pool_method(sample.input, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 757, in max_pool2d_with_indices
    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_unpool3d_grad_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 700, in _test_math_view
    for sample in samples:
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 43, in generator_context
    response = gen.send(None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7646, in sample_inputs_max_unpool_grad
    for sample in sample_inputs_max_unpool(op_info, device, dtype, requires_grad, **kwargs):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7630, in sample_inputs_max_unpool
    pool, indices = pool_method(sample.input, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 843, in max_pool3d_with_indices
    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_max_unpool3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 700, in _test_math_view
    for sample in samples:
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 43, in generator_context
    response = gen.send(None)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py", line 7630, in sample_inputs_max_unpool
    pool, indices = pool_method(sample.input, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_jit_internal.py", line 483, in fn
    return if_true(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 843, in max_pool3d_with_indices
    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_nn_functional_nll_loss_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 2701, in nll_loss
    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: 1D target tensor expected, multi-target not supported

======================================================================
ERROR: test_neg_view_nn_functional_pad_replicate_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: non-empty 3D or 4D (batch mode) tensor expected for input, but got: [ XPUDoubleType{0,3,3,3} ]

======================================================================
ERROR: test_neg_view_nn_functional_upsample_nearest_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 4015, in upsample_nearest
    return interpolate(input, size, scale_factor, mode="nearest")
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/nn/functional.py", line 3920, in interpolate
    return torch._C._nn.upsample_nearest1d(input, output_size, scale_factors)
RuntimeError: Double is not supported in oneDNN!

======================================================================
ERROR: test_neg_view_ops_nvprims_native_batch_norm_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_ops.py", line 442, in __call__
    return self._op(*args, **kwargs or {})
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_prims_common/wrappers.py", line 311, in _autograd_impl
    return redispatch_prim(args, kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_prims_common/wrappers.py", line 281, in redispatch_prim
    return prim(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_ops.py", line 257, in __call__
    return self._op(*args, **kwargs or {})
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_prims/nvfuser_prims.py", line 352, in _prim_impl
    return torch.native_batch_norm(
RuntimeError: DPCPP batch_norm backend got unsupported type=Double

======================================================================
ERROR: test_neg_view_scatter_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 717, in _test_math_view
    expected_forward.sum().backward(retain_graph=True)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.

======================================================================
ERROR: test_neg_view_topk_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 435, in instantiated_test
    raise rte
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 703, in _test_math_view
    expected_forward = op(sample.input, *sample.args, **sample.kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/opinfo/core.py", line 1068, in __call__
    return self.op(*args, **kwargs)
RuntimeError: expected excluded dim between -1 and dims - 1

======================================================================
FAIL: test_neg_view_bernoulli_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 2 / 3 (66.7%)
Greatest absolute difference: 1.0 at index (0,) (up to 0.001 allowed)
Greatest relative difference: inf at index (0,) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_linalg_pinv_singular_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1286, in wrapper
    fn(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 150 / 150 (100.0%)
Greatest absolute difference: nan at index (0, 0) (up to 0.001 allowed)
Greatest relative difference: nan at index (0, 0) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_mode_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 5 (20.0%)
Greatest absolute difference: 0.5451632117269707 at index (0,) (up to 0.001 allowed)
Greatest relative difference: 0.06244809732687583 at index (0,) (up to 1e-07 allowed)

The failure occurred for item [0]

======================================================================
FAIL: test_neg_view_multinomial_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 2 / 3 (66.7%)
Greatest absolute difference: 1 at index (1,) (up to 0.001 allowed)
Greatest relative difference: 1.0 at index (1,) (up to 0.0 allowed)

======================================================================
FAIL: test_neg_view_nn_functional__scaled_dot_product_attention_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 77 / 120 (64.2%)
Greatest absolute difference: 17.755407397141305 at index (1, 0, 5) (up to 0.001 allowed)
Greatest relative difference: inf at index (0, 0, 0) (up to 1e-07 allowed)

The failure occurred for item [0]

======================================================================
FAIL: test_neg_view_nn_functional_dropout2d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 90 / 125 (72.0%)
Greatest absolute difference: 17.927370156646017 at index (1, 3, 2) (up to 0.001 allowed)
Greatest relative difference: inf at index (0, 1, 0) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_nn_functional_dropout3d_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 250 / 625 (40.0%)
Greatest absolute difference: 17.95621682152988 at index (2, 3, 0, 0) (up to 0.001 allowed)
Greatest relative difference: 1.0 at index (1, 0, 0, 0) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_nn_functional_dropout_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 11 / 25 (44.0%)
Greatest absolute difference: 16.911042934339513 at index (4, 4) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 1) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_nn_functional_feature_alpha_dropout_with_train_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 325 / 625 (52.0%)
Greatest absolute difference: 9.52035325257723 at index (1, 0, 1, 1) (up to 0.001 allowed)
Greatest relative difference: 14.487527853627606 at index (3, 1, 1, 4) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_nn_functional_grid_sample_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 721, in _test_math_view
    self.assertEqual(tensor.grad, cloned1_tensor.grad)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 8 / 150 (5.3%)
Greatest absolute difference: 1.7897518097599083 at index (0, 0, 4, 4) (up to 0.001 allowed)
Greatest relative difference: 2.272219441067232 at index (0, 0, 3, 3) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_nn_functional_pdist_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 721, in _test_math_view
    self.assertEqual(tensor.grad, cloned1_tensor.grad)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 5 / 5 (100.0%)
Greatest absolute difference: 0.0020008369996091964 at index (0, 0) (up to 0.001 allowed)
Greatest relative difference: inf at index (1, 0) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_nn_functional_rrelu_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 7 / 20 (35.0%)
Greatest absolute difference: 5.733476652816312 at index (18,) (up to 0.001 allowed)
Greatest relative difference: 33.79474368124429 at index (10,) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_normal_number_mean_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 12 / 12 (100.0%)
Greatest absolute difference: 11.032117666997364 at index (0, 0) (up to 0.001 allowed)
Greatest relative difference: 15.596980772859013 at index (2, 2) (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_normal_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Absolute difference: 0.4204619616265579 (up to 0.001 allowed)
Relative difference: 0.09629226815476202 (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_pca_lowrank_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_cuda.py", line 159, in wrapped
    return f(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 5 / 10 (50.0%)
Greatest absolute difference: 1.6426615471232269 at index (1, 0) (up to 0.001 allowed)
Greatest relative difference: 2.0000000000000004 at index (3, 0) (up to 1e-07 allowed)

The failure occurred for item [0]

======================================================================
FAIL: test_neg_view_rand_like_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Absolute difference: 0.6277597881678192 (up to 0.001 allowed)
Relative difference: 0.36706695056688987 (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_randint_like_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Absolute difference: 2.0 (up to 0.001 allowed)
Relative difference: 0.6666666666666666 (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_randn_like_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Scalars are not close!

Absolute difference: 1.5081643367088517 (up to 0.001 allowed)
Relative difference: 3.61491652383039 (up to 1e-07 allowed)

======================================================================
FAIL: test_neg_view_svd_lowrank_xpu_float64 (__main__.TestMathBitsXPU)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2004, in wrapper
    method(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/common/pytorch_test_base.py", line 430, in instantiated_test
    result = test(self, **param_kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_cuda.py", line 159, in wrapped
    return f(*args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 853, in dep_fn
    return fn(slf, *args, **kwargs)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 815, in test_wrapper
    return test(*args, **kwargs)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 748, in test_neg_view
    self._test_math_view(device, dtype, op, samples, math_op_physical, math_op_view, is_bit_set, lambda x: True)
  File "/home/gta/xunsongh/ipex-gpu/tests/gpu/experimental/test/test_ops.py", line 705, in _test_math_view
    self.assertEqual(expected_forward, forward_with_mathview)
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2468, in assertEqual
    assert_equal(
  File "/home/gta/miniconda3/envs/xunsongh/lib/python3.9/site-packages/torch/testing/_comparison.py", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 10 (10.0%)
Greatest absolute difference: 0.0017121942946338176 at index (4, 1) (up to 0.001 allowed)
Greatest relative difference: 0.01047605626970406 at index (4, 1) (up to 1e-07 allowed)

The failure occurred for item [0]

----------------------------------------------------------------------
Ran 1853 tests in 119.090s

FAILED (failures=19, errors=46, skipped=1131, expected failures=17)
Raised CalledProcessError: return code 1.