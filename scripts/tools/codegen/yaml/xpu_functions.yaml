backend: XPU
cpp_namespace: at
class_name: AtenIpexTypeXPU
use_out_as_primary: true
device_guard: true
supported:
  - _adaptive_avg_pool2d
  - _adaptive_avg_pool2d_backward
  - _aminmax
  - _aminmax.dim
  - _cat
  - _cat.out
  - _cdist_backward
  - _cdist_forward
  - _ctc_loss
  - _ctc_loss_backward
#  - _cumprod
  - cumprod.out
#  - _cumsum	# diabled in torch
#  - _cumsum.out
  - cumsum.out  # newly added
  - _embedding_bag
  - _embedding_bag_dense_backward
  - _empty_affine_quantized
  - _empty_per_channel_affine_quantized
#  - _fft_with_size.norm_modes  # different schema
  - _fft_c2r
  - _fft_r2c
  - _fft_c2c
  - _fft_c2r.out
  - _fft_r2c.out
  - _fft_c2c.out
  - _fused_dropout
  - _masked_scale
  - _index_put_impl_ 
  - _inverse_helper
  - _linalg_qr_helper
  - _local_scalar_dense
  - _log_softmax
  - _log_softmax_backward_data
#  - _lu_solve_helper  # different schema
  - _lu_with_info
  - _make_per_channel_quantized_tensor
  - _make_per_tensor_quantized_tensor
  - _pdist_backward
  - _pdist_forward
  - _reshape_alias  # newly added
  - _s_where
  - _softmax
  - _softmax_backward_data
  - _solve_helper
#  - _std  # different schema
  - _svd_helper  # different schema
  - _symeig_helper
#  - _triangular_solve_helper  # different schema
  - _unique
  - _unique2
#  - _var  # different schema
  - abs.out
  - acos.out
  - adaptive_avg_pool2d.out
#  - adaptive_avg_pool3d        # should not register
  - _adaptive_avg_pool3d
  - adaptive_avg_pool3d.out
  - _adaptive_avg_pool3d_backward
  - adaptive_avg_pool3d_backward.grad_input
  - adaptive_max_pool2d
  - adaptive_max_pool2d.out
  - adaptive_max_pool2d_backward
  - adaptive_max_pool2d_backward.grad_input
  - adaptive_max_pool3d
  - adaptive_max_pool3d.out
  - adaptive_max_pool3d_backward
  - adaptive_max_pool3d_backward.grad_input
  - add.Scalar
  - add.Tensor
  - add.out
  - add_.Scalar
  - add_.Tensor
  - addbmm
  - addbmm_
  - addbmm.out
#  - addcdiv	# different signature
  - addcdiv.out
#  - addcdiv_	# different signature
#  - addcmul	# different signature
  - addcmul.out
#  - addcmul_	# different signature
#  - addmm	# different signature
  - addmm.out  # newly added
#  - addmm_	# different signature
  - addmv.out  # newly added
#  - addr	# different signature
#  - addr_	# different signature
  - all
  - all.dim
  - all.out
  - amax
  - amax.out
  - any
  - any.dim
  - any.out
  - arange.start_out
  - argmax
  - argmin
  - as_strided
  - asin.out
  - atan.out
  - atan2
  - atan2.out
  - atan2_
#  - atan_
  - avg_pool2d
  - avg_pool2d.out
  - avg_pool2d_backward
  - avg_pool2d_backward.grad_input
  - avg_pool3d
  - avg_pool3d.out
  - avg_pool3d_backward
  - avg_pool3d_backward.grad_input
  - baddbmm
  - baddbmm.out
  - baddbmm_
  - bernoulli_.Tensor
  - bernoulli_.float
  - binary_cross_entropy
  - binary_cross_entropy.out
  - binary_cross_entropy_backward
  - binary_cross_entropy_backward.grad_input
  - bincount
#  - bitwise_and.Scalar_out  # different signature
  - bitwise_and.Tensor_out
  - bitwise_not
  - bitwise_not.out
  - bitwise_not_
#  - bitwise_or.Scalar_out  # different signature
  - bitwise_or.Tensor_out
#  - bitwise_xor.Scalar_out  # different signature
  - bitwise_xor.Tensor_out
  - bmm
  - bmm.out
  - cauchy_
  - ceil.out
#  - chain_matmul       # should not register
  - cholesky
  - cholesky.out
  - cholesky_inverse
  - cholesky_inverse.out
  - cholesky_solve
  - cholesky_solve.out
#  - clamp  # different signature
  - clamp.out
#  - clamp_  # different signature
  - clamp_max.out
#  - clamp_max_  # different signature
  - clamp_min.out
#  - clamp_min_  # different signature
  - clone
  - col2im
  - col2im.out
  - col2im_backward
  - col2im_backward.grad_input
#  - conj.out  # different schema
  - conv_tbc
#  - conv_tbc_backward  # should not register
  - convolution_backward_overrideable
  - convolution_overrideable
  - copy_
  - cos.out
#  - cos_
  - cosh.out
  - cross
  - cross.out
  - dequantize.self
  - diag
  - diag.out
  - digamma
  - digamma.out
  - digamma_
  - div.Tensor
  - div.out
  - div_.Tensor
  - div.out_mode
  - dot
  - elu
  - elu.out
  - elu_
  - elu_backward
  - elu_backward.grad_input
  - embedding_dense_backward
  - empty.memory_format
  - empty_strided
  - eq.Scalar
  - eq.Scalar_out
  - eq.Tensor
  - eq.Tensor_out
  - equal
  - erf.out
#  - erf_
  - erfc.out
#  - erfc_
  - erfinv
  - erfinv.out
  - erfinv_
  - exp.out
#  - exp_
  - expm1.out
  - exponential_
  - eye.m_out
  - eye.out
  - fill_.Scalar
  - fill_.Tensor
  - flip
  - floor.out
  - floor_divide
  - floor_divide.out
  - floor_divide_.Tensor
  - fmod.Scalar
  - fmod.Scalar_out
  - fmod.Tensor
  - fmod.Tensor_out
  - fmod_.Scalar
  - fmod_.Tensor
  - frac.out
  - fractional_max_pool2d
  - fractional_max_pool2d.output
  - fractional_max_pool2d_backward
  - fractional_max_pool2d_backward.grad_input
  - fractional_max_pool3d
  - fractional_max_pool3d.output
  - fractional_max_pool3d_backward
  - fractional_max_pool3d_backward.grad_input
  - gather
  - gather.out
  - ge.Scalar
  - ge.Scalar_out
  - ge.Tensor
  - ge.Tensor_out
  - gelu
  - gelu_backward
#  - geometric_  # different signature
  - geqrf
  - geqrf.a
#  - ger        # should not register
#  - ger.out  # dispatch is False
  - glu
  - glu.out
  - glu_backward
  - glu_backward.grad_input
  - gt.Scalar
  - gt.Scalar_out
  - gt.Tensor
  - gt.Tensor_out
  - grid_sampler_2d
  - grid_sampler_3d
  - grid_sampler_2d_backward
  - grid_sampler_3d_backward
  - hardshrink
  - hardshrink.out
  - hardshrink_backward
  - hardtanh
  - hardtanh.out
  - hardtanh_
  - hardtanh_backward
  - hardtanh_backward.grad_input
  - histc
  - histc.out
  - im2col
  - im2col.out
  - im2col_backward
  - im2col_backward.grad_input
  - index.Tensor
#  - index_add_         # should not register
  - index_copy_
  - index_fill_.int_Scalar
  - index_fill_.int_Tensor
  - index_select
  - index_select.out
  - index_add_.alpha
  - inverse
  - inverse.out
  - is_set_to
  - isnan
  - kthvalue.values
  - l1_loss
  - l1_loss.out
  - l1_loss_backward
  - l1_loss_backward.grad_input
  - le.Scalar
  - le.Scalar_out
  - le.Tensor
  - le.Tensor_out
  - leaky_relu
  - leaky_relu.out
  - leaky_relu_
  - leaky_relu_backward
  - lerp.Scalar
  - lerp.Scalar_out
  - lerp.Tensor
  - lerp.Tensor_out
  - lerp_.Scalar
  - lerp_.Tensor
  - lgamma
  - lgamma_
  - linspace.out
  - log.out
  - log10.out
  - log1p.out
  - log1p_
  - log2.out
  - log_normal_
  - log_sigmoid_backward
  - log_sigmoid_backward.grad_input
  - log_sigmoid_forward
  - log_sigmoid_forward.output
  - logical_and.out
#  - logical_not        # should not register
  - logical_not.out
#  - logical_not_       # should not register
  - logical_or.out
  - logical_xor.out
  - logspace.out
  - lt.Scalar
  - lt.Scalar_out
  - lt.Tensor
  - lt.Tensor_out
  - lu_solve
  - lu_solve.out
  - masked_fill_.Scalar
  - masked_fill_.Tensor
  - masked_scatter_
  - masked_select
  - masked_select.out
  - max
  - max.dim
  - max.dim_max
#  - max.out  # dispatch is False
  - max_pool2d_with_indices
  - max_pool2d_with_indices.out
  - max_pool2d_with_indices_backward
  - max_pool2d_with_indices_backward.grad_input
  - max_pool3d_with_indices
  - max_pool3d_with_indices.out
  - max_pool3d_with_indices_backward
  - max_pool3d_with_indices_backward.grad_input
  - max_unpool2d
  - max_unpool2d.out
  - max_unpool2d_backward
  - max_unpool2d_backward.grad_input
  - max_unpool3d
  - max_unpool3d.out
  - max_unpool3d_backward
  - max_unpool3d_backward.grad_input
  - maximum
  - maximum.out
  - mean
  - mean.dim
  - mean.out
  - median
  - min
  - min.dim
  - min.dim_min
#  - min.out  # dispatch is False
  - minimum
  - minimum.out
  - mm
  - mm.out
  - mode
  - mode.values
  - mse_loss
  - mse_loss.out
  - mse_loss_backward
  - mse_loss_backward.grad_input
#  - mul.Scalar  # different signature
  - mul.Tensor
  - mul.out
#  - mul_.Scalar  # different signature
  - mul_.Tensor
  - multi_margin_loss
  - multi_margin_loss.out
  - multi_margin_loss_backward
  - multi_margin_loss_backward.grad_input
  - multilabel_margin_loss_backward
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_forward
  - multilabel_margin_loss_forward.output
  - multinomial
#  - multinomial.out  # different signature
  - mv
  - mvlgamma
  - mvlgamma_
  - narrow_copy
  - native_batch_norm
  - native_batch_norm_backward
  - native_group_norm
  - native_group_norm_backward
  - native_layer_norm
  - native_layer_norm_backward
  - ne.Scalar
  - ne.Scalar_out
  - ne.Tensor
  - ne.Tensor_out
  - neg.out
  - nll_loss2d_backward
  - nll_loss2d_backward.grad_input
  - nll_loss2d_forward
  - nll_loss2d_forward.output
  - nll_loss_backward
  - nll_loss_backward.grad_input
  - nll_loss_forward
  - nll_loss_forward.output
  - nonzero
  - nonzero.out
#  - norm.Scalar  # different signature
#  - norm.ScalarOpt_dim  # different signature
#  - norm.ScalarOpt_dim_dtype  # different signature
#  - norm.ScalarOpt_dtype  # different signature
#  - norm.dtype_out  # different signature
  - norm.out
  - normal.Tensor_Tensor
  - normal.Tensor_Tensor_out
  - normal.Tensor_float
  - normal.Tensor_float_out
  - normal.float_Tensor
  - normal.float_Tensor_out
  - normal_
#  - orgqr      # should not register
#  - orgqr.out  # dispatch is False
  - ormqr
  - ormqr.out
  - polygamma
  - polygamma.out
  - polygamma_
#  - pow.Scalar  # different signature
#  - pow.Scalar_out  # different signature
#  - pow.Tensor_Scalar  # different signature
  - pow.Tensor_Scalar_out
  - pow.Tensor_Tensor
  - pow.Tensor_Tensor_out
#  - pow_.Scalar  # different signature
  - pow_.Tensor
  - prelu
  - prelu_backward
  - prod
  - prod.dim_int
  - prod.int_out
  - put_
  - quantize_per_channel
  - quantize_per_tensor
  - random_
  - random_.from
  - random_.to
#  - randperm  # different schema
#  - randperm.generator  # different schema
  - randperm.generator_out
#  - randperm.out  # dispatch is False
  - range.out
  - reciprocal.out
  - record_stream
  - reflection_pad1d
  - reflection_pad1d.out
  - reflection_pad1d_backward
  - reflection_pad1d_backward.grad_input
  - reflection_pad2d
  - reflection_pad2d.out
  - reflection_pad2d_backward
  - reflection_pad2d_backward.grad_input
  - relu
  - relu_
  - remainder.Scalar
  - remainder.Scalar_out
  - remainder.Tensor
  - remainder.Tensor_out
  - remainder_.Scalar
  - remainder_.Tensor
#  - renorm  # different signature
  - renorm.out
#  - renorm_  # different signature
  - repeat_interleave.Tensor
  - replication_pad2d
  - replication_pad2d.out
  - replication_pad2d_backward
  - replication_pad2d_backward.grad_input
  - resize_
  - resize_as_
  - roll
  - rot90
  - round.out
#  - rrelu  # dispatch is False
#  - rrelu_  # dispatch is False
  - rrelu_with_noise
  - rrelu_with_noise.out
  - rrelu_with_noise_
  - rrelu_with_noise_backward
  - rsqrt.out
#  - rsub.Scalar  # different signature
#  - rsub.Tensor  # different signature
  - scatter.src
  - scatter_.src
#  - scatter_.value
#  - scatter_add_
  - scatter.value_out
  - scatter_add.out  # newly added
  - set_
  - set_.source_Storage
  - set_.source_Storage_storage_offset
  - set_.source_Tensor
  - sgn.out
#  - sigmoid
  - sigmoid.out
#  - sigmoid_
#  - sigmoid_backward
  - sigmoid_backward.grad_input
  - sign
  - sign.out
  - sign_
  - sin.out
  - sinh.out
#  - slice.Tensor  # different schema
  - smooth_l1_loss
  - smooth_l1_loss.out
  - smooth_l1_loss_backward
  - smooth_l1_loss_backward.grad_input
  - soft_margin_loss
  - soft_margin_loss.out
  - soft_margin_loss_backward
  - soft_margin_loss_backward.grad_input
  - softplus
  - softplus.out
  - softplus_backward
  - softplus_backward.grad_input
#  - softshrink  # different signature
  - softshrink.out
#  - softshrink_backward  # different signature
  - softshrink_backward.grad_input
  - solve
  - solve.solution
  - sort
  - sort.values
  - sqrt.out
#  - std        # should not register
  - std.correction
#  - std.dim    # should not register
#  - std.out  # dispatch is False
#  - std_mean   # should not register
  - std_mean.correction
#  - std_mean.dim       # should not register
#  - sub.Scalar  # different signature
#  - sub.Tensor  # different signature
  - sub.out
#  - sub_.Scalar  # different signature
#  - sub_.Tensor  # different signature
  - sum
  - sum.IntList_out
  - sum.dim_IntList
  - nansum
  - nansum.IntList_out
  - nansum.dim_IntList
#  - svd        # should not register
#  - svd.U  # dispatch is False
  - take
  - take.out
  - tan.out
#  - tan_
#  - tanh
  - tanh.out
#  - tanh_
  - tanh_backward
  - tanh_backward.grad_input
  - threshold
  - threshold.out
  - threshold_
  - threshold_backward
  - threshold_backward.grad_input
#  - topk    # we do not implement topk, and torch.topk will call aten::topk.values
  - topk.values
  - trace
  - triangular_solve
  - triangular_solve.X
  - tril.out
  - tril_
  - tril_indices
  - triu.out
  - triu_
  - triu_indices
  - trunc.out
  - unfold
  - uniform_
  - unique_consecutive
  - unique_dim
  - unique_dim_consecutive
  - upsample_bicubic2d
  - upsample_bicubic2d.out
  - upsample_bicubic2d.vec
  - upsample_bicubic2d_backward
  - upsample_bicubic2d_backward.grad_input
  - upsample_bicubic2d_backward.vec
  - upsample_bilinear2d
  - upsample_bilinear2d.out
  - upsample_bilinear2d.vec
  - upsample_bilinear2d_backward
  - upsample_bilinear2d_backward.grad_input
  - upsample_bilinear2d_backward.vec
  - upsample_linear1d
  - upsample_linear1d.out
  - upsample_linear1d.vec
  - upsample_linear1d_backward
  - upsample_linear1d_backward.grad_input
  - upsample_linear1d_backward.vec
  - upsample_nearest1d
  - upsample_nearest1d.out
  - upsample_nearest1d.vec
  - upsample_nearest1d_backward
  - upsample_nearest1d_backward.grad_input
  - upsample_nearest1d_backward.vec
  - upsample_nearest2d
  - upsample_nearest2d.out
  - upsample_nearest2d.vec
  - upsample_nearest2d_backward
  - upsample_nearest2d_backward.grad_input
  - upsample_nearest2d_backward.vec
  - upsample_nearest3d
  - upsample_nearest3d.out
  - upsample_nearest3d.vec
  - upsample_nearest3d_backward
  - upsample_nearest3d_backward.grad_input
  - upsample_nearest3d_backward.vec
  - upsample_trilinear3d
  - upsample_trilinear3d.out
  - upsample_trilinear3d.vec
  - upsample_trilinear3d_backward
  - upsample_trilinear3d_backward.grad_input
  - upsample_trilinear3d_backward.vec
#  - var        # should not register
  - var.correction
#  - var.dim    # should not register
#  - var_mean   # should not register
#  - var_mean.dim       # should not register
  - var_mean.correction
  - view
  - view_as_real   # newly added
  - view_as_complex
  - zero_
  - channel_shuffle
  - linalg_slogdet
  - linalg_slogdet.out
  - _det_lu_based_helper
  - conj_physical.out
  - conj_physical_
  - angle
  - angle.out
  - linalg_eig
  - linalg_eig.out
  - _det_lu_based_helper_backward_helper
  - linalg_solve
  - linalg_solve.out
  - complex.out
autograd:
  - cdist
  - lstm.input
  - gru.input
