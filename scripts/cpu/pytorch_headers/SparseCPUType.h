#pragma once

// @generated by aten/src/ATen/gen.py from TypeDerived.h

#include <c10/core/TensorOptions.h>
#include <c10/core/Scalar.h>
#include <c10/core/QScheme.h>
#include <c10/core/MemoryFormat.h>
#include <c10/util/ArrayRef.h>
#include <c10/util/intrusive_ptr.h>
#include <torch/csrc/WindowsTorchApiMacro.h>
#include <ATen/Dimname.h>



namespace c10 {
struct Storage;
}

namespace at {

class Tensor;
using TensorList = ArrayRef<Tensor>;

class Context;
struct Generator;

struct Quantizer;
// This is temporary typedef to enable Quantizer in aten native function API
// we'll remove them when we are actually exposing Quantizer class
// to frontend
using ConstQuantizerPtr = const c10::intrusive_ptr<Quantizer>&;

namespace SparseCPUType {
  Tensor empty(IntArrayRef size, optional<DimnameList> names, optional<ScalarType> dtype, optional<Layout> layout, optional<Device> device, optional<bool> pin_memory, optional<MemoryFormat> memory_format);
  Tensor empty(IntArrayRef size, optional<ScalarType> dtype, optional<Layout> layout, optional<Device> device, optional<bool> pin_memory, optional<MemoryFormat> memory_format);
  Tensor add(const Tensor & self, const Tensor & other, Scalar alpha);
  Tensor & add_(Tensor & self, const Tensor & other, const Scalar & alpha);
  Tensor & add_out(const Tensor & self, const Tensor & other, const Scalar & alpha, Tensor & out);
  Tensor div(const Tensor & self, const Tensor & other);
  Tensor & div_(Tensor & self, const Tensor & other);
  Tensor & div_out(const Tensor & self, const Tensor & other, Tensor & out);
  Tensor floor_divide(const Tensor & self, const Tensor & other);
  Tensor & floor_divide_(Tensor & self, const Tensor & other);
  Tensor & floor_divide_out(const Tensor & self, const Tensor & other, Tensor & out);
  Tensor isnan(const Tensor & self);
  Tensor & log1p_(Tensor & self);
  Tensor & log1p_out(const Tensor & self, Tensor & out);
  Tensor mm(const Tensor & self, const Tensor & mat2);
  Tensor & mm_out(const Tensor & self, const Tensor & mat2, Tensor & out);
  Tensor mul(const Tensor & self, const Tensor & other);
  Tensor & mul_(Tensor & self, const Tensor & other);
  Tensor & mul_out(const Tensor & self, const Tensor & other, Tensor & out);
  Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length);
  Tensor & narrow_copy_out(const Tensor & self, int64_t dim, int64_t start, int64_t length, Tensor & out);
  Tensor & sspaddmm_out(const Tensor & self, const Tensor & mat1, const Tensor & mat2, const Scalar & beta, const Scalar & alpha, Tensor & out);
  Tensor true_divide(const Tensor & self, const Tensor & other);
  Tensor & true_divide_(Tensor & self, const Tensor & other);
  Tensor & true_divide_out(const Tensor & self, const Tensor & other, Tensor & out);
  Tensor native_norm(const Tensor & self, const Scalar & p);
  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim);
  Tensor clone(const Tensor & self, optional<MemoryFormat> memory_format);
  Tensor & pow_out(const Tensor & self, const Tensor & exponent, Tensor & out);
  Tensor pow(const Tensor & self, const Scalar & exponent);
  Tensor & zero_(Tensor & self);
  Tensor & sub_out(const Tensor & self, const Tensor & other, const Scalar & alpha, Tensor & out);
  Tensor sub(const Tensor & self, const Tensor & other, const Scalar & alpha);
  Tensor & sub_(Tensor & self, const Tensor & other, const Scalar & alpha);
  Tensor & addmm_out(const Tensor & self, const Tensor & mat1, const Tensor & mat2, const Scalar & beta, const Scalar & alpha, Tensor & out);
  Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, const Scalar & beta, const Scalar & alpha);
  Tensor & addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, const Scalar & beta, const Scalar & alpha);
  Tensor _sparse_coo_tensor_unsafe(const Tensor & indices, const Tensor & values, IntArrayRef size, optional<ScalarType> dtype, optional<Layout> layout, optional<Device> device, optional<bool> pin_memory);
  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, optional<ScalarType> dtype, optional<Layout> layout, optional<Device> device, optional<bool> pin_memory);
  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, optional<ScalarType> dtype, optional<Layout> layout, optional<Device> device, optional<bool> pin_memory);
  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim);
  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim);
  Tensor sparse_mask(const Tensor & self, const Tensor & mask);
  Tensor to_dense(const Tensor & self, optional<ScalarType> dtype);
  int64_t sparse_dim(const Tensor & self);
  int64_t _dimI(const Tensor & self);
  int64_t dense_dim(const Tensor & self);
  int64_t _dimV(const Tensor & self);
  int64_t _nnz(const Tensor & self);
  Tensor coalesce(const Tensor & self);
  Tensor _coalesce(const Tensor & self);
  bool is_coalesced(const Tensor & self);
  Tensor _indices(const Tensor & self);
  Tensor _values(const Tensor & self);
  Tensor & _coalesced_(Tensor & self, bool coalesced);
  Tensor indices(const Tensor & self);
  Tensor values(const Tensor & self);
  Tensor & hspmm_out(const Tensor & mat1, const Tensor & mat2, Tensor & out);
  Tensor hspmm(const Tensor & mat1, const Tensor & mat2);
  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking);
  Tensor index_select(const Tensor & self, int64_t dim, const Tensor & index);
  Tensor any(const Tensor & self);
  Tensor & copy_(Tensor & self, const Tensor & src, bool non_blocking);
}

} // namespace at
