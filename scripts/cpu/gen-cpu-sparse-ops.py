from __future__ import print_function

from enum import Enum
import argparse
import collections
import lark
import os
import re
import string
import sys


def namedtuple_with_defaults(typename, field_names, default_values=()):
    ntuple = collections.namedtuple(typename, field_names)
    ntuple.__new__.__defaults__ = (None,) * len(ntuple._fields)
    if isinstance(default_values, collections.Mapping):
        prototype = ntuple(**default_values)
    else:
        prototype = ntuple(*default_values)
    ntuple.__new__.__defaults__ = tuple(prototype)
    return ntuple


FuncDecl = namedtuple_with_defaults('FuncDecl', 'cpp_sig, aten_sig')

FuncGen = namedtuple_with_defaults(
    'FuncGen',
    'tree, xtree, rwxtree, func, xfunc, code, sig, rwsig, cppsig, funsig, mapsig, aten_sig'
)

_GRAMMAR = r"""
    start: type fnname "(" params ")"
    type: CONST? core_type refspec?
    fnname: CNAME
    refspec: REF
           | PTR
    core_type: template
        | TNAME
    template: TNAME "<" typelist ">"
    typelist: type
            | type "," typelist
    REF: "&"
    PTR: "*"
    CONST: "const"
    TNAME: /[a-zA-Z0-9_:]+/
    HEXNUMBER: /0x[0-9a-fA-F]+/
    params: param
          | param "," params
    param: type param_name param_defval?
    param_name: CNAME

    param_defval: "=" init_value
    init_value: "true"
              | "false"
              | "{}"
              | NUMBER
              | SIGNED_NUMBER
              | HEXNUMBER
              | ESCAPED_STRING

    %import common.CNAME -> CNAME
    %import common.NUMBER -> NUMBER
    %import common.SIGNED_NUMBER -> SIGNED_NUMBER
    %import common.ESCAPED_STRING -> ESCAPED_STRING
    %import common.WS
    %ignore WS
    """

_PARSER = lark.Lark(_GRAMMAR, parser='lalr', propagate_positions=True)

_XPARSER = lark.Lark(
    _GRAMMAR, parser='lalr', propagate_positions=True, keep_all_tokens=True)

_FN_BLACKLIST = set([])

_FN_BLACKLIST_REGEX = [
    # ATEN CUDA functions
    r'[^(]*cudnn',
    r'[^(]*cufft',
    r'[^(]*mkldnn',
]

_FALLBACK_TO_CPU_TENSOR_LIST = 'fallbackToCPUTensorList'
_FALLBACK_TO_CPU_TENSOR = 'fallbackToCPUTensor'
_UPGRADE_TO_DPCPP_TENSOR = 'upgradeToDPCPPTensor'
_UPGRADE_TO_DPCPP_TENSOR_VEC = 'upgradeToDPCPPTensorVec'
_SHALLOW_FALLBACK_TO_CPU_TENSOR_LIST = 'shallowFallbackToCPUTensorList'
_SHALLOW_FALLBACK_TO_CPU_TENSOR = 'shallowFallbackToCPUTensor'
_SHALLOW_UPGRADE_TO_DPCPP_TENSOR = 'shallowUpgradeToDPCPPTensor'
_SHALLOW_UPGRADE_TO_DPCPP_TENSOR_VEC = 'shallowUpgradeToDPCPPTensorVec'
_SHALLOW_UPGRADE_TO_DPCPP_TENSOR_A = 'shallowUpgradeToDPCPPTensorA'
_SHALLOW_UPGRADE_TO_DPCPP_TENSOR_AW = 'shallowUpgradeToDPCPPTensorAW'


_TYPE_NSMAP = {
    'Tensor': 'at::Tensor',
    'TensorList': 'at::TensorList',
    'Scalar': 'at::Scalar',
    'Storage': 'at::Storage',
    'IntList': 'at::IntList',
    'IntArrayRef': 'at::IntArrayRef',
    'Generator': 'at::Generator',
    'ScalarType': 'at::ScalarType',
    'TensorOptions': 'at::TensorOptions',
    'SparseTensorRef': 'at::SparseTensorRef',
    'Device': 'c10::Device',
    'optional': 'c10::optional',
    'MemoryFormat': 'at::MemoryFormat',
    'QScheme': 'at::QScheme',
    'ConstQuantizerPtr': 'at::ConstQuantizerPtr',
    'Dimname': 'at::Dimname',  # namedtensor-only
    'DimnameList': 'at::DimnameList',  # namedtensor-only
}


_SPARSE_H_HEADER = """// Autogenerated file by {gen}. Do not edit directly!
#pragma once

#include <ATen/Tensor.h>

namespace torch_ipex {{
namespace cpu {{

class AtenIpexCPUSparse {{
 public:
{hfuncs}
}};

}}  // namespace cpu
}}  // namespace torch_ipex
"""


_SPARSE_CPP_HEADER = """// Autogenerated file by {gen}. Do not edit directly!
#include "SparseOPs.h"
#include "aten_ipex_bridge.h"
#include "ipex_sparse_tensor_impl.h"

namespace torch_ipex {{
namespace cpu {{

{funcs}

}}  // namespace cpu
}}  // namespace torch_ipex
"""

_FUNCTION_OPTIONS = {}

_RESULT_NAME = '_cpu_result'


class Context(object):

    def __init__(self, functions):
        with open(functions, 'r') as ff:
            self.functions_data = ff.read()

    def get_function(self, name):
        if self.functions_data.find(' {}('.format(name)) >= 0:
            return 'at::{}'.format(name)

    def contain_sig(self, name):
        if self.functions_data.find('{};'.format(name)) >= 0:
            return True
        return False


class StringEmit(object):

    def __init__(self, sref):
        self.sref = sref
        self.sval = ''
        self.pos = -1

    def __repr__(self):
        return self.sval

    def advance(self, t):
        start = t.column - 1
        end = t.end_column - 1
        pos = self.pos if self.pos >= 0 else start
        if start > pos:
            self.sval += self.sref[pos:start]
        self.sval += t.value
        self.pos = end

    def skip(self, t):
        self.pos = last_match(t) if self.pos >= 0 else -1

    def append(self, s):
        self.sval += s
        self.pos = -1


class TensorFetcher(object):

    def __init__(self, var_name):
        self.var_name = var_name
        self.tvar_name = '{}_tensors'.format(self.var_name)
        self.tensors = []
        self.writeable_tensors = []

    def add(self, name, writeable):
        new_tensor_name = '_cpu_{}'.format(name)
        if writeable:
            self.writeable_tensors.append((name, new_tensor_name))
        self.tensors.append(name)
        return new_tensor_name

    def generate_fetches(self):
        ipex_code = ''
        for tensor in self.tensors:
            ipex_code += '  auto&& _cpu_{} = bridge::{}({});\n'.format(tensor, _SHALLOW_FALLBACK_TO_CPU_TENSOR, tensor)
        return ipex_code

    def generate_updates(self):
        ipex_code = ''
        if len(self.writeable_tensors) > 0:
            for (w_tensor_name, w_new_tensor_name) in self.writeable_tensors:
                ipex_code += '  bridge::{}({}, {});\n'.format(_SHALLOW_UPGRADE_TO_DPCPP_TENSOR_AW,
                                                              w_tensor_name,
                                                              w_new_tensor_name)
        return ipex_code


def list_get(l, n):
    return l[n] if n < len(l) else None


def is_blacklisted_fn(fname, mapsig):
    if fname in _FN_BLACKLIST or mapsig in _FN_BLACKLIST:
        return True
    for frx in _FN_BLACKLIST_REGEX:
        if re.match(frx, fname) or re.match(frx, mapsig):
            return True
    return False


def is_write_param(fnopts, pname, defval):
    if fnopts and fnopts.wparams:
        if pname in fnopts.wparams:
            return True
    return defval


def first_match(t):
    if isinstance(t, lark.lexer.Token):
        return t.column - 1
    assert isinstance(t, lark.tree.Tree)
    return first_match(t.children[0])


def last_match(t):
    if isinstance(t, lark.lexer.Token):
        return t.end_column - 1
    assert isinstance(t, lark.tree.Tree)
    return last_match(t.children[-1])


def for_every_token(t, fn):
    if isinstance(t, lark.lexer.Token):
        fn(t)
    else:
        assert isinstance(t, lark.tree.Tree)
        for c in t.children:
            for_every_token(c, fn)


def emit_string(t, emit, emit_fn):
    status = emit_fn(t)
    if status > 0:

        def do_emit(tok):
            emit.advance(tok)

        for_every_token(t, do_emit)
    elif status == 0:
        if isinstance(t, lark.lexer.Token):
            emit.advance(t)
        else:
            assert isinstance(t, lark.tree.Tree)
            for c in t.children:
                emit_string(c, emit, emit_fn)
    else:
        emit.skip(t)


def typed_child(t, n, ttype):
    assert isinstance(t, lark.tree.Tree)
    assert n < len(t.children)
    c = t.children[n]
    assert isinstance(c, lark.tree.Tree)
    assert c.data == ttype, t.pretty()
    return c


def rewrite_sig(tree, orig_sig, emit_fn=lambda x: 0):
    emit = StringEmit(orig_sig)
    emit_string(tree, emit, emit_fn)
    return str(emit)


def rewrite_signature(sig, tmap):
    def rewrite(t):
        if t.type == 'TNAME':
            new_type = tmap.get(t.value, None)
            if new_type is not None:
                t.value = new_type

    def emit_fn(t):
        if isinstance(t, lark.lexer.Token):
            return 0
        return -1 if t.data == 'param_defval' else 0

    xtree = _XPARSER.parse(sig)
    for_every_token(xtree, rewrite)
    return rewrite_sig(xtree, sig, emit_fn=emit_fn)


def create_stdfunc_sig(tree, orig_sig):
    def emit_fn(t):
        if isinstance(t, lark.lexer.Token):
            return 0
        return -1 if t.data == 'param_name' else 0

    emit = StringEmit(orig_sig)
    # Emit full function return type.
    emit_string(typed_child(tree, 0, 'type'), emit, emit_fn)
    emit.append('(')
    # Emit parameter list w/out parameter names.
    emit_string(typed_child(tree, 3, 'params'), emit, emit_fn)
    emit.append(')')
    return str(emit)


def create_map_sig(tree, orig_sig):
    def emit_fn(t):
        if isinstance(t, lark.lexer.Token):
            return -1 if t.type in ['CONST', 'REF', 'PTR'] else 0
        return -1 if t.data in ['param_name', 'param_defval'] else 0

    emit = StringEmit(orig_sig)
    # Emit full function return type.
    emit_string(typed_child(tree, 1, 'fnname'), emit, emit_fn)
    emit.append('(')
    # Emit parameter list w/out parameter names.
    emit_string(typed_child(tree, 3, 'params'), emit, emit_fn)
    emit.append(') -> ')
    emit_string(typed_child(tree, 0, 'type'), emit, emit_fn)
    return str(emit)


def type_core(t):
    assert isinstance(t, lark.tree.Tree)
    for c in t.children:
        if isinstance(c, lark.tree.Tree) and c.data == 'core_type':
            c = c.children[0]
            if isinstance(c, lark.lexer.Token):
                return c.value
            assert isinstance(c, lark.tree.Tree) and c.data == 'template'
            if c.children[0].value == 'c10::optional':
                type_list = c.children[1]
                assert isinstance(type_list, lark.tree.Tree) and type_list.data == 'typelist'
                return type_core(type_list.children[0])
            return c.children[0].value
    raise RuntimeError('Not a type tree: {}'.format(t))


def type_is_optional(t):
    assert isinstance(t, lark.tree.Tree)
    for c in t.children:
        if isinstance(c, lark.tree.Tree) and c.data == 'core_type':
            c = c.children[0]
            if isinstance(c, lark.lexer.Token):
                return False
            assert isinstance(c, lark.tree.Tree) and c.data == 'template'
            if c.children[0].value == 'c10::optional':
                return True
            else:
                return False
    raise RuntimeError('Not a type tree: {}'.format(t))


def type_is_const(t):
    assert isinstance(t, lark.tree.Tree)
    c = t.children[0]
    return isinstance(c, lark.lexer.Token) and c.value == 'const'


def fn_is_inplace(fname):
    if fname.endswith('_'):
        return True
    else:
        return False


def type_is_refptr(t, kind):
    assert isinstance(t, lark.tree.Tree)
    c = t.children[-1]
    if not isinstance(c, lark.tree.Tree) or c.data != 'refspec':
        return False
    c = c.children[0]
    return isinstance(c, lark.lexer.Token) and c.value == kind


def extract_list(t, l):
    assert isinstance(t, lark.tree.Tree)
    l.append(t.children[0])
    if len(t.children) == 2:
        c = t.children[1]
        if isinstance(c, lark.tree.Tree) and c.data == t.data:
            extract_list(c, l)
    return l


def tuple_type_list(t):
    assert isinstance(t, lark.tree.Tree)
    c = t.children[0]
    assert isinstance(c, lark.tree.Tree) and c.data == 'core_type'
    c = c.children[0]
    assert isinstance(c, lark.tree.Tree) and c.data == 'template'
    types = []
    return extract_list(c.children[1], types)


def get_function_name(t):
    assert isinstance(t, lark.tree.Tree)
    fname = t.children[1]
    assert isinstance(fname, lark.tree.Tree)
    assert fname.data == 'fnname'
    return fname.children[0].value


def get_function_signature(t, orig_sig, namefn):
    emit = StringEmit(orig_sig)
    # Emit full function return type.
    emit_string(typed_child(t, 0, 'type'), emit, lambda t: 0)
    fnname = typed_child(t, 1, 'fnname').children[0]
    xfname = namefn(fnname.value)
    emit.append(' {}('.format(xfname))
    # Emit parameter list w/out parameter names.
    emit_string(typed_child(t, 3, 'params'), emit, lambda t: 0)
    emit.append(')')
    return str(emit), fnname.value, xfname


def get_parameters(t):
    assert isinstance(t, lark.tree.Tree)
    c = t.children[2]
    assert isinstance(c, lark.tree.Tree)
    assert c.data == 'params'
    params = []
    extract_list(c, params)
    return params


def param_name(t):
    assert isinstance(t, lark.tree.Tree)
    c = t.children[1]
    assert isinstance(c, lark.tree.Tree)
    assert c.data == 'param_name'
    token = c.children[0]
    assert isinstance(token, lark.lexer.Token)
    return token.value


def param_type(t):
    assert isinstance(t, lark.tree.Tree)
    c = t.children[0]
    assert isinstance(c, lark.tree.Tree)
    return c


def get_optional(fnopts, name, defval=None):
    if fnopts is None or not hasattr(fnopts, name):
        return defval
    return getattr(fnopts, name, defval) or defval


def get_return_value(rtype, rname, param, var, ref_param, fnopts, fname):
    crtype = type_core(rtype)
    ret_check = ''
    if type_is_const(rtype) or type_is_refptr(rtype, '&'):
        # If the return type is a const or a reference, return the matching
        # parameter. In these cases we operated on IPEX tensors data (the ATEN one),
        # but the returned references are the input parameters.
        assert param
        return ret_check, param_name(param)
    elif crtype != 'Tensor':
        return ret_check, rname
    else:
        # If instead the return type is a value Tensor, we create a new one by
        # wrapping the proper local variable which has been created by calling
        # into the CPU tensor implementation.
        if fn_is_inplace(fname):
            # Conver at::Tensor AtenIpexCPUDefault::__xxx__(const at::Tensor & xxx, ...) {
            ptype = param_type(param)
            if type_is_const(ptype):
                # ret_check += '  TORCH_INTERNAL_ASSERT({}.is_contiguous());\n'.format(rname)
                return ret_check, 'bridge::{}({})'.format(_SHALLOW_UPGRADE_TO_DPCPP_TENSOR, rname)
            else:
                assert False
        else:
            # ret_check += '  TORCH_INTERNAL_ASSERT({}.is_contiguous());\n'.format(rname)
            return ret_check, 'bridge::{}({})'.format(_SHALLOW_UPGRADE_TO_DPCPP_TENSOR, rname)


def get_reference_param(params, fnopts=None):
    # The reference parameter is the Tensor object which we use to extract the
    # result Tensor device, if any.
    ref_param = None
    other = None
    for p in params:
        ptype = param_type(p)
        cptype = type_core(ptype)
        pname = param_name(p)
        if get_optional(fnopts, 'ref_param') == pname:
            return p
        if not other and (cptype == 'TensorOptions' or cptype == 'TensorList'):
            other = p
        if cptype != 'Tensor':
            continue
        if not ref_param and (pname == 'self' or type_is_const(ptype)):
            ref_param = p
        other = p
    return ref_param or other


def get_tuple_return(rtype, rtype_str, rname, params, param_vars, ref_param, fnopts, fname):
    types = tuple_type_list(rtype)
    ret_check_str = ''
    ret_str = '{}('.format(rtype_str)
    for i, ttype in enumerate(types):
        if i > 0:
            ret_str += ', '
        tuple_var = 'std::get<{}>({})'.format(i, rname)
        ret_check, ret = get_return_value(ttype,
                                          tuple_var,
                                          list_get(params, i),
                                          list_get(param_vars, i),
                                          ref_param,
                                          fnopts,
                                          fname)
        ret_str += ret
        ret_check_str += ret_check
    ret_str += ')'
    return ret_check_str, ret_str


def get_return_type_str(t, orig_sig):
    assert isinstance(t, lark.tree.Tree)
    fname = t.children[1]
    assert isinstance(fname, lark.tree.Tree)
    assert fname.data == 'fnname'
    token = fname.children[0]
    assert isinstance(token, lark.lexer.Token)
    return orig_sig[0:token.column - 2]


def generate_return_stmt(t, rtype_str, fname, rname, params, param_vars, ref_param, fnopts):
    assert isinstance(t, lark.tree.Tree)
    rtype = t.children[0]
    ctype = type_core(rtype)
    post_check = ''
    if ctype == 'std::tuple':
        assert not fn_is_inplace(fname)
        ret_check_str, retstr = get_tuple_return(rtype,
                                                 rtype_str,
                                                 rname,
                                                 params,
                                                 param_vars,
                                                 ref_param,
                                                 fnopts,
                                                 fname)
        post_check += ret_check_str
    elif ctype == 'std::vector':
        assert not fn_is_inplace(fname)
        retstr = 'bridge::{}({})'.format(_SHALLOW_UPGRADE_TO_DPCPP_TENSOR_VEC, rname)
    elif ctype == 'Tensor':
        ret_check_str, retstr = get_return_value(rtype,
                                                 rname,
                                                 params[0],
                                                 param_vars[0],
                                                 ref_param,
                                                 fnopts,
                                                 fname)
        post_check += ret_check_str
    elif ctype == 'void' and not type_is_refptr(rtype, '*'):
        return ''
    else:
        retstr = rname
    return post_check + '  return {};\n'.format(retstr)


def generate_result_assignment(t, rname):
    assert isinstance(t, lark.tree.Tree)
    rtype = t.children[0]
    ctype = type_core(rtype)
    if ctype == 'void' and not type_is_refptr(rtype, '*'):
        return ''
    return 'auto&& {} = '.format(rname)


def get_handling_function(ctx, fname, ipex_ref_param, param_vars):
    function = ctx.get_function(fname)
    if function:
        code = '{}({})'.format(function, ', '.join(param_vars))
    else:
        other_params = list(param_vars)
        other_params.remove(ipex_ref_param)
        code = '{}.{}({})'.format(ipex_ref_param, fname, ', '.join(other_params))
    return code


def rewrite_tensor_options(fname, pname):
    xname = '_cpu_{}'.format(pname)
    check_cond = '{}.device().type() == at::DeviceType::DPCPP'.format(pname)
    code = '  TORCH_INTERNAL_ASSERT({});\n'.format(check_cond)
    code += '  at::TensorOptions {} = {}.device(at::DeviceType::CPU);\n'.format(xname, pname)
    return code, xname


def get_param_names(params):
    param_vars = []
    for p in params:
        pname = param_name(p)
        param_vars.append(pname)
    return param_vars


def expand_fn_template(tmpl, param_vars):
    mdict = {}
    for i, pname in enumerate(param_vars):
        mdict[str(i)] = pname
    return tmpl.substitute(mdict)


def create_call(fname, param_vars):
    return '{}({})'.format(fname, ', '.join(param_vars))


def generate_shape_checks(param_vars, shape_check_indices, fname):
    code = ''
    for i, j in shape_check_indices:
        code += ('  XLA_CHECK({}.sizes() == {}.sizes()) << "Operand shapes must be '
                 'identical for {}, mismatch for arguments {} and {}";\n').format(
            param_vars[i], param_vars[j], fname, i + 1, j + 1)
    return code


def generate_aten_remap(ctx, fname, sig, params, fnopts):
    code = '{} {{\n'.format(sig)

    param_vars = get_param_names(params)
    if fnopts.outfn_template is not None:
        fcall = expand_fn_template(fnopts.outfn_template, param_vars)
    else:
        assert fnopts.outfn_name
        fcall = create_call(fnopts.outfn_name, param_vars)

    if fnopts.shape_check_indices is not None:
        code += generate_shape_checks(param_vars, fnopts.shape_check_indices, fname)
    code += '  return {};\n'.format(fcall)
    code += '}'
    return code


def generate_outfn_result_copy(dest, src):
    return '  {}.unsafeGetTensorImpl()->shallow_copy_from({}.getIntrusivePtr());\n'.format(
        dest, src)


def generate_aten_out(ctx, tree, rwxtree, fname, sig, rwsig, params, fnopts):
    rtype = tree.children[0]
    num_outputs = None
    if type_core(rtype) == 'std::tuple':
        num_outputs = len(tuple_type_list(rtype))

    code = '{} {{\n'.format(sig)

    param_vars = get_param_names(params)
    if fnopts.outfn_template is not None:
        fcall = expand_fn_template(fnopts.outfn_template, param_vars)
    else:
        m = re.match(r'(.*)_out$', fname)
        assert m is not None, fname
        out_count = num_outputs if num_outputs is not None else 1
        fcall = create_call('AtenXlaType::{}'.format(m.group(1)),
                            param_vars[out_count:])

    tmp_result = '{}_tmp'.format(fname)
    code += '  auto {} = {};\n'.format(tmp_result, fcall)
    if num_outputs is None:
        code += generate_outfn_result_copy(param_vars[0], tmp_result)
        code += '  return {};\n'.format(param_vars[0])
    else:
        for i in range(0, num_outputs):
            code += generate_outfn_result_copy(
                param_vars[i], 'std::get<{}>({})'.format(i, tmp_result))
        code += '  return {}('.format(get_return_type_str(rwxtree, rwsig))
        for i in range(0, num_outputs):
            if i > 0:
                code += ', '
            code += param_vars[i]
        code += ');\n'
    code += '}'
    return code


def generate_aten_to_ipex(ctx, tree, rwxtree, fname, sig, rwsig, params, fnopts):
    ref_param = get_reference_param(params, fnopts=fnopts)

    code = '{} {{\n'.format(sig)
    ipex_ref_param = param_name(ref_param) if ref_param else None
    tfetcher = TensorFetcher('IPEXSparseTensor')
    param_vars = []
    is_inplace = fn_is_inplace(fname)
    for p in params:
        ptype = param_type(p)
        cptype = type_core(ptype)
        pname = param_name(p)
        if cptype == 'TensorList':
            xname = '_cpu_{}'.format(pname)
            code += ('  auto&& {} = bridge::{}({});\n').format(xname, _SHALLOW_FALLBACK_TO_CPU_TENSOR_LIST, pname)
            param_vars.append(xname)
        elif cptype == 'TensorOptions':
            gcode, xname = rewrite_tensor_options(fname, pname)
            code += gcode
            param_vars.append(xname)
        elif cptype == 'Storage':
            code += '  TORCH_INTERNAL_ASSERT({}.device_type() == c10::DeviceType::DPCPP);\n'.format(pname)
            param_vars.append(pname)
        elif cptype == 'MemoryFormat':
            if type_is_optional(ptype):
                check_cond = '{}.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous'.format(pname)
            else:
                check_cond = '{} != c10::MemoryFormat::Contiguous'.format(pname)
            code += '  if ({})\n'.format(check_cond)
            code += '      TORCH_WARN({});\n'.format(check_cond)
            param_vars.append(pname)
        elif cptype != 'Tensor':
            param_vars.append(pname)

        # Tensor
        else:
            assert cptype == 'Tensor'
            if type_is_const(ptype):
                defval = is_write_param(fnopts, pname, False)
            else:
                defval = is_write_param(fnopts, pname, True)
            xname = tfetcher.add(pname, defval)
            param_vars.append(xname)

        if p == ref_param and not get_optional(fnopts, 'ref_param'):
            ipex_ref_param = param_vars[-1]

    code += tfetcher.generate_fetches()
    result_assign = generate_result_assignment(tree, _RESULT_NAME)
    code += '  {}{};\n'.format(
        result_assign,
        get_handling_function(ctx, fname, ipex_ref_param, param_vars))
    code += tfetcher.generate_updates()
    if result_assign:
        code += ('  static_cast<void>({}); // Avoid warnings in case not used\n'.format(_RESULT_NAME))
    code += generate_return_stmt(tree,
                                 get_return_type_str(rwxtree, rwsig),
                                 fname,
                                 _RESULT_NAME if result_assign else None, params,
                                 param_vars,
                                 ref_param,
                                 fnopts)
    code += '}'
    return code


def get_ipex_wrapper(fndecl, ctx):
    tree = _PARSER.parse(fndecl.cpp_sig)
    xtree = _XPARSER.parse(fndecl.cpp_sig)
    mapsig = create_map_sig(xtree, fndecl.cpp_sig)
    rwsig = rewrite_signature(fndecl.cpp_sig, _TYPE_NSMAP)
    rwxtree = _XPARSER.parse(rwsig)
    params = get_parameters(tree)
    fnopts = _FUNCTION_OPTIONS.get(mapsig, None)

    def gen_fnname(x):
        return 'AtenIpexCPUSparse::{}'.format(x)

    sig, fname, xfname = get_function_signature(rwxtree, rwsig, gen_fnname)

    if not is_blacklisted_fn(fname, mapsig):
        code = generate_aten_to_ipex(ctx, tree, rwxtree, fname, sig, rwsig, params, fnopts)
    return FuncGen(
        tree=tree,
        xtree=xtree,
        rwxtree=rwxtree,
        func=fname,
        xfunc=xfname,
        code=code,
        sig=fndecl.cpp_sig,
        rwsig=rwsig,
        cppsig=sig,
        mapsig=mapsig,
        funsig=create_stdfunc_sig(rwxtree, rwsig),
        aten_sig=fndecl.aten_sig)


def is_tensor_api(fndecl):
    fndecl = fndecl.replace('at::', '')
    fndecl = fndecl.replace('c10::Device', 'Device')
    m = re.search(r'\bTensor\b', fndecl)
    return m is not None, fndecl


def parse_functions(path):
    functions = []
    errors = []
    for line in open(path, 'r'):
        m = re.match(r'\s*([^\s].*); //\s+(.*)', line)
        if not m:
            continue
        fndecl = m.group(1)
        try:
            _XPARSER.parse(fndecl)
            functions.append(FuncDecl(cpp_sig=fndecl, aten_sig=m.group(2)))
        except Exception as e:
            if is_tensor_api(fndecl)[0]:
                errors.append((fndecl, str(e)))
                print('Error parsing "{}": {}'.format(fndecl, e), file=sys.stderr)
    return functions, errors


def get_mapsig_key(mapsig):
    # PyTorch generates std::tuple<> without space among the tuple types,
    # which would require special understanding in the string rewriter.
    # Since we are using this as simple key, we can just string the spaces.
    return mapsig.replace(' ', '')


def parse_local_overrides(path):
    functions = []
    fndecl = None
    for line in open(path, 'r'):
        line = line.strip()
        if not fndecl:
            m = re.match(r'static\s+(.*);', line)
            if m:
                functions.append(m.group(1))
                continue
            m = re.match(r'static\s+(.*)', line)
            if m:
                fndecl = m.group(1)
        else:
            fndecl = '{} {}'.format(fndecl, line)
            if fndecl.endswith(';'):
                functions.append(fndecl[:-1])
                fndecl = None
    assert fndecl is None

    overrides = {}
    for fndecl in functions:
        # Discard static XLA type functions which are not ATEN.
        is_tensor, fndecl = is_tensor_api(fndecl)
        if is_tensor:
            xtree = _XPARSER.parse(fndecl)
            mapsig_key = get_mapsig_key(create_map_sig(xtree, fndecl))
            overrides[mapsig_key] = fndecl
    return overrides


def get_overridden_fns(code_gens, overrides):
    overridden = set()
    for code_gen in code_gens:
        mapsig_key = get_mapsig_key(code_gen.mapsig)
        if mapsig_key in overrides:
            overridden.add(mapsig_key)
    return overridden


def generate_functions(code_gens):
    code = ''
    for code_gen in code_gens:
        if code_gen.code:
            code += '{}\n\n'.format(code_gen.code)
    return code


def generate_class_functions(code_gens):
    code = ''
    for code_gen in code_gens:
        if code_gen.code:
            code += '  static {};\n'.format(code_gen.rwsig)
    return code


def gen_output_file(args, name):
    if not args.output_folder:
        return sys.stdout
    return open(os.path.join(args.output_folder, name), 'w')


def gen_h_output_file(args, path):
    return gen_output_file(args, path)


def gen_cpp_output_file(args, path):
    return gen_output_file(args, path)


def check_overrides(overrides, overridden):
    misses = 0
    for mapsig, cpp_sig in overrides.items():
        mapsig_key = get_mapsig_key(mapsig)
        if not mapsig_key in overridden:
            misses += 1
            print('AtenIPEXType sparse function missed override: {}; // {}'.format(cpp_sig, mapsig), file=sys.stderr)
    return misses == 0


def generate(args):
    # Parse all PyTorch exposed functions
    decls, errors = parse_functions(args.declarations)
    print('Extracted {} functions ({} errors) from {}'.format(len(decls), len(errors), args.declarations), file=sys.stderr)
    assert len(errors) == 0

    overrides = parse_local_overrides(args.sparse_ops)
    print('{} function overrides in {}'.format(len(overrides), args.sparse_ops), file=sys.stderr)

    code_gens = []
    dense_ctx = Context(args.functions)
    sparse_ctx = Context(args.sparse_functions)

    for decl in decls:
        # Skip non Sparse functions
        tree = _PARSER.parse(decl.cpp_sig)
        if not sparse_ctx.contain_sig(decl.cpp_sig):
            continue
        # Skip Sparse Attribute Ops
        if Context(args.sparse_attrs).get_function(get_function_name(tree)):
            continue
        code_gen = get_ipex_wrapper(decl, dense_ctx)
        if code_gen:
            code_gens.append(code_gen)
    print('Generated {} wrappers for {}'.format(len(code_gens), args.declarations), file=sys.stderr)

    functions = generate_functions(code_gens)
    hfunctions = generate_class_functions(code_gens)
    overridden = get_overridden_fns(code_gens, overrides)
    # assert check_overrides(overrides, overridden)
    check_overrides(overrides, overridden)
    # Create output files ...
    with open(args.sparse_attrs, 'r') as ff:
        sparse_attrs = ff.read()
        print(
            _SPARSE_H_HEADER.format(gen=os.path.basename(sys.argv[0]), hfuncs= sparse_attrs + hfunctions),
            file=gen_h_output_file(args, 'SparseOPs.h'))
    print(
        _SPARSE_CPP_HEADER.format(
            gen=os.path.basename(sys.argv[0]), funcs=functions),
        file=gen_cpp_output_file(args, 'SparseOPs.cpp'))


if __name__ == '__main__':
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument('--output_folder', type=str)
    arg_parser.add_argument(
        'declarations',
        type=str,
        metavar='DECLARATIONS_FROM_PYTORCH',
        help='The path to PyTorch RegistrationDeclarations.h')
    arg_parser.add_argument(
        'functions',
        type=str,
        metavar='FUNCTIONS_FROM_PYTORCH',
        help='The path to the Functions.h')
    arg_parser.add_argument(
        'sparse_functions',
        type=str,
        metavar='SPARSE_FUNCTIONS_FROM_PYTORCH',
        help='The path to the SparseCPUType.h')
    arg_parser.add_argument(
        'sparse_attrs',
        type=str,
        metavar='SPARSE_ATTRS_FILE',
        help='The path to the SparseAttr.h')
    arg_parser.add_argument(
        'sparse_ops',
        type=str,
        metavar='SPARSE_OPS_HEADER',
        help='The path to IPEX overrides file for sparse ops')
    args, files = arg_parser.parse_known_args()
    print(args)
    print(files)
    generate(args)
