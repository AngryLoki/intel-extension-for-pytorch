From 9a60b998821f8f5ea8547ea28a80c2be1f74c875 Mon Sep 17 00:00:00 2001
From: zhuhong61 <95205772+zhuhong61@users.noreply.github.com>
Date: Tue, 1 Mar 2022 14:24:08 +0800
Subject: [PATCH 06/28] Add xpu for ddp (fix the index error in bert training)
 (#27)

---
 torch/nn/parallel/_functions.py  | 34 +++++++++++++++++++++++---------
 torch/nn/parallel/distributed.py | 34 ++++++++++++++++++++++----------
 2 files changed, 49 insertions(+), 19 deletions(-)

diff --git a/torch/nn/parallel/_functions.py b/torch/nn/parallel/_functions.py
index 834d444ec8..5a2464b650 100644
--- a/torch/nn/parallel/_functions.py
+++ b/torch/nn/parallel/_functions.py
@@ -93,14 +93,24 @@ class Scatter(Function):
         if torch.cuda.is_available() and ctx.input_device == -1:
             # Perform CPU to GPU copies in a background stream
             streams = [_get_stream(device) for device in target_gpus]
+        elif hasattr(torch, "xpu") and torch.xpu.is_available() and ctx.input_device == -1:
+            # Perform CPU to GPU copies in a background stream
+            streams = [_get_stream(device) for device in target_gpus]
         outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
         # Synchronize with the copy stream
         if streams is not None:
-            for i, output in enumerate(outputs):
-                with torch.cuda.device(target_gpus[i]):
-                    main_stream = torch.cuda.current_stream()
-                    main_stream.wait_stream(streams[i])
-                    output.record_stream(main_stream)
+            if torch.cuda.is_available():
+                for i, output in enumerate(outputs):
+                    with torch.cuda.device(target_gpus[i]):
+                        main_stream = torch.cuda.current_stream()
+                        main_stream.wait_stream(streams[i])
+                        output.record_stream(main_stream)
+            elif hasattr(torch, "xpu") and torch.xpu.is_available():
+                for i, output in enumerate(outputs):
+                    with torch.xpu.device(target_gpus[i]):
+                        main_stream = torch.xpu.current_stream()
+                        main_stream.wait_stream(streams[i])
+                        output.record_stream(main_stream)
         return outputs
 
     @staticmethod
@@ -117,8 +127,14 @@ def _get_stream(device: int):
     global _streams
     if device == -1:
         return None
-    if _streams is None:
-        _streams = [None] * torch.cuda.device_count()
-    if _streams[device] is None:
-        _streams[device] = torch.cuda.Stream(device)
+    if torch.cuda.is_available():
+        if _streams is None:
+            _streams = [None] * torch.cuda.device_count()
+        if _streams[device] is None:
+            _streams[device] = torch.cuda.Stream(device)
+    elif hasattr(torch, "xpu") and torch.xpu.is_available():
+        if _streams is None:
+            _streams = [None] * torch.xpu.device_count()
+        if _streams[device] is None:
+            _streams[device] = torch.xpu.Stream(device)
     return _streams[device]
diff --git a/torch/nn/parallel/distributed.py b/torch/nn/parallel/distributed.py
index 76260aa80d..9d5f7e9689 100644
--- a/torch/nn/parallel/distributed.py
+++ b/torch/nn/parallel/distributed.py
@@ -954,22 +954,36 @@ class DistributedDataParallel(Module, Joinable):
             if isinstance(obj, torch.Tensor):
                 if obj.device == torch.device("cuda", target_gpu):
                     return (obj,)
+                elif hasattr(torch, "xpu") and obj.device == torch.device("xpu", target_gpu):
+                    return (obj,)
                 if not self.use_side_stream_for_tensor_copies:
                     return (obj.to(target_gpu),)
                 else:
                     # Perform CPU -> GPU copies in a background stream. This code is
                     # motivated from similar logic in torch/nn/parallel/_functions.py
                     stream = _get_stream(target_gpu)
-                    with torch.cuda.stream(stream):
-                        output = obj.to(target_gpu)
-                    # synchronize with the copy stream
-                    with torch.cuda.device(target_gpu):
-                        current_stream = torch.cuda.current_stream()
-                        # Sync the current stream with the copy stream
-                        current_stream.wait_stream(stream)
-                        # Ensure tensor memory is not reused until work on
-                        # main stream is complete
-                        output.record_stream(current_stream)
+                    if torch.cuda.is_available():
+                        with torch.cuda.stream(stream):
+                            output = obj.to(target_gpu)
+                        # synchronize with the copy stream
+                        with torch.cuda.device(target_gpu):
+                            current_stream = torch.cuda.current_stream()
+                            # Sync the current stream with the copy stream
+                            current_stream.wait_stream(stream)
+                            # Ensure tensor memory is not reused until work on
+                            # main stream is complete
+                            output.record_stream(current_stream)
+                    elif hasattr(torch, "xpu") and torch.xpu.is_available():
+                        with torch.xpu.stream(stream):
+                            output = obj.to(target_gpu)
+                        # synchronize with the copy stream
+                        with torch.xpu.device(target_gpu):
+                            current_stream = torch.xpu.current_stream()
+                            # Sync the current stream with the copy stream
+                            current_stream.wait_stream(stream)
+                            # Ensure tensor memory is not reused until work on
+                            # main stream is complete
+                            output.record_stream(current_stream)
                     return (output,)
             if is_namedtuple(obj):
                 return [type(obj)(*args) for args in zip(*map(to_map, obj))]
-- 
2.25.1

