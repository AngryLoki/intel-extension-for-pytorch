From 8cab93822010cd5eef89cad9900701f8da5051d3 Mon Sep 17 00:00:00 2001
From: KevinYuk <kevin.yu@intel.com>
Date: Fri, 5 Aug 2022 15:04:30 +0800
Subject: [PATCH 25/35] Revert "channels last 1d for rebase pytorch 1.10 (#29)"
 (#53)

This reverts commit be2033436a83b80f91e9bd793802a3ad96875122.
---
 aten/src/ATen/core/TensorBase.h           |  8 +--
 aten/src/ATen/native/Convolution.cpp      |  2 +-
 aten/src/ATen/templates/Functions.cpp     |  3 -
 c10/core/MemoryFormat.h                   | 69 -----------------------
 c10/core/TensorImpl.cpp                   | 31 ----------
 c10/core/TensorImpl.h                     | 42 ++++----------
 test/test_sparse.py                       | 12 ++--
 test/test_torch.py                        | 30 +---------
 torch/csrc/utils/tensor_memoryformats.cpp |  1 -
 torch/nn/modules/module.py                |  2 +-
 10 files changed, 21 insertions(+), 179 deletions(-)

diff --git a/aten/src/ATen/core/TensorBase.h b/aten/src/ATen/core/TensorBase.h
index f06f33cb03..352aef9a2e 100644
--- a/aten/src/ATen/core/TensorBase.h
+++ b/aten/src/ATen/core/TensorBase.h
@@ -240,13 +240,7 @@ class TORCH_API TensorBase {
     // Setting channels_last_strides_exact_match to true forces function to
     // check 0,1 - sized dimension strides.
     if (!is_mkldnn() && !is_sparse()) {
-      if (impl_->is_strides_like_channels_last_1d()) {
-        if (!channels_last_strides_exact_match ||
-            get_channels_last_strides_1d(sizes()) == strides()) {
-          return at::MemoryFormat::ChannelsLast1d;
-        }
-      }
-      else if (impl_->is_strides_like_channels_last()) {
+      if (impl_->is_strides_like_channels_last()) {
         if (!channels_last_strides_exact_match ||
             get_channels_last_strides_2d(sizes()) == strides()) {
           return at::MemoryFormat::ChannelsLast;
diff --git a/aten/src/ATen/native/Convolution.cpp b/aten/src/ATen/native/Convolution.cpp
index 122f074089..78eb889f8c 100644
--- a/aten/src/ATen/native/Convolution.cpp
+++ b/aten/src/ATen/native/Convolution.cpp
@@ -830,7 +830,7 @@ at::Tensor _convolution(
 
   if (k == 3) {
     // avoid accidentally going through NHWC for permuted 3d input.
-    if (!input_is_mkldnn && !input.is_xpu()) {
+    if (!input_is_mkldnn) {
       input = input.contiguous();
     }
     params.view1d_as_2d();
diff --git a/aten/src/ATen/templates/Functions.cpp b/aten/src/ATen/templates/Functions.cpp
index 28ee3d2653..c98bd80a79 100644
--- a/aten/src/ATen/templates/Functions.cpp
+++ b/aten/src/ATen/templates/Functions.cpp
@@ -80,9 +80,6 @@ Tensor TensorMaker::make_tensor() {
    static std::int64_t zeros[5] = {0, 0, 0, 0, 0};
    if (opts_.has_memory_format()) {
      MemoryFormat format = *opts_.memory_format_opt();
-     if (format == MemoryFormat::ChannelsLast1d) {
-       return IntArrayRef(zeros, 3);
-     }
      if (format == MemoryFormat::ChannelsLast) {
        return IntArrayRef(zeros, 4);
      }
diff --git a/c10/core/MemoryFormat.h b/c10/core/MemoryFormat.h
index 311ae550e2..8cafde1b5c 100644
--- a/c10/core/MemoryFormat.h
+++ b/c10/core/MemoryFormat.h
@@ -28,7 +28,6 @@ namespace c10 {
 enum class MemoryFormat : int8_t {
   Contiguous,
   Preserve,
-  ChannelsLast1d,
   ChannelsLast,
   ChannelsLast3d
 };
@@ -50,8 +49,6 @@ inline std::ostream& operator<<(
       return stream << "Preserve";
     case MemoryFormat::Contiguous:
       return stream << "Contiguous";
-    case MemoryFormat::ChannelsLast1d:
-      return stream << "ChannelsLast1d";
     case MemoryFormat::ChannelsLast:
       return stream << "ChannelsLast";
     case MemoryFormat::ChannelsLast3d:
@@ -63,24 +60,6 @@ inline std::ostream& operator<<(
 
 // Note: Hardcoded the channel last stride indices here to get better
 // performance
-inline std::vector<int64_t> get_channels_last_strides_1d(IntArrayRef sizes) {
-  std::vector<int64_t> strides(sizes.size());
-  switch (sizes.size()) {
-    case 3:
-      strides[1] = 1;
-      strides[2] = sizes[1];
-      strides[0] = strides[2] * sizes[2];
-      return strides;
-    case 2:
-      strides[0] = 1;
-      strides[1] = sizes[0];
-      return strides;
-    default:
-      TORCH_INTERNAL_ASSERT(
-          false, "ChannelsLast1d doesn't support size ", sizes.size());
-  }
-}
-
 inline std::vector<int64_t> get_channels_last_strides_2d(IntArrayRef sizes) {
   std::vector<int64_t> strides(sizes.size());
   switch (sizes.size()) {
@@ -134,40 +113,6 @@ inline std::vector<int64_t> get_channels_last_strides_3d(IntArrayRef sizes) {
 // input
 // 3. All helper functions have similar comments, only 1st helper function is
 // commented here.
-inline bool is_channels_last_strides_1d_s3(
-    const IntArrayRef sizes,
-    const IntArrayRef strides) {
-  int64_t min = 0;
-  // special case for trivial C dimension. default to NCL
-  if (strides[1] == 0) {
-    return false;
-  }
-  // loop strides indices
-  for (auto& d : {1, 2, 0}) {
-    if (sizes[d] == 0) {
-      return false;
-    }
-    if (strides[d] < min) {
-      return false;
-    }
-    // Fallback to NCL as default layout for ambiguous cases
-    // This is the flaw of implicit memory_format from strides.
-    // N11 tensor with identical strides for size 1 dimension;
-    // Two cases could lead us here:
-    // a. N11 contiguous Tensor ([N,1,1]@[1,1,1])
-    // b. N1L contiguous Tensor sliced on the L-dimension. ([N,1,1]@[L,L,L])
-    if (d == 0 && min == strides[1]) {
-      return false;
-    }
-
-    min = strides[d];
-    if (sizes[d] > 1) {
-      min *= sizes[d];
-    }
-  }
-  return true;
-}
-
 inline bool is_channels_last_strides_2d_s4(
     const IntArrayRef sizes,
     const IntArrayRef strides) {
@@ -284,20 +229,6 @@ inline bool is_channels_last_strides_3d_s5(
 // implementation. Please check the helper functions
 // (is_channels_last_strides_*d_s*) for more details.
 
-inline bool is_channels_last_strides_1d(
-    const IntArrayRef sizes,
-    const IntArrayRef strides) {
-  switch (sizes.size()) {
-    case 3:
-      return is_channels_last_strides_1d_s3(sizes, strides);
-    case 2:
-      // TODO dim == 2 case will be enabled once it is fully tested
-      return false;
-    default:
-      return false;
-  }
-}
-
 inline bool is_channels_last_strides_2d(
     const IntArrayRef sizes,
     const IntArrayRef strides) {
diff --git a/c10/core/TensorImpl.cpp b/c10/core/TensorImpl.cpp
index 2616644650..fe8ad99e42 100644
--- a/c10/core/TensorImpl.cpp
+++ b/c10/core/TensorImpl.cpp
@@ -266,32 +266,6 @@ bool TensorImpl::compute_contiguous() const {
   return is_contiguous;
 }
 
-bool TensorImpl::compute_channels_last_contiguous_1d() const {
-  // Please don't combine these code, constant array is used here to let
-  // compiler fully unroll the loop to get better performance
-  switch (sizes_and_strides_.size()) {
-    case 3: {
-      int64_t expected = 1;
-      for (auto& d : {1, 2, 0}) {
-        const auto size_d = sizes_and_strides_.size_at_unchecked(d);
-        if (size_d != 1) {
-          if (sizes_and_strides_.stride_at_unchecked(d) != expected) {
-            return false;
-          }
-          expected *= size_d;
-        }
-      }
-      return true;
-    }
-    // NOLINTNEXTLINE(bugprone-branch-clone)
-    case 2:
-      // TODO dim == 2 case will be enabled once it is fully tested
-      return false;
-    default:
-      return false;
-  }
-}
-
 bool TensorImpl::compute_channels_last_contiguous_2d() const {
   // Please don't combine these code, constant array is used here to let
   // compiler fully unroll the loop to get better performance
@@ -344,11 +318,6 @@ bool TensorImpl::compute_channels_last_contiguous_3d() const {
   }
 }
 
-bool TensorImpl::compute_strides_like_channels_last_1d() const {
-  return is_channels_last_strides_1d(
-      TensorImpl::sizes(), TensorImpl::strides());
-}
-
 bool TensorImpl::compute_strides_like_channels_last_2d() const {
   return is_channels_last_strides_2d(
       TensorImpl::sizes(), TensorImpl::strides());
diff --git a/c10/core/TensorImpl.h b/c10/core/TensorImpl.h
index 6b519d6313..bd47391f61 100644
--- a/c10/core/TensorImpl.h
+++ b/c10/core/TensorImpl.h
@@ -790,9 +790,7 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
       return is_contiguous_nondefault_policy_impl(memory_format);
     }
     TORCH_INTERNAL_ASSERT_DEBUG_ONLY(compute_contiguous() == is_contiguous_);
-    if (memory_format == at::MemoryFormat::ChannelsLast1d) {
-      return compute_channels_last_contiguous_1d();
-    } else if (memory_format == at::MemoryFormat::ChannelsLast) {
+    if (memory_format == at::MemoryFormat::ChannelsLast) {
       return is_channels_last_contiguous_;
     } else if (memory_format == at::MemoryFormat::ChannelsLast3d) {
       return is_channels_last_3d_contiguous_;
@@ -2080,12 +2078,6 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
         }
         break;
       }
-      case MemoryFormat::ChannelsLast1d: {
-        TORCH_CHECK(
-            dim() == 3, "required rank 3 tensor to use channels_last format");
-        set_sizes_and_strides(sizes(), get_channels_last_strides_1d(sizes()));
-        break;
-      }
       case MemoryFormat::ChannelsLast: {
         TORCH_CHECK(
             dim() == 4, "required rank 4 tensor to use channels_last format");
@@ -2110,10 +2102,6 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
     refresh_contiguous();
   }
 
-  bool is_strides_like_channels_last_1d() const {
-    return compute_strides_like_channels_last_1d();
-  }
-
   bool is_strides_like_channels_last() const {
     return is_channels_last_;
   }
@@ -2223,14 +2211,10 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
    */
   bool compute_contiguous() const;
 
-  bool compute_channels_last_contiguous_1d() const;
-
   bool compute_channels_last_contiguous_2d() const;
 
   bool compute_channels_last_contiguous_3d() const;
 
-  bool compute_strides_like_channels_last_1d() const;
-
   bool compute_strides_like_channels_last_2d() const;
 
   bool compute_strides_like_channels_last_3d() const;
@@ -2267,10 +2251,10 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
   void refresh_contiguous() {
     is_contiguous_ = compute_contiguous();
     // Note:
-    // Dim 0, 1, 2  will never be a channels last 2d/3d format
-    // Dim 3+ is possibly be a channels last 2d format (Dim 4 only at
-    // this point) Dim 4+ is possibly be a channels last 3d format (Dim 5 only
-    // at this point)
+    // Dim 0, 1, 2 will never be a channels last 2d/3d format
+    // Dim 3+ is possibly be a channels last 2d format (Dim 4 only at this
+    // point) Dim 4+ is possibly be a channels last 3d format (Dim 5 only at
+    // this point)
     switch (dim()) {
       case 4:
         is_channels_last_contiguous_ = compute_channels_last_contiguous_2d();
@@ -2278,8 +2262,7 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
         is_channels_last_ = compute_strides_like_channels_last_2d();
         is_channels_last_3d_ = false;
         is_non_overlapping_and_dense_ = is_contiguous_ ||
-            is_channels_last_contiguous_ ||
-            compute_non_overlapping_and_dense();
+            is_channels_last_contiguous_ || compute_non_overlapping_and_dense();
         break;
       case 5:
         is_channels_last_contiguous_ = compute_channels_last_contiguous_2d();
@@ -2290,18 +2273,17 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
         is_channels_last_3d_ =
             !is_channels_last_ && compute_strides_like_channels_last_3d();
         is_non_overlapping_and_dense_ = is_contiguous_ ||
-            is_channels_last_contiguous_ ||
-            is_channels_last_3d_contiguous_ ||
+            is_channels_last_contiguous_ || is_channels_last_3d_contiguous_ ||
             compute_non_overlapping_and_dense();
         break;
       default:
         is_channels_last_contiguous_ = false;
         is_channels_last_3d_contiguous_ = false;
-        // is_channels_last_ and is_channels_last_3d_ are
-        // suggested memory_format. memory_format. Being
-        // channels_last_contiguous doesn't necessarily mean the tensor is
-        // strided like channels_last: for strides on channel dimension could
-        // suggest desired memory_layout, but it doesn't affect memory storage
+        // is_channels_last_ and is_channels_last_3d_ are suggested
+        // memory_format. Being channels_last_contiguous doesn't necessarily
+        // mean the tensor is strided like channels_last: for strides on channel
+        // dimension could suggest desired memory_layout, but it doesn't affect
+        // memory storage
         is_channels_last_ = false;
         is_channels_last_3d_ = false;
         is_non_overlapping_and_dense_ =
diff --git a/test/test_sparse.py b/test/test_sparse.py
index 9c9aac5e77..f9ed0dc11f 100644
--- a/test/test_sparse.py
+++ b/test/test_sparse.py
@@ -1816,10 +1816,8 @@ class TestSparse(TestCase):
         test_shape([2, 3, 4], [0, 4, 5, 6], [9, 12])
 
         sparse_tensor, _, _ = self._gen_sparse(len([2, 3]), 9, [2, 3] + [5, 6], dtype, device, coalesced)
-        data = (sparse_tensor, sparse_tensor, sparse_tensor,
-                sparse_tensor.to_dense().reshape([2 * 3, 5, 6]).to_sparse(), sparse_tensor.unsqueeze(0))
-        mem_formats = [torch.channels_last, torch.contiguous_format, torch.preserve_format,
-                       torch.channels_last_1d, torch.channels_last_3d]
+        data = (sparse_tensor, sparse_tensor, sparse_tensor, sparse_tensor.unsqueeze(0))
+        mem_formats = [torch.channels_last, torch.contiguous_format, torch.preserve_format, torch.channels_last_3d]
         for x, mem_format in zip(data, mem_formats):
 
             with self.assertRaisesRegex(RuntimeError, "memory format option is only supported by strided tensors"):
@@ -1855,10 +1853,8 @@ class TestSparse(TestCase):
         self.assertEqual(result.dense_dim(), sparse_tensor.dense_dim())
 
         sparse_tensor, _, _ = self._gen_sparse(len([2, 3]), 9, [2, 3] + [5, 6], dtype, device, coalesced)
-        data = (sparse_tensor, sparse_tensor, sparse_tensor,
-                sparse_tensor.to_dense().reshape([2 * 3, 5, 6]).to_sparse(), sparse_tensor.unsqueeze(0))
-        mem_formats = [torch.channels_last, torch.contiguous_format, torch.preserve_format,
-                       torch.channels_last_1d, torch.channels_last_3d]
+        data = (sparse_tensor, sparse_tensor, sparse_tensor, sparse_tensor.unsqueeze(0))
+        mem_formats = [torch.channels_last, torch.contiguous_format, torch.preserve_format, torch.channels_last_3d]
         for x, mem_format in zip(data, mem_formats):
 
             with self.assertRaisesRegex(RuntimeError, "memory format option is only supported by strided tensors"):
diff --git a/test/test_torch.py b/test/test_torch.py
index 3fbeb50eea..a43f0ac9f2 100644
--- a/test/test_torch.py
+++ b/test/test_torch.py
@@ -2476,7 +2476,6 @@ tensor([[[1.+1.j, 1.+1.j, 1.+1.j,  ..., 1.+1.j, 1.+1.j, 1.+1.j],
                 self.assertTrue(y.is_contiguous(memory_format=memory_format))
                 self.assertEqual(y, x)
 
-            test_helper(torch.randn(4, 3, 8), torch.channels_last_1d)
             test_helper(torch.randn(4, 3, 8, 8), torch.channels_last)
             test_helper(torch.randn(4, 3, 8, 8, 8), torch.channels_last_3d)
 
@@ -2486,7 +2485,6 @@ tensor([[[1.+1.j, 1.+1.j, 1.+1.j,  ..., 1.+1.j, 1.+1.j, 1.+1.j],
                 alias.fill_(7)
                 self.assertEqual(x, alias)
 
-            test_helper(torch.randn(4, 8, 3).permute(0, 2, 1), torch.channels_last_1d)
             test_helper(torch.randn(4, 8, 8, 3).permute(0, 3, 1, 2), torch.channels_last)
             test_helper(torch.randn(4, 8, 8, 8, 3).permute(0, 4, 1, 2, 3), torch.channels_last_3d)
 
@@ -2497,7 +2495,6 @@ tensor([[[1.+1.j, 1.+1.j, 1.+1.j,  ..., 1.+1.j, 1.+1.j, 1.+1.j],
                 x = torch.empty(dim2, memory_format=memory_format)
                 self.assertTrue(x.is_contiguous(memory_format=memory_format))
 
-            test_helper((3), (3, 3, 3), torch.channels_last_1d)
             test_helper((3, 3), (3, 3, 3, 3), torch.channels_last)
             test_helper((3, 3, 3), (3, 3, 3, 3, 3), torch.channels_last_3d)
 
@@ -6807,11 +6804,6 @@ else:
         _test_serialization(BytesIOContext)
 
     def test_memory_format_preserved_after_permute(self, device):
-        x = torch.randn(4, 3, 8, device=device)
-        nwc = x.contiguous(memory_format=torch.channels_last_1d)
-        y = nwc.permute(0, 2, 1).permute(0, 2, 1)
-        self.assertTrue(y.is_contiguous(memory_format=torch.channels_last_1d))
-
         x = torch.randn(4, 3, 8, 8, device=device)
         nhwc = x.contiguous(memory_format=torch.channels_last)
         y = nhwc.permute(0, 1, 3, 2).permute(0, 1, 3, 2)
@@ -6892,7 +6884,6 @@ else:
             with self.assertRaises(RuntimeError):
                 z = torch.empty_like(sparse, memory_format=torch.preserve_format)
 
-        test_helper(torch.randn(4, 3, 8, device=device), torch.channels_last_1d)
         test_helper(torch.randn(4, 3, 8, 8, device=device), torch.channels_last)
         test_helper(torch.randn(4, 3, 8, 8, 8, device=device), torch.channels_last_3d)
 
@@ -6902,8 +6893,6 @@ else:
         self.assertEqual(x.size(), x_rep.size())
         self.assertEqual(x.stride(), x_rep.stride())
         self.assertEqual(x.is_contiguous(), x_rep.is_contiguous())
-        self.assertEqual(
-            x.is_contiguous(memory_format=torch.channels_last_1d), x_rep.is_contiguous(memory_format=torch.channels_last_1d))
         self.assertEqual(x.is_contiguous(memory_format=torch.channels_last), x_rep.is_contiguous(memory_format=torch.channels_last))
         self.assertEqual(
             x.is_contiguous(memory_format=torch.channels_last_3d), x_rep.is_contiguous(memory_format=torch.channels_last_3d))
@@ -7061,12 +7050,6 @@ else:
                     result.is_contiguous(memory_format=torch.contiguous_format),
                     "result of the '{}' is not in '{}' format".format(inspect.getsource(fn).strip(), torch.contiguous_format))
 
-        _test_helper(
-            torch.randn((4, 3, 8), device=device).contiguous(memory_format=torch.channels_last_1d),
-            abs(torch.randn((4, 3, 8), device=device)) + 1,
-            torch.randn((1, 3, 1), device=device).contiguous(memory_format=torch.channels_last_1d),
-            torch.channels_last_1d)
-
         _test_helper(
             torch.randn((4, 3, 8, 8), device=device).contiguous(memory_format=torch.channels_last),
             abs(torch.randn((4, 3, 8, 8), device=device)) + 1,
@@ -7401,16 +7384,12 @@ else:
     def _test_memory_format_transformations(self, device, input_generator_fn, transformation_fn,
                                             memory_format, compare_data=True, default_is_preserve=False):
 
-        assert(memory_format == torch.channels_last_1d or
-               memory_format == torch.channels_last or
-               memory_format == torch.channels_last_3d)
+        assert(memory_format == torch.channels_last or memory_format == torch.channels_last_3d)
 
         # xc is a channels last tensor
         xc = input_generator_fn(device)
         # xc is not memory dense, but looks like channels last
-        if memory_format == torch.channels_last_1d:
-            xc = xc[..., ::2]
-        elif memory_format == torch.channels_last:
+        if memory_format == torch.channels_last:
             xc = xc[..., ::2, ::2]
         else:
             xc = xc[..., ::2, ::2, ::2]
@@ -7459,7 +7438,6 @@ else:
             return tensor.to(dtype=torch.float64, **kwargs)
 
         formats_shapes = (
-            (torch.channels_last_1d, (4, 3, 8)),
             (torch.channels_last, (4, 3, 8, 8)),
             (torch.channels_last_3d, (4, 3, 8, 8, 8)))
 
@@ -7477,7 +7455,6 @@ else:
             return tensor.to(torch.float64, **kwargs)
 
         formats_shapes = (
-            (torch.channels_last_1d, (4, 3, 8)),
             (torch.channels_last, (4, 3, 8, 8)),
             (torch.channels_last_3d, (4, 3, 8, 8, 8)))
 
@@ -7495,7 +7472,6 @@ else:
             return tensor.clone(**kwargs)
 
         formats_shapes = (
-            (torch.channels_last_1d, (4, 3, 8)),
             (torch.channels_last, (4, 3, 8, 8)),
             (torch.channels_last_3d, (4, 3, 8, 8, 8)))
 
@@ -7520,7 +7496,6 @@ else:
             lambda t, **kwargs: torch.empty_like(t, **kwargs)]
 
         formats_shapes = (
-            (torch.channels_last_1d, (4, 3, 8)),
             (torch.channels_last, (4, 3, 8, 8)),
             (torch.channels_last_3d, (4, 3, 8, 8, 8)))
 
@@ -7548,7 +7523,6 @@ else:
             shortcuts += ['bfloat16']
 
         formats_shapes = (
-            (torch.channels_last_1d, (4, 3, 8)),
             (torch.channels_last, (4, 3, 8, 8)),
             (torch.channels_last_3d, (4, 3, 8, 8, 8)))
 
diff --git a/torch/csrc/utils/tensor_memoryformats.cpp b/torch/csrc/utils/tensor_memoryformats.cpp
index 3868c6f75b..9eb8f413db 100644
--- a/torch/csrc/utils/tensor_memoryformats.cpp
+++ b/torch/csrc/utils/tensor_memoryformats.cpp
@@ -30,7 +30,6 @@ void initializeMemoryFormats() {
 
   _ADD_MEMORY_FORMAT(at::MemoryFormat::Preserve, "preserve_format");
   _ADD_MEMORY_FORMAT(at::MemoryFormat::Contiguous, "contiguous_format");
-  _ADD_MEMORY_FORMAT(at::MemoryFormat::ChannelsLast1d, "channels_last_1d");
   _ADD_MEMORY_FORMAT(at::MemoryFormat::ChannelsLast, "channels_last");
   _ADD_MEMORY_FORMAT(at::MemoryFormat::ChannelsLast3d, "channels_last_3d");
 
diff --git a/torch/nn/modules/module.py b/torch/nn/modules/module.py
index 3a552ce693..28b220e240 100644
--- a/torch/nn/modules/module.py
+++ b/torch/nn/modules/module.py
@@ -891,7 +891,7 @@ class Module:
                     "if a complex module does not work as expected.")
 
         def convert(t):
-            if convert_to_format is not None and t.dim() in (3, 4, 5):
+            if convert_to_format is not None and t.dim() in (4, 5):
                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
                             non_blocking, memory_format=convert_to_format)
             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
-- 
2.25.1

