From 3acdba313b81ee18b776ecc9256e573647f89dab Mon Sep 17 00:00:00 2001
From: Gu Jinghui <jinghui.gu@intel.com>
Date: Fri, 14 Aug 2020 00:51:25 +0800
Subject: [PATCH 6/7] Add QuantizedDPCPP backend for INT8 support on DPCPP
 device

---
 aten/src/ATen/preprocess_declarations.py |  4 ++--
 c10/core/Backend.h                       | 14 +++++++++++++-
 c10/core/DispatchKey.cpp                 |  2 ++
 c10/core/DispatchKey.h                   |  1 +
 c10/core/TensorImpl.h                    |  3 ++-
 c10/core/TensorOptions.h                 |  9 +++++++--
 setup.py                                 |  1 +
 torch/csrc/utils/tensor_layouts.cpp      |  1 +
 8 files changed, 29 insertions(+), 6 deletions(-)

diff --git a/aten/src/ATen/preprocess_declarations.py b/aten/src/ATen/preprocess_declarations.py
index b7d56125f4..85ad08fe60 100644
--- a/aten/src/ATen/preprocess_declarations.py
+++ b/aten/src/ATen/preprocess_declarations.py
@@ -28,7 +28,7 @@ type_map = {
 all_types = type_map['floating_point'] + type_map['integral'] + type_map['quantized']
 type_map['all'] = all_types
 
-all_backends = ['CPU', 'CUDA', 'SparseCPU', 'SparseCUDA', 'MkldnnCPU', 'QuantizedCPU']
+all_backends = ['CPU', 'CUDA', 'SparseCPU', 'SparseCUDA', 'MkldnnCPU', 'QuantizedCPU', 'QuantizedDPCPP']
 default_backends = ['CPU', 'CUDA']
 
 
@@ -44,7 +44,7 @@ def process_types_and_backends(option):
 
         backend_types = {}
         for backend in backends:
-            if backend == 'QuantizedCPU':
+            if backend in ('QuantizedCPU', 'QuantizedDPCPP'):
                 backend_types[backend] = type_map['quantized']
             else:
                 backend_types[backend] = option.get('types', all_types)
diff --git a/c10/core/Backend.h b/c10/core/Backend.h
index 19e215e558..651e8ebdf5 100644
--- a/c10/core/Backend.h
+++ b/c10/core/Backend.h
@@ -25,7 +25,7 @@ namespace c10 {
  * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
  * "CUDNN".
  */
-enum class Backend { CPU, CUDA, HIP, DPCPP, SparseCPU, SparseCUDA, SparseHIP, SparseDPCPP, MSNPU, XLA, QuantizedCPU, Undefined, MkldnnCPU, NumOptions };
+enum class Backend { CPU, CUDA, HIP, DPCPP, SparseCPU, SparseCUDA, SparseHIP, SparseDPCPP, MSNPU, XLA, QuantizedCPU, QuantizedDPCPP, Undefined, MkldnnCPU, NumOptions };
 
 static inline Backend toSparse(Backend b) {
   switch (b) {
@@ -72,6 +72,8 @@ static inline Backend toDense(Backend b) {
       return Backend::HIP;
     case Backend::QuantizedCPU:
       return Backend::QuantizedCPU;
+    case Backend::QuantizedDPCPP:
+      return Backend::QuantizedDPCPP;
     default:
       throw std::runtime_error("Unknown backend");
   }
@@ -102,6 +104,8 @@ static inline Backend dispatchKeyToBackend(DispatchKey t) {
     return Backend::MkldnnCPU;
   } else if (t == DispatchKey::QuantizedCPUTensorId) {
     return Backend::QuantizedCPU;
+  } else if (t == DispatchKey::QuantizedDPCPPTensorId) {
+    return Backend::QuantizedDPCPP;
   } else if (t == DispatchKey::Undefined) {
     return Backend::Undefined;
   } else {
@@ -135,6 +139,8 @@ static inline DispatchKey backendToDispatchKey(Backend b) {
       return DispatchKey::MkldnnCPUTensorId;
     case Backend::QuantizedCPU:
       return DispatchKey::QuantizedCPUTensorId;
+    case Backend::QuantizedDPCPP:
+      return DispatchKey::QuantizedDPCPPTensorId;
     case Backend::Undefined:
       return DispatchKey::Undefined;
     default:
@@ -166,6 +172,8 @@ static inline DeviceType backendToDeviceType(Backend b) {
     case Backend::MkldnnCPU:
     case Backend::QuantizedCPU:
       return DeviceType::CPU;
+    case Backend::QuantizedDPCPP:
+      return DeviceType::DPCPP;
     case Backend::Undefined:
       AT_ERROR("Undefined backend is not a valid device type");
     default:
@@ -196,6 +204,8 @@ static inline Backend backendToCPU(Backend b) {
       return Backend::MkldnnCPU;
     case Backend::QuantizedCPU:
       return Backend::QuantizedCPU;
+    case Backend::QuantizedDPCPP:
+      return Backend::QuantizedCPU;
     case Backend::Undefined:
       return Backend::Undefined;
     default:
@@ -272,6 +282,8 @@ static inline const char* toString(Backend b) {
       return "MkldnnCPU";
     case Backend::QuantizedCPU:
       return "QuantizedCPU";
+    case Backend::QuantizedDPCPP:
+      return "QuantizedDPCPP";
     default:
       return "UNKNOWN_BACKEND";
   }
diff --git a/c10/core/DispatchKey.cpp b/c10/core/DispatchKey.cpp
index c3863e54e9..9262042d57 100644
--- a/c10/core/DispatchKey.cpp
+++ b/c10/core/DispatchKey.cpp
@@ -38,6 +38,8 @@ const char* toString(DispatchKey t) {
       return "MkldnnCPUTensorId";
     case DispatchKey::QuantizedCPUTensorId:
       return "QuantizedCPUTensorId";
+    case DispatchKey::QuantizedDPCPPTensorId:
+      return "QuantizedDPCPPTensorId";
     case DispatchKey::VariableTensorId:
       return "VariableTensorId";
     case DispatchKey::BackendSelect:
diff --git a/c10/core/DispatchKey.h b/c10/core/DispatchKey.h
index 0c34c70be4..150f3dc612 100644
--- a/c10/core/DispatchKey.h
+++ b/c10/core/DispatchKey.h
@@ -65,6 +65,7 @@ enum class DispatchKey : uint8_t {
   // Here are backends which specify more specialized operators
   // based on the dtype of the tensor.
   QuantizedCPUTensorId, // registered at build/aten/src/ATen/QuantizedCPUType.cpp
+  QuantizedDPCPPTensorId, // registered at build/aten/src/ATen/QuantizedDPCPPType.cpp
   ComplexCPUTensorId,   // lives out of tree at https://gitlab.com/pytorch-complex/pytorch-cpu-strided-complex
   ComplexCUDATensorId,  // and https://gitlab.com/pytorch-complex/pytorch-cuda-strided-complex
                         // tested at test/cpp_extensions/complex_registration_extension.cpp
diff --git a/c10/core/TensorImpl.h b/c10/core/TensorImpl.h
index de11b22141..b11388a32b 100644
--- a/c10/core/TensorImpl.h
+++ b/c10/core/TensorImpl.h
@@ -430,7 +430,8 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
 
   bool is_quantized() const {
     // NB: This method is not virtual and avoid dispatches for performance reasons.
-    return key_set_.has(DispatchKey::QuantizedCPUTensorId);
+    return key_set_.has(DispatchKey::QuantizedCPUTensorId) ||
+           key_set_.has(DispatchKey::QuantizedDPCPPTensorId);
   }
 
   bool is_cuda() const {
diff --git a/c10/core/TensorOptions.h b/c10/core/TensorOptions.h
index 8574aafe19..b170e242a4 100644
--- a/c10/core/TensorOptions.h
+++ b/c10/core/TensorOptions.h
@@ -398,8 +398,13 @@ struct C10_API TensorOptions {
             return DispatchKey::MSNPUTensorId;
           case DeviceType::XLA:
             return DispatchKey::XLATensorId;
-          case DeviceType::DPCPP:
-            return DispatchKey::DPCPPTensorId;
+          case DeviceType::DPCPP: {
+            auto dtype_tmp = typeMetaToScalarType(dtype());
+              if (isQIntType(dtype_tmp)) {
+                return DispatchKey::QuantizedDPCPPTensorId;
+              }
+              return DispatchKey::DPCPPTensorId;
+          }
           default:
             AT_ERROR("Unsupported device type for dense layout: ", device().type());
         }
diff --git a/setup.py b/setup.py
index 7352d3b667..a10f0804c1 100644
--- a/setup.py
+++ b/setup.py
@@ -797,6 +797,7 @@ if __name__ == '__main__':
                 'include/ATen/native/cpu/*.h',
                 'include/ATen/native/quantized/*.h',
                 'include/ATen/native/quantized/cpu/*.h',
+                'include/ATen/quantized/*.h',
                 'include/caffe2/utils/*.h',
                 'include/caffe2/utils/**/*.h',
                 'include/c10/*.h',
diff --git a/torch/csrc/utils/tensor_layouts.cpp b/torch/csrc/utils/tensor_layouts.cpp
index e42a7baf27..55404ea733 100644
--- a/torch/csrc/utils/tensor_layouts.cpp
+++ b/torch/csrc/utils/tensor_layouts.cpp
@@ -25,6 +25,7 @@ void initializeLayouts() {
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::XLA);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::DPCPP);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::QuantizedCPU);
+  registerLayoutObject((THPLayout*)strided_layout, at::Backend::QuantizedDPCPP);
 
   PyObject *sparse_coo_layout = THPLayout_New(at::Layout::Sparse, "torch.sparse_coo");
   Py_INCREF(sparse_coo_layout);
-- 
2.17.1

