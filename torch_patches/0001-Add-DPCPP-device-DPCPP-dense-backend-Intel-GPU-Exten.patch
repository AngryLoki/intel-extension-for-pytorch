From 321b4e5a4ba7617b41dda048abc6985ac49c175b Mon Sep 17 00:00:00 2001
From: fengyuan <feng1.yuan@intel.com>
Date: Fri, 6 Dec 2019 10:31:03 +0800
Subject: [PATCH 1/9] Add DPCPP device/DPCPP (dense) backend (Intel GPU
 Extension for PyTorch)

Signed-off-by: fengyuan <feng1.yuan@intel.com>
---
 c10/core/Backend.h                  | 30 +++++++++-
 c10/core/Device.cpp                 |  5 +-
 c10/core/DeviceType.cpp             |  3 +
 c10/core/DeviceType.h               |  4 +-
 c10/core/DispatchKey.cpp            |  4 ++
 c10/core/DispatchKey.h              |  2 +
 c10/core/TensorOptions.h            |  8 +++
 torch/autograd/profiler.py          | 91 +++++++++++++++++++++++++----
 torch/csrc/autograd/init.cpp        |  3 +
 torch/csrc/autograd/profiler.cpp    | 57 ++++++++++++++----
 torch/csrc/autograd/profiler.h      | 58 ++++++++++++++++--
 torch/csrc/utils/tensor_layouts.cpp |  1 +
 torch/csrc/utils/tensor_new.cpp     |  3 +
 13 files changed, 239 insertions(+), 30 deletions(-)

diff --git a/c10/core/Backend.h b/c10/core/Backend.h
index 5f3d8c7733..19e215e558 100644
--- a/c10/core/Backend.h
+++ b/c10/core/Backend.h
@@ -25,12 +25,14 @@ namespace c10 {
  * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
  * "CUDNN".
  */
-enum class Backend { CPU, CUDA, HIP, SparseCPU, SparseCUDA, SparseHIP, MSNPU, XLA, QuantizedCPU, Undefined, MkldnnCPU, NumOptions };
+enum class Backend { CPU, CUDA, HIP, DPCPP, SparseCPU, SparseCUDA, SparseHIP, SparseDPCPP, MSNPU, XLA, QuantizedCPU, Undefined, MkldnnCPU, NumOptions };
 
 static inline Backend toSparse(Backend b) {
   switch (b) {
     case Backend::CPU:
       return Backend::SparseCPU;
+    case Backend::DPCPP:
+      return Backend::SparseDPCPP;
     case Backend::CUDA:
       return Backend::SparseCUDA;
     case Backend::HIP:
@@ -58,6 +60,10 @@ static inline Backend toDense(Backend b) {
       return Backend::MSNPU;
     case Backend::XLA:
       return Backend::XLA;
+    case Backend::DPCPP:
+      return Backend::DPCPP;
+    case Backend::SparseDPCPP:
+      return Backend::DPCPP;
     case Backend::SparseCPU:
       return Backend::CPU;
     case Backend::SparseCUDA:
@@ -82,6 +88,10 @@ static inline Backend dispatchKeyToBackend(DispatchKey t) {
     return Backend::MSNPU;
   } else if (t == DispatchKey::XLATensorId || t == DispatchKey::XLAPreAutograd) {
     return Backend::XLA;
+  } else if (t == DispatchKey::DPCPPTensorId) {
+    return Backend::DPCPP;
+  } else if (t == DispatchKey::SparseDPCPPTensorId) {
+    return Backend::SparseDPCPP;
   } else if (t == DispatchKey::SparseCPUTensorId) {
     return Backend::SparseCPU;
   } else if (t == DispatchKey::SparseCUDATensorId) {
@@ -111,6 +121,10 @@ static inline DispatchKey backendToDispatchKey(Backend b) {
       return DispatchKey::MSNPUTensorId;
     case Backend::XLA:
       return DispatchKey::XLATensorId;
+    case Backend::DPCPP:
+      return DispatchKey::DPCPPTensorId;
+    case Backend::SparseDPCPP:
+      return DispatchKey::SparseDPCPPTensorId;
     case Backend::SparseCPU:
       return DispatchKey::SparseCPUTensorId;
     case Backend::SparseCUDA:
@@ -146,6 +160,9 @@ static inline DeviceType backendToDeviceType(Backend b) {
       return DeviceType::CUDA;
     case Backend::SparseHIP:
       return DeviceType::HIP;
+    case Backend::SparseDPCPP:
+    case Backend::DPCPP:
+      return DeviceType::DPCPP;
     case Backend::MkldnnCPU:
     case Backend::QuantizedCPU:
       return DeviceType::CPU;
@@ -158,12 +175,14 @@ static inline DeviceType backendToDeviceType(Backend b) {
 
 static inline Backend backendToCPU(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
     case Backend::CPU:
       return Backend::CPU;
     case Backend::CUDA:
       return Backend::CPU;
     case Backend::HIP:
       return Backend::CPU;
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
       return Backend::SparseCPU;
     case Backend::SparseCUDA:
@@ -186,12 +205,14 @@ static inline Backend backendToCPU(Backend b) {
 
 static inline Backend backendToCUDA(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
     case Backend::CPU:
     case Backend::CUDA:
     case Backend::HIP:
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::CUDA;
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
@@ -205,12 +226,14 @@ static inline Backend backendToCUDA(Backend b) {
 
 static inline Backend backendToHIP(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
     case Backend::CPU:
     case Backend::CUDA:
     case Backend::HIP:
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::HIP;
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
@@ -225,6 +248,8 @@ static inline Backend backendToHIP(Backend b) {
 // TODO: This probably shouldn't actually be static inline
 static inline const char* toString(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
+      return "DPCPP";
     case Backend::CPU:
       return "CPU";
     case Backend::CUDA:
@@ -235,6 +260,8 @@ static inline const char* toString(Backend b) {
       return "MSNPU";
     case Backend::XLA:
       return "XLA";
+    case Backend::SparseDPCPP:
+      return "SparseDPCPP";
     case Backend::SparseCPU:
       return "SparseCPU";
     case Backend::SparseCUDA:
@@ -252,6 +279,7 @@ static inline const char* toString(Backend b) {
 
 static inline bool isSparse(Backend b) {
   switch (b) {
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
diff --git a/c10/core/Device.cpp b/c10/core/Device.cpp
index 82a02fdf04..82680381ed 100644
--- a/c10/core/Device.cpp
+++ b/c10/core/Device.cpp
@@ -13,7 +13,7 @@
 namespace c10 {
 namespace {
 DeviceType parse_type(const std::string& device_string) {
-  static const std::array<std::pair<std::string, DeviceType>, 9> types = {{
+  static const std::array<std::pair<std::string, DeviceType>, 10> types = {{
       {"cpu", DeviceType::CPU},
       {"cuda", DeviceType::CUDA},
       {"mkldnn", DeviceType::MKLDNN},
@@ -23,6 +23,7 @@ DeviceType parse_type(const std::string& device_string) {
       {"hip", DeviceType::HIP},
       {"msnpu", DeviceType::MSNPU},
       {"xla", DeviceType::XLA},
+      {"dpcpp", DeviceType::DPCPP},
   }};
   auto device = std::find_if(
       types.begin(),
@@ -34,7 +35,7 @@ DeviceType parse_type(const std::string& device_string) {
     return device->second;
   }
   AT_ERROR(
-      "Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu device type at start of device string: ", device_string);
+      "Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, dpcpp, hip, msnpu device type at start of device string: ", device_string);
 }
 } // namespace
 
diff --git a/c10/core/DeviceType.cpp b/c10/core/DeviceType.cpp
index 017267cd97..486b02e1ec 100644
--- a/c10/core/DeviceType.cpp
+++ b/c10/core/DeviceType.cpp
@@ -27,6 +27,8 @@ std::string DeviceTypeName(DeviceType d, bool lower_case) {
       return lower_case ? "msnpu" : "MSNPU";
     case DeviceType::XLA:
       return lower_case ? "xla" : "XLA";
+    case DeviceType::DPCPP:
+      return lower_case ? "dpcpp" : "DPCPP";
     default:
       AT_ERROR(
           "Unknown device: ",
@@ -59,6 +61,7 @@ bool isValidDeviceType(DeviceType d) {
     case DeviceType::FPGA:
     case DeviceType::MSNPU:
     case DeviceType::XLA:
+    case DeviceType::DPCPP:
       return true;
     default:
       return false;
diff --git a/c10/core/DeviceType.h b/c10/core/DeviceType.h
index 9f759666d7..23f68080d4 100644
--- a/c10/core/DeviceType.h
+++ b/c10/core/DeviceType.h
@@ -23,11 +23,12 @@ enum class DeviceType : int16_t {
   FPGA = 7, // FPGA
   MSNPU = 8, // MSNPU
   XLA = 9, // XLA / TPU
+  DPCPP = 10, // DPCPP
   // NB: If you add more devices:
   //  - Change the implementations of DeviceTypeName and isValidDeviceType
   //    in DeviceType.cpp
   //  - Change the number below
-  COMPILE_TIME_MAX_DEVICE_TYPES = 10,
+  COMPILE_TIME_MAX_DEVICE_TYPES = 11,
   ONLY_FOR_TEST = 20901, // This device type is only for test.
 };
 
@@ -36,6 +37,7 @@ constexpr DeviceType kCUDA = DeviceType::CUDA;
 constexpr DeviceType kHIP = DeviceType::HIP;
 constexpr DeviceType kMSNPU = DeviceType::MSNPU;
 constexpr DeviceType kXLA = DeviceType::XLA;
+constexpr DeviceType kDPCPP = DeviceType::DPCPP;
 
 // define explicit int constant
 constexpr int COMPILE_TIME_MAX_DEVICE_TYPES =
diff --git a/c10/core/DispatchKey.cpp b/c10/core/DispatchKey.cpp
index cf20e515c2..c3863e54e9 100644
--- a/c10/core/DispatchKey.cpp
+++ b/c10/core/DispatchKey.cpp
@@ -26,10 +26,14 @@ const char* toString(DispatchKey t) {
       return "HIPTensorId";
     case DispatchKey::SparseHIPTensorId:
       return "SparseHIPTensorId";
+    case DispatchKey::SparseDPCPPTensorId:
+      return "SparseDPCPPTensorId";
     case DispatchKey::MSNPUTensorId:
       return "MSNPUTensorId";
     case DispatchKey::XLATensorId:
       return "XLATensorId";
+    case DispatchKey::DPCPPTensorId:
+      return "DPCPPTensorId";
     case DispatchKey::MkldnnCPUTensorId:
       return "MkldnnCPUTensorId";
     case DispatchKey::QuantizedCPUTensorId:
diff --git a/c10/core/DispatchKey.h b/c10/core/DispatchKey.h
index da7c3c564e..0c34c70be4 100644
--- a/c10/core/DispatchKey.h
+++ b/c10/core/DispatchKey.h
@@ -51,6 +51,7 @@ enum class DispatchKey : uint8_t {
   HIPTensorId,    // NB: I think this is not actually used, due to Note [Masquerading as CUDA]
   MSNPUTensorId,  // unused externally, but tested at test/cpp_extensions/msnpu_extension.cpp
   XLATensorId,    // lives out of tree at https://github.com/pytorch/xla
+  DPCPPTensorId,
 
   // These are Caffe2 device types which we grandfathered into
   // DispatchKey.
@@ -89,6 +90,7 @@ enum class DispatchKey : uint8_t {
   SparseCPUTensorId,  // registered at build/aten/src/ATen/SparseCPUType.cpp
   SparseCUDATensorId, // registered at build/aten/src/ATen/SparseCUDAType.cpp
   SparseHIPTensorId,  // TODO: I think this is not actually used, due to Note [Masquerading as CUDA]
+  SparseDPCPPTensorId,
 
   // Here are reserved backends for user-defined backends, see Note [Private use TensorId]
   // To see some example about how to use this, check out MSNPU
diff --git a/c10/core/TensorOptions.h b/c10/core/TensorOptions.h
index 9a4c9b3eb9..8574aafe19 100644
--- a/c10/core/TensorOptions.h
+++ b/c10/core/TensorOptions.h
@@ -398,6 +398,8 @@ struct C10_API TensorOptions {
             return DispatchKey::MSNPUTensorId;
           case DeviceType::XLA:
             return DispatchKey::XLATensorId;
+          case DeviceType::DPCPP:
+            return DispatchKey::DPCPPTensorId;
           default:
             AT_ERROR("Unsupported device type for dense layout: ", device().type());
         }
@@ -409,6 +411,8 @@ struct C10_API TensorOptions {
             return DispatchKey::SparseCUDATensorId;
           case DeviceType::HIP:
             return DispatchKey::SparseHIPTensorId;
+          case DeviceType::DPCPP:
+            return DispatchKey::DPCPPTensorId;
           default:
             AT_ERROR("Unsupported device type for sparse layout: ", device().type());
         }
@@ -634,12 +638,16 @@ inline DeviceType computeDeviceType(DispatchKey tid) {
     return DeviceType::MSNPU;
   } else if (tid == DispatchKey::XLATensorId) {
     return DeviceType::XLA;
+  } else if (tid == DispatchKey::DPCPPTensorId) {
+    return DeviceType::DPCPP;
   } else if (tid == DispatchKey::SparseCPUTensorId) {
     return DeviceType::CPU;
   } else if (tid == DispatchKey::SparseCUDATensorId) {
     return DeviceType::CUDA;
   } else if (tid == DispatchKey::SparseHIPTensorId) {
     return DeviceType::HIP;
+  } else if (tid == DispatchKey::SparseDPCPPTensorId) {
+    return DeviceType::DPCPP;
   } else if (tid == DispatchKey::MkldnnCPUTensorId) {
     return DeviceType::CPU;
   } else {
diff --git a/torch/autograd/profiler.py b/torch/autograd/profiler.py
index 718b7c5522..ae4451f0cc 100644
--- a/torch/autograd/profiler.py
+++ b/torch/autograd/profiler.py
@@ -526,8 +526,10 @@ class FormattedTimesMixin(object):
     """
     cpu_time_str = attr_formatter('cpu_time')
     cuda_time_str = attr_formatter('cuda_time')
+    dpcpp_time_str = attr_formatter('dpcpp_time')
     cpu_time_total_str = attr_formatter('cpu_time_total')
     cuda_time_total_str = attr_formatter('cuda_time_total')
+    dpcpp_time_total_str = attr_formatter('dpcpp_time_total')
     self_cpu_time_total_str = attr_formatter('self_cpu_time_total')
 
     @property
@@ -538,6 +540,11 @@ class FormattedTimesMixin(object):
     def cuda_time(self):
         return 0.0 if self.count == 0 else 1.0 * self.cuda_time_total / self.count
 
+    @property
+    def dpcpp_time(self):
+        return 0.0 if self.count == 0 else 1.0 * self.dpcpp_time_total / self.count
+
+
 
 class Interval(object):
     def __init__(self, start, end):
@@ -557,16 +564,26 @@ class FunctionEvent(FormattedTimesMixin):
     def __init__(self, id, name, thread, cpu_start, cpu_end, input_shapes=None):
         self.id = id
         self.name = name
-        self.cpu_interval = Interval(cpu_start, cpu_end)
+        # self.cpu_interval = Interval(cpu_start, cpu_end)
+        self.cpu_start = cpu_start
+        self.cpu_end = cpu_end
         self.thread = thread
         self.kernels = []
+        self.dpcpp_kernels = []
         self.count = 1
         self.cpu_children = []
         self.input_shapes = input_shapes
 
+    @property
+    def cpu_interval(self):
+        return Interval(self.cpu_start, self.cpu_end)
+
     def append_kernel(self, name, device, start, end):
         self.kernels.append(Kernel(name, device, Interval(start, end)))
 
+    def append_dpcpp_kernel(self, name, device, duration):
+        self.dpcpp_kernels.append(Kernel(name, device, Interval(0, duration)))
+
     def append_cpu_child(self, child):
         """Append a CPU child of type FunctionEvent.
 
@@ -590,6 +607,10 @@ class FunctionEvent(FormattedTimesMixin):
     def cpu_time_total(self):
         return self.cpu_interval.elapsed_us()
 
+    @property
+    def dpcpp_time_total(self):
+        return sum(kinfo.interval.elapsed_us() for kinfo in self.dpcpp_kernels)
+
     @property
     def key(self):
         return self.name
@@ -597,13 +618,14 @@ class FunctionEvent(FormattedTimesMixin):
     def __repr__(self):
         return (
             '<FunctionEvent id={} cpu_time={} cpu_start={} cpu_end={} '
-            'cpu_children={} cuda_time={} name={} thread={} input_shapes={}>'.format(
+            'cpu_children={} cuda_time={} dpcpp_time={} name={} thread={} input_shapes={}>'.format(
                 self.id,
                 self.cpu_time_str,
                 self.cpu_interval.start,
                 self.cpu_interval.end,
                 str([child.id for child in self.cpu_children]),
                 self.cuda_time_str,
+                self.dpcpp_time_str,
                 self.name,
                 self.thread,
                 str(self.input_shapes),
@@ -618,6 +640,7 @@ class FunctionEventAvg(FormattedTimesMixin):
         self.count = 0
         self.cpu_time_total = 0
         self.cuda_time_total = 0
+        self.dpcpp_time_total = 0
         self.self_cpu_time_total = 0
         self.input_shapes = None
 
@@ -635,6 +658,7 @@ class FunctionEventAvg(FormattedTimesMixin):
         assert other.key == self.key
         self.cpu_time_total += other.cpu_time_total
         self.cuda_time_total += other.cuda_time_total
+        self.dpcpp_time_total += other.dpcpp_time
         self.self_cpu_time_total += other.self_cpu_time_total
         self.count += other.count
         return self
@@ -645,11 +669,12 @@ class FunctionEventAvg(FormattedTimesMixin):
     def __repr__(self):
         return (
             '<FunctionEventAvg key={} self_cpu_time={} cpu_time={} '
-            'cuda_time={} input_shapes={}>'.format(
+            'cuda_time={} dpcpp_time={} input_shapes={}>'.format(
                 self.key,
                 self.self_cpu_time_total_str,
                 self.cpu_time_str,
                 self.cuda_time_str,
+                self.dpcpp_time_str,
                 str(self.input_shapes),
             )
         )
@@ -672,6 +697,7 @@ def parse_cpu_trace(thread_records):
     start_record = None
     cuda_records = {}
     functions = []
+    function_stack = []
     record_stack = []
     string_table = StringTable()
 
@@ -696,19 +722,51 @@ def parse_cpu_trace(thread_records):
 
     for record in itertools.chain(*thread_records):
         if record.kind() == 'mark':
+            if record.has_dpcpp():
+                if len(function_stack) > 0:
+                    function_frame = function_stack[-1]
+                    function_frame.append_dpcpp_kernel(record.name,
+                                                      record.device,
+                                                      record.dpcpp_elapsed_us())
+                else:
+                    # a dpcpp event is submitted but no parent function was recorded.
+                    fe = FunctionEvent(
+                            id=next_id,
+                            name="unknow parent dpcpp kernel",
+                            thread=record.thread_id(),
+                            cpu_start=start_record.cpu_elapsed_us(record),
+                            cpu_end=start_record.cpu_elapsed_us(record),
+                            input_shapes=record.shapes())
+                    fe.append_dpcpp_kernel(record.name,
+                                          record.device,
+                                          record.dpcpp_elapsed_us())
+                    functions.append(fe)
+                    next_id += 1
             continue
         elif record.kind() == 'push':
-            record_stack.append((next_id, record))
+            # record_stack.append((next_id, record))
+            fe = FunctionEvent(
+                id=next_id,
+                name=string_table[record.name()],
+                thread=record.thread_id(),
+                cpu_start=start_record.cpu_elapsed_us(record),
+                cpu_end=0,
+                input_shapes=record.shapes())
+            function_stack.append(fe)
+            record_stack.append(record)
             next_id += 1
         elif record.kind() == 'pop':
-            function_id, start = record_stack.pop()
-            fe = FunctionEvent(
-                id=function_id,
-                name=string_table[start.name()],
-                thread=start.thread_id(),
-                cpu_start=start_record.cpu_elapsed_us(start),
-                cpu_end=start_record.cpu_elapsed_us(record),
-                input_shapes=start.shapes())
+            # function_id, start = record_stack.pop()
+            # fe = FunctionEvent(
+            #     id=function_id,
+            #     name=string_table[start.name()],
+            #     thread=start.thread_id(),
+            #     cpu_start=start_record.cpu_elapsed_us(start),
+            #     cpu_end=start_record.cpu_elapsed_us(record),
+            #     input_shapes=start.shapes())
+            start = record_stack.pop()
+            fe = function_stack.pop()
+            fe.cpu_end = start_record.cpu_elapsed_us(record)
             if start.has_cuda():
                 cuda_start = adjusted_time(start)
                 cuda_end = adjusted_time(record)
@@ -825,6 +883,9 @@ def build_table(events, sort_by=None, header=None, row_limit=100, use_cuda=True)
         'CPU total %',
         'CPU total',
         'CPU time avg',
+        'DPCPP total %',
+        'DPCPP total',
+        'DPCPP time avg',
     ]
     if use_cuda:
         headers.extend([
@@ -869,6 +930,7 @@ def build_table(events, sort_by=None, header=None, row_limit=100, use_cuda=True)
 
     self_cpu_time_total = sum([event.self_cpu_time_total for event in events])
     cuda_time_total = sum([evt.cuda_time_total for evt in events])
+    dpcpp_time_total = sum([evt.dpcpp_time_total for evt in events])
     # Actual printing
     if header is not None:
         append('=' * line_length)
@@ -888,6 +950,10 @@ def build_table(events, sort_by=None, header=None, row_limit=100, use_cuda=True)
             format_time_share(evt.cpu_time_total, self_cpu_time_total),
             evt.cpu_time_total_str,  # CPU total
             evt.cpu_time_str,  # CPU time avg
+            # DPCPP time total %
+            format_time_share(evt.dpcpp_time_total, dpcpp_time_total),
+            evt.dpcpp_time_total_str,
+            evt.dpcpp_time_str,  # Sycl time avg
         ]
         if use_cuda:
             row_values.extend([
@@ -905,6 +971,7 @@ def build_table(events, sort_by=None, header=None, row_limit=100, use_cuda=True)
 
     append(header_sep)
     append("Self CPU time total: {}".format(format_time(self_cpu_time_total)))
+    append("DPCPP time total: {}".format(format_time(dpcpp_time_total)))
     if use_cuda:
         append("CUDA time total: {}".format(format_time(cuda_time_total)))
     return ''.join(result)
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index 96c32632b6..be895b9dbe 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -33,6 +33,7 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
       .value("Disabled", ProfilerState::Disabled)
       .value("CPU", ProfilerState::CPU)
       .value("CUDA", ProfilerState::CUDA)
+      .value("DPCPP", ProfilerState::DPCPP)
       .value("NVTX", ProfilerState::NVTX);
 
   py::class_<ProfilerConfig>(m, "ProfilerConfig")
@@ -45,7 +46,9 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
       .def("device", &Event::device)
       .def("cpu_elapsed_us", &Event::cpu_elapsed_us)
       .def("cuda_elapsed_us", &Event::cuda_elapsed_us)
+      .def("dpcpp_elapsed_us", &Event::dpcpp_elapsed_us)
       .def("has_cuda", &Event::has_cuda)
+      .def("has_dpcpp", &Event::has_dpcpp)
       .def("shapes", &Event::shapes);
 
   m.def("_enable_profiler", enableProfiler);
diff --git a/torch/csrc/autograd/profiler.cpp b/torch/csrc/autograd/profiler.cpp
index ee6f9b82db..d9c8b6e1fa 100644
--- a/torch/csrc/autograd/profiler.cpp
+++ b/torch/csrc/autograd/profiler.cpp
@@ -8,15 +8,19 @@
 #include <string>
 #include <vector>
 
+
 namespace torch { namespace autograd { namespace profiler {
 
 namespace {
 
 CUDAStubs default_stubs;
+DPCPPStubs default_dpcpp_stubs;
 constexpr CUDAStubs* default_stubs_addr = &default_stubs;
+constexpr DPCPPStubs* default_dpcpp_stubs_addr = &default_dpcpp_stubs;
 // constant initialization, so it is guaranteed to be initialized before
 // static initialization calls which may invoke registerCUDAMethods
 static CUDAStubs* cuda_stubs = default_stubs_addr;
+static DPCPPStubs* dpcpp_stubs = default_dpcpp_stubs_addr;
 
 ProfilerState state = ProfilerState::Disabled;
 // Protects access all_event_lists_map.
@@ -32,6 +36,10 @@ void registerCUDAMethods(CUDAStubs* stubs) {
   cuda_stubs = stubs;
 }
 
+void registerDPCPPMethods(DPCPPStubs* stubs) {
+  dpcpp_stubs = stubs;
+}
+
 ProfilerConfig::~ProfilerConfig() = default;
 
 RangeEventList& getEventList() {
@@ -44,7 +52,7 @@ RangeEventList& getEventList() {
   return *event_list;
 }
 
-void mark(std::string name, bool include_cuda /* = true */) {
+void mark(std::string name, bool not_cpu /* = true */) {
   if (state == ProfilerState::Disabled) {
     return;
   }
@@ -55,8 +63,21 @@ void mark(std::string name, bool include_cuda /* = true */) {
         EventKind::Mark,
         StringView(std::move(name)),
         thread_id,
-        include_cuda && state == ProfilerState::CUDA);
+        // include_cuda && state == ProfilerState::CUDA);
+        not_cpu ? (ProfilerState :: CPU) : state);
+  }
+}
+
+void mark_dpcpp(std::string name, DPCPPEventStub dpcpp_event) {
+  if (state == ProfilerState::Disabled) {
+    return;
   }
+  getEventList().record(
+          EventKind::Mark,
+          StringView(std::move(name)),
+          thread_id,
+          state,
+          std::move(dpcpp_event));
 }
 
 bool profilerEnabled() {
@@ -104,7 +125,8 @@ void pushRangeImpl(
         EventKind::PushRange,
         name,
         thread_id,
-        state == ProfilerState::CUDA,
+        // state == ProfilerState::CUDA,
+        state,
         std::move(shapes));
   }
 }
@@ -124,7 +146,8 @@ void popRange() {
         EventKind::PopRange,
         StringView(""),
         thread_id,
-        state == ProfilerState::CUDA);
+        // state == ProfilerState::CUDA);
+        state);
   }
 }
 
@@ -184,7 +207,8 @@ void enableProfiler(ProfilerConfig config) {
                       EventKind::PopRange,
                       StringView(""),
                       fn.getStartCallbacksThreadId(),
-                      state == ProfilerState::CUDA);
+                      // state == ProfilerState::CUDA);
+                      state);
           }
         } else {
           popRange();
@@ -244,12 +268,17 @@ thread_event_lists disableProfiler() {
   }
 }
 
-void Event::record(bool record_cuda) {
-  if (record_cuda) {
-    cuda_stubs->record(&device_, &event, &cpu_ns_);
-    return;
+void Event::record(ProfilerState state) {
+  switch(state) {
+    case ProfilerState::CUDA:
+      cuda_stubs->record(&device_, &event, &cpu_ns_);
+      break;
+    /*case ProfilerState::DPCPP:
+      break;*/
+    default:
+      cpu_ns_ = getTime();
+      break;
   }
-  cpu_ns_ = getTime();
 }
 
 double Event::cuda_elapsed_us(const Event & e) {
@@ -262,8 +291,16 @@ double Event::cuda_elapsed_us(const Event & e) {
   return cuda_stubs->elapsed(event, e.event);
 }
 
+double Event::dpcpp_elapsed_us() {
+  if(!has_dpcpp()) {
+    throw std::logic_error("Events were not recorded for DPCPP");
+  }
+  return dpcpp_stubs->elapsed(dpcpp_event);
+}
+
 CUDAStubs::~CUDAStubs() = default;
 
+DPCPPStubs::~DPCPPStubs() = default;
 
 static jit::CodeTemplate event_template(R"(
 {
diff --git a/torch/csrc/autograd/profiler.h b/torch/csrc/autograd/profiler.h
index 01e5c38c98..a080bee7df 100644
--- a/torch/csrc/autograd/profiler.h
+++ b/torch/csrc/autograd/profiler.h
@@ -17,8 +17,17 @@
 
 #include <torch/csrc/autograd/record_function.h>
 
+
 typedef struct CUevent_st* CUDAEventStub;
 
+struct DPCPPEventStubBase {
+  virtual float elapsed() {
+    TORCH_CHECK(0, "no DPCPP instance ...");
+  }
+  virtual ~DPCPPEventStubBase() = default;
+} ;
+typedef std::shared_ptr<DPCPPEventStubBase> DPCPPEventStub;
+
 namespace torch { namespace autograd {
 
 struct Node;
@@ -61,6 +70,24 @@ private:
 
 TORCH_API void registerCUDAMethods(CUDAStubs* stubs);
 
+struct TORCH_API DPCPPStubs {
+  virtual float elapsed(DPCPPEventStub event) {
+    fail();
+    return 0.f;
+  }
+  virtual bool enabled() {
+    return false;
+  }
+  virtual ~DPCPPStubs();
+
+private:
+  void fail() {
+    AT_ERROR("DPCPP used in profiler but not enabled.");
+  }
+};
+
+TORCH_API void registerDPCPPMethods(DPCPPStubs* stubs);
+
 constexpr inline size_t ceilToMultiple(size_t a, size_t b) {
   return ((a + b - 1) / b) * b;
 }
@@ -101,6 +128,7 @@ enum class TORCH_API ProfilerState {
     Disabled,
     CPU, // CPU-only profiling
     CUDA, // CPU + CUDA events
+    DPCPP, // CPU + DPCPP events
     NVTX,  // only emit NVTX markers
 };
 
@@ -126,16 +154,31 @@ struct TORCH_API Event final {
       EventKind kind,
       StringView name,
       uint16_t thread_id,
-      bool record_cuda,
+      ProfilerState state,
       std::vector<std::vector<int64_t>>&& shapes = {})
       : name_(std::move(name)),
         kind_(kind),
         thread_id_(thread_id),
+        dpcpp_kernel(false),
         shapes_(shapes) {
-    record(record_cuda);
+    record(state);
   }
 
-  void record(bool record_cuda);
+  Event(
+          EventKind kind,
+          StringView name,
+          uint16_t thread_id,
+          ProfilerState state,
+          DPCPPEventStub event)
+          : name_(std::move(name)),
+            kind_(kind),
+            thread_id_(thread_id),
+            dpcpp_kernel(true),
+            dpcpp_event(event) {
+    record(state);
+  }
+
+  void record(ProfilerState state);
   std::string kind() const {
     switch(kind_) {
       case EventKind::Mark: return "mark";
@@ -157,9 +200,13 @@ struct TORCH_API Event final {
     return (e.cpu_ns_ - cpu_ns_)/(1000.0);
   }
   double cuda_elapsed_us(const Event & e);
+  double dpcpp_elapsed_us();
   bool has_cuda() const {
     return event != nullptr;
   }
+  bool has_dpcpp() const {
+    return dpcpp_kernel;
+  }
   int device() const {
     return device_;
   }
@@ -169,9 +216,11 @@ private:
   StringView name_;
   EventKind kind_;
   uint16_t thread_id_;
+  bool dpcpp_kernel;
   std::vector<std::vector<int64_t>> shapes_;
   int device_ = -1;
   struct CUevent_st* event = nullptr;
+  DPCPPEventStub dpcpp_event;
 };
 
 // a linked-list of fixed sized vectors, to avoid
@@ -228,9 +277,10 @@ struct RangeEventList {
 };
 
 TORCH_API RangeEventList& getEventList();
-TORCH_API void mark(std::string name, bool include_cuda = true);
+TORCH_API void mark(std::string name, bool not_cpu = true);
 TORCH_API void pushRange(std::string name);
 TORCH_API void popRange();
+TORCH_API void mark_dpcpp(std::string name, DPCPPEventStub dpcpp_event);
 
 using thread_event_lists = std::vector<std::vector<Event>>;
 // NOTE: changing profiler modes is **NOT THREAD SAFE**. You should ensure that
diff --git a/torch/csrc/utils/tensor_layouts.cpp b/torch/csrc/utils/tensor_layouts.cpp
index 6fcd84f3db..e42a7baf27 100644
--- a/torch/csrc/utils/tensor_layouts.cpp
+++ b/torch/csrc/utils/tensor_layouts.cpp
@@ -23,6 +23,7 @@ void initializeLayouts() {
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::CUDA);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::MSNPU);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::XLA);
+  registerLayoutObject((THPLayout*)strided_layout, at::Backend::DPCPP);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::QuantizedCPU);
 
   PyObject *sparse_coo_layout = THPLayout_New(at::Layout::Sparse, "torch.sparse_coo");
diff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index 85add73c57..9807075e07 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -59,6 +59,9 @@ Backend backendToBackendOfDeviceType(Backend b, DeviceType d) {
     case DeviceType::XLA:
       TORCH_CHECK(!isSparse(b), "Sparse not implemented for XLA");
       return Backend::XLA;
+    case DeviceType::DPCPP:
+      TORCH_CHECK(!isSparse(b), "Sparse not implemented for DPCPP");
+      return Backend::DPCPP;
     default:
       AT_ERROR("Unknown device type");
   }
-- 
2.17.1

