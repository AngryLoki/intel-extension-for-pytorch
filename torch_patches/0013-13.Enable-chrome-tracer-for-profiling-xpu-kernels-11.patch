From 9a9b46dc0feea76da55e03d1c1d17084d0322453 Mon Sep 17 00:00:00 2001
From: chengjunlu <chengjun.lu@intel.com>
Date: Tue, 6 Jul 2021 16:17:51 +0800
Subject: [PATCH 13/14] 13.Enable chrome tracer for profiling xpu kernels (#11)

Co-authored-by: Xunsong, Huang <xunsong.huang@intel.com>
---
 torch/autograd/profiler.py       | 68 ++++++++++++++++++++++++++------
 torch/csrc/autograd/init.cpp     |  5 ++-
 torch/csrc/autograd/profiler.cpp | 21 +++++++---
 torch/csrc/autograd/profiler.h   | 16 +++++---
 4 files changed, 85 insertions(+), 25 deletions(-)

diff --git a/torch/autograd/profiler.py b/torch/autograd/profiler.py
index 51a5e65b62..e6a2adcc7f 100644
--- a/torch/autograd/profiler.py
+++ b/torch/autograd/profiler.py
@@ -43,7 +43,7 @@ class EventList(list):
         self._use_cuda = use_cuda
         self._use_xpu = use_xpu
         self._profile_memory = profile_memory
-        self._with_calling_stack = with_calling_stack 
+        self._with_calling_stack = with_calling_stack
 
 
     def __str__(self):
@@ -219,6 +219,7 @@ class EventList(list):
                         else f'" node_id:{evt.node_id}, thread_id:{evt.thread} "',
                     )
                 )
+                # write down cuda kernels
                 for k in evt.kernels:
                     # 's' and 'f' draw Flow arrows from
                     # the CPU launch to the GPU kernel
@@ -248,6 +249,36 @@ class EventList(list):
                             '"args": {}}, ' % (k.name, k.interval.start,
                                                k.interval.elapsed_us(), k.device))
                     next_id += 1
+                # write down XPU kernels
+                for k in evt.xpu_kernels:
+                    # 's' and 'f' draw Flow arrows from
+                    # the CPU launch to the GPU kernel
+                    f.write('{"name": "%s", '
+                            '"ph": "s", '
+                            '"ts": %s, '
+                            '"tid": %s, '
+                            '"pid": "CPU functions", '
+                            '"id": %s, '
+                            '"cat": "cpu_to_xpu", '
+                            '"args": {}}, ' % (evt.name, evt.cpu_interval.start,
+                                               evt.thread, next_id))
+                    f.write('{"name": "%s", '
+                            '"ph": "f", '
+                            '"ts": %s, '
+                            '"tid": %s, '
+                            '"pid": "XPU functions", '
+                            '"id": %s, '
+                            '"cat": "cpu_to_xpu", '
+                            '"args": {}}, ' % (k.name, k.interval.start, k.device, next_id))
+                    f.write('{"name": "%s", '
+                            '"ph": "X", '
+                            '"ts": %s, '
+                            '"dur": %s, '
+                            '"tid": %s, '
+                            '"pid": "XPU functions", '
+                            '"args": {}}, ' % (k.name, k.interval.start,
+                                               k.interval.elapsed_us(), k.device))
+                    next_id += 1
 
             # remove trailing whitespace and comma
             f.seek(f.tell() - 2, os.SEEK_SET)
@@ -390,7 +421,7 @@ class profile(object):
         self.entered = False
         self.record_shapes = record_shapes
         self.profile_memory = profile_memory
-        self.with_calling_stack = with_calling_stack 
+        self.with_calling_stack = with_calling_stack
         self.with_stack = with_stack
 
     def __enter__(self):
@@ -779,7 +810,7 @@ class FunctionEvent(FormattedTimesMixin):
         self.cpu_children: List[FunctionEvent] = []
         self.cpu_parent: Optional[FunctionEvent] = None
         self.input_shapes: Tuple[int, ...] = input_shapes
-        self.cstack: Tuple[int, ...] = cstack 
+        self.cstack: Tuple[int, ...] = cstack
         self.stack: List = stack
         self.scope: int = scope
         self.cpu_memory_usage: int = cpu_memory_usage
@@ -792,8 +823,8 @@ class FunctionEvent(FormattedTimesMixin):
     def append_kernel(self, name, device, start, end):
         self.kernels.append(Kernel(name, device, Interval(start, end)))
 
-    def append_xpu_kernel(self, name, device, duration):
-        self.xpu_kernels.append(Kernel(name, device, Interval(0, duration)))
+    def append_xpu_kernel(self, name, device, start, end):
+        self.xpu_kernels.append(Kernel(name, device, Interval(start, end)))
 
     def append_cpu_child(self, child):
         """Append a CPU child of type FunctionEvent.
@@ -1007,6 +1038,7 @@ def parse_event_records(thread_records):
 
     next_id = 0
     start_record = None
+    xpu_start_record = None
     cuda_records = {}
     functions = []
     function_stack = []
@@ -1044,6 +1076,11 @@ def parse_event_records(thread_records):
         cuda_time_0 = cuda_records_map[(cuda_record.node_id(), cuda_record.device())]
         return cuda_time_0.cuda_elapsed_us(cuda_record) + start_record.cpu_elapsed_us(cuda_time_0)
 
+    def xpu_adjusted_time(xpu_record, xpu_start_record, start_record):
+        xpu_kernel_offset = xpu_start_record.xpu_elapsed_us(xpu_record)
+        cpu_start_offset = start_record.cpu_elapsed_us(xpu_start_record)
+        return xpu_kernel_offset + cpu_start_offset# + 200
+
     # '__start_profile' is not guaranteed to be first, so we must find it here
     for record in itertools.chain(*thread_records):
         name = record.name()
@@ -1055,6 +1092,8 @@ def parse_event_records(thread_records):
             # key for cuda_records is (node_id, device) in case of multiple nodes
             # having the same device
             cuda_records[(record.node_id(), record.device())] = record
+        elif '__xpu_start_event' in name:
+            xpu_start_record = record
 
     assert start_record is not None and not start_record.is_remote()
 
@@ -1159,12 +1198,16 @@ def parse_event_records(thread_records):
                 for handle in xpu_memory_allocs.keys():
                     xpu_memory_allocs[handle] += record.xpu_memory_usage()
             elif record.kind() == 'mark':
+                if '__xpu_start_event' in record.name():
+                    continue
                 if record.has_xpu():
+                    xpu_start = xpu_adjusted_time(record, xpu_start_record, start_record)
                     if len(function_stack) > 0:
                         fe = function_stack[-1]
-                        fe.append_xpu_kernel(record.name,
-                                               record.device,
-                                               record.xpu_elapsed_us())
+                        fe.append_xpu_kernel(fe.name+"("+record.name()+")",
+                                               record.device(),
+                                               xpu_start,
+                                               xpu_start+record.xpu_kernel_us())
                     else:
                         # a xpu event is submitted but no parent function was recorded.
                         fe = FunctionEvent(
@@ -1174,9 +1217,10 @@ def parse_event_records(thread_records):
                             thread=record.thread_id(),
                             input_shapes=record.shapes(),
                             cstack = tuple(calling_stack))
-                        fe.append_xpu_kernel(record.name,
-                                               record.device,
-                                               record.xpu_elapsed_us())
+                        fe.append_xpu_kernel(fe.name+"("+record.name()+")",
+                                               record.device(),
+                                               xpu_start,
+                                               xpu_start+record.xpu_kernel_us())
                         functions.append(fe)
             prev_record = record
 
@@ -1403,7 +1447,7 @@ def build_table(
 
     self_cpu_time_total = sum([event.self_cpu_time_total for event in events])
     cuda_time_total = sum([evt.self_cuda_time_total for evt in events])
-    xpu_time_total = sum([evt.xpu_time_total for evt in events])
+    xpu_time_total = sum([evt.self_xpu_time_total for evt in events])
     # Actual printing
     if header is not None:
         append('=' * line_length)
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index 905d349d0f..5dd059ba8e 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -54,9 +54,10 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
       .def("device", &Event::device)
       .def("cpu_elapsed_us", &Event::cpuElapsedUs)
       .def("cuda_elapsed_us", &Event::cudaElapsedUs)
-      .def("xpu_elapsed_us", &Event::xpu_elapsed_us)
+      .def("xpu_elapsed_us", &Event::xpuElapsedUs)
+      .def("xpu_kernel_us", &Event::xpuKernelTimeUs)
       .def("has_cuda", &Event::hasCuda)
-      .def("has_xpu", &Event::has_xpu)
+      .def("has_xpu", &Event::hasXpu)
       .def("shapes", &Event::shapes)
       .def("cpu_memory_usage", &Event::cpuMemoryUsage)
       .def("cuda_memory_usage", &Event::cudaMemoryUsage)
diff --git a/torch/csrc/autograd/profiler.cpp b/torch/csrc/autograd/profiler.cpp
index 894040d275..66bd317f94 100644
--- a/torch/csrc/autograd/profiler.cpp
+++ b/torch/csrc/autograd/profiler.cpp
@@ -573,6 +573,8 @@ void enableProfiler(const ProfilerConfig& new_config) {
     cuda_stubs->onEachDevice([state](int d) {
         state->mark("__cuda_start_event");
     });
+  } else if (new_config.state == ProfilerState::XPU) {
+    state->mark("__xpu_start_event");
   }
   state->mark("__start_profile", false);
 }
@@ -621,8 +623,12 @@ void Event::record(ProfilerState state) {
     case ProfilerState::CUDA:
       cuda_stubs->record(&device_, &cuda_event, &cpu_ns_);
       break;
-      /*case ProfilerState::XPU:
-        break;*/
+    case ProfilerState::XPU:
+      if (name_ == at::StringView("__xpu_start_event")) {
+        xpu_stubs->record(xpu_event);
+      }
+      cpu_ns_ = getTime();
+      break;
     default:
       cpu_ns_ = getTime();
       break;
@@ -730,10 +736,13 @@ double Event::cudaElapsedUs(const Event& e) const {
   return cuda_stubs->elapsed(&cuda_event, &e.cuda_event);
 }
 
-double Event::xpu_elapsed_us() {
-  if(!has_xpu()) {
-    throw std::logic_error("Events were not recorded for XPU");
-  }
+double Event::xpuElapsedUs(const Event& e) const {
+  TORCH_CHECK(hasXpu() && e.hasXpu(), "Events were not recorded for XPU");
+  return xpu_stubs->elapsed(xpu_event, e.xpu_event);
+}
+
+double Event::xpuKernelTimeUs() const {
+  TORCH_CHECK(hasXpu(), "Events were not recorded for XPU");
   return xpu_stubs->elapsed(xpu_event);
 }
 
diff --git a/torch/csrc/autograd/profiler.h b/torch/csrc/autograd/profiler.h
index 8b0c50e74e..8227cae47c 100644
--- a/torch/csrc/autograd/profiler.h
+++ b/torch/csrc/autograd/profiler.h
@@ -24,9 +24,6 @@ struct CUevent_st;
 typedef std::shared_ptr<CUevent_st> CUDAEventStub;
 
 struct XPUEventStubBase {
-  virtual float elapsed() {
-    TORCH_CHECK(0, "no XPU instance ...");
-  }
   virtual ~XPUEventStubBase() = default;
 } ;
 typedef std::shared_ptr<XPUEventStubBase> XPUEventStub;
@@ -74,6 +71,13 @@ private:
 TORCH_API void registerCUDAMethods(CUDAStubs* stubs);
 
 struct TORCH_API XPUStubs {
+  virtual void record(XPUEventStub& event) {
+    fail();
+  }
+  virtual float elapsed(XPUEventStub event, XPUEventStub event2) {
+    fail();
+    return 0.f;
+  }
   virtual float elapsed(XPUEventStub event) {
     fail();
     return 0.f;
@@ -291,13 +295,15 @@ struct TORCH_API Event final {
 
   double cudaElapsedUs(const Event& e) const;
 
-  double xpu_elapsed_us();
+  double xpuElapsedUs(const Event& e) const;
+
+  double xpuKernelTimeUs() const;
 
   bool hasCuda() const {
     return cuda_event != nullptr || (isRemote() && device_ != -1);
   }
 
-  bool has_xpu() const {
+  bool hasXpu() const {
     return (xpu_event != nullptr);
   }
 
-- 
2.25.1

