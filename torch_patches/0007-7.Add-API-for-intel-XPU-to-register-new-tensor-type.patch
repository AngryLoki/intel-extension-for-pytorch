From 57d50293d07973a51cc333b5c5d990865995c422 Mon Sep 17 00:00:00 2001
From: johnlu <chengjun.lu@intel.com>
Date: Thu, 4 Mar 2021 20:59:55 +0800
Subject: [PATCH 7/8] 7.Add API for intel XPU to register new tensor type.

---
 setup.py                            |   1 +
 torch/csrc/tensor/python_tensor.cpp | 124 ++++++++++++++++++----------
 torch/csrc/tensor/python_tensor.h   |   4 +
 torch/csrc/utils/tensor_types.cpp   |   2 +-
 torch/csrc/utils/tensor_types.h     |   1 +
 5 files changed, 88 insertions(+), 44 deletions(-)

diff --git a/setup.py b/setup.py
index 355bce6b6a..f8e75da934 100644
--- a/setup.py
+++ b/setup.py
@@ -882,6 +882,7 @@ if __name__ == '__main__':
                 'include/torch/csrc/onnx/*.h',
                 'include/torch/csrc/utils/*.h',
                 'include/torch/csrc/generic/*.h',
+                'include/torch/csrc/tensor/*.h',
                 'include/pybind11/*.h',
                 'include/pybind11/detail/*.h',
                 'include/TH/*.h*',
diff --git a/torch/csrc/tensor/python_tensor.cpp b/torch/csrc/tensor/python_tensor.cpp
index 46fd6d595e..3e473c24e4 100644
--- a/torch/csrc/tensor/python_tensor.cpp
+++ b/torch/csrc/tensor/python_tensor.cpp
@@ -56,7 +56,7 @@ static_assert(std::is_standard_layout<PyTensorType>::value, "PyTensorType must b
 // This is always an instance of VariableType
 static PyTensorType* default_tensor_type;
 
-static void py_bind_tensor_types(const std::vector<PyTensorType>& tensor_types);
+static void py_bind_tensor_type(const PyTensorType& tensor_types);
 
 static TypeError unavailable_type(const PyTensorType& type) {
   return TypeError("type %s not available. Torch not compiled with CUDA enabled.", type.name);
@@ -178,26 +178,47 @@ static void py_initialize_tensor_type(PyTypeObject& type, const char* name, PyOb
   }
 }
 
-static const char* get_module(Backend backend) {
-  switch (backend) {
-    case Backend::CPU: return "torch";
-    case Backend::CUDA: return "torch.cuda";
-    case Backend::SparseCPU: return "torch.sparse";
-    case Backend::SparseCUDA: return "torch.cuda.sparse";
-    default: AT_ERROR("invalid backend: ", toString(backend));
-  }
-}
-
 static std::string get_name(Backend backend, ScalarType scalarType) {
   std::ostringstream ss;
-  ss << get_module(backend) << "." << toString(scalarType) << "Tensor";
+  ss << torch::utils::backend_to_string(backend) << "." << toString(scalarType) << "Tensor";
   return ss.str();
 }
 
-static THPObjectPtr get_storage_obj(PyTensorType* type) {
-  auto module_name = get_module(type->get_backend());
+static THPObjectPtr get_py_module(c10::Backend backend) {
+  auto torch_module = THPObjectPtr(PyImport_ImportModule("torch"));
+  if (!torch_module) throw python_error();
+
+  auto module_name = torch::utils::backend_to_string(backend);
   auto module_obj = THPObjectPtr(PyImport_ImportModule(module_name));
-  if (!module_obj) throw python_error();
+  if (!module_obj) {
+    std::string name(module_name);
+    auto module_idx = name.find('.');
+    if (module_idx != std::string::npos) {
+      auto module_name = name.substr(module_idx + 1);
+      module_obj = THPObjectPtr(PyObject_GetAttrString(torch_module.get(), module_name.c_str()));
+      if (!module_obj) throw python_error();
+    }
+    else {
+      throw python_error();
+    }
+  }
+  return module_obj;
+}
+
+static std::vector<std::unique_ptr<PyTensorType>> tensor_types;
+
+static PyTensorType* get_tensor_type(c10::Backend backend, c10::ScalarType scalar_type) {
+  auto it = std::find_if(tensor_types.begin(), tensor_types.end(),
+                         [backend, scalar_type](const std::unique_ptr<PyTensorType>& x) {
+                             return x->get_backend() == backend && x->get_scalar_type() == scalar_type;
+                         });
+  if (it != tensor_types.end())
+    return (*it).get();
+  return nullptr;
+}
+
+static THPObjectPtr get_storage_obj(PyTensorType* type) {
+  auto module_obj = get_py_module(type->get_backend());
 
   auto storage_name = std::string(toString(type->get_scalar_type())) + "Storage";
   THPObjectPtr storage(PyObject_GetAttrString(module_obj.get(), storage_name.c_str()));
@@ -245,8 +266,6 @@ static THPObjectPtr get_tensor_dict() {
   return res;
 }
 
-static std::vector<PyTensorType> tensor_types;
-
 void set_default_tensor_type(PyTensorType* type) {
   if (!at::isFloatingType(type->get_scalar_type())) {
     throw TypeError("only floating-point types are supported as the default type");
@@ -273,13 +292,14 @@ void set_default_tensor_type(PyTensorType* type) {
   }
 }
 
-static void initialize_aten_types(std::vector<PyTensorType>& tensor_types) {
+static void initialize_aten_types(std::vector<std::unique_ptr<PyTensorType>>& tensor_types) {
   // includes CUDA types even when PyTorch is not built with CUDA
   auto declared_types = torch::utils::all_declared_types();
   tensor_types.resize(declared_types.size());
 
   for (size_t i = 0, end = declared_types.size(); i != end; i++) {
-    auto& tensor_type = tensor_types[i];
+    tensor_types[i] = std::make_unique<PyTensorType>();
+    auto& tensor_type = *tensor_types[i];
     Backend backend = declared_types[i].first;
     ScalarType scalar_type = declared_types[i].second;
     set_type(tensor_type, backend, scalar_type);
@@ -309,46 +329,64 @@ void initialize_python_bindings() {
 
   // Initialize each Python type object torch.FloatTensor, torch.DoubleTensor, etc.
   for (auto& tensor_type : tensor_types) {
-    py_initialize_tensor_type(tensor_type.py_type, tensor_type.name, tensor_dict.get());
+    py_initialize_tensor_type(tensor_type->py_type, tensor_type->name, tensor_dict.get());
   }
 
   // Add the type objects to their corresponding modules. e.g. torch.FloatTensor
   // is added to the `torch` module as `FloatTensor`. Also add all the type
   // objects to the set torch._tensor_classes.
-  py_bind_tensor_types(tensor_types);
+  for (auto& tensor_type : tensor_types) {
+    py_bind_tensor_type(*tensor_type);
+  }
+}
+
+PyObject* register_python_tensor_type(Backend backend, ScalarType scalar_type) {
+  PyTensorType* registed_tensor_type = get_tensor_type(backend, scalar_type);
+  if (registed_tensor_type)
+    return (PyObject*)registed_tensor_type;
+
+  std::unique_ptr<PyTensorType> new_tensor_type = std::make_unique<PyTensorType>();
+
+  set_type(*new_tensor_type, backend, scalar_type);
+  set_name(*new_tensor_type, get_name(backend, scalar_type));
+
+  auto tensor_dict = get_tensor_dict();
+
+  py_initialize_tensor_type(new_tensor_type->py_type, new_tensor_type->name, tensor_dict.get());
+
+  py_bind_tensor_type(*new_tensor_type);
+
+  PyTensorType* ret = new_tensor_type.get();
+  tensor_types.push_back(std::move(new_tensor_type));
+  return (PyObject*)ret;
 }
 
-static void py_bind_tensor_types(const std::vector<PyTensorType>& tensor_types) {
+static void py_bind_tensor_type(const PyTensorType& tensor_type) {
   auto torch_module = THPObjectPtr(PyImport_ImportModule("torch"));
   if (!torch_module) throw python_error();
 
   auto tensor_classes = THPObjectPtr(PyObject_GetAttrString(torch_module.get(), "_tensor_classes"));
   if (!tensor_classes) throw python_error();
 
-  for (auto& tensor_type : tensor_types) {
-    auto name = std::string(tensor_type.name);
-    auto idx = name.rfind('.');
-    auto type_name = name.substr(idx + 1);
-    auto module_name = name.substr(0, idx);
-
-    auto module_obj = THPObjectPtr(PyImport_ImportModule(module_name.c_str()));
-    if (!module_obj) throw python_error();
+  auto name = std::string(tensor_type.name);
+  auto idx = name.rfind('.');
+  auto type_name = name.substr(idx + 1);
+  auto module_obj = get_py_module(tensor_type.get_backend());
 
-    PyObject* type_obj = (PyObject*)&tensor_type;
-    Py_INCREF(type_obj);
-    if (PyModule_AddObject(module_obj.get(), type_name.c_str(), type_obj) < 0) {
-      throw python_error();
-    }
-    if (PySet_Add(tensor_classes.get(), type_obj) < 0) {
-      throw python_error();
-    }
+  PyObject* type_obj = (PyObject*)&tensor_type;
+  Py_INCREF(type_obj);
+  if (PyModule_AddObject(module_obj.get(), type_name.c_str(), type_obj) < 0) {
+    throw python_error();
+  }
+  if (PySet_Add(tensor_classes.get(), type_obj) < 0) {
+    throw python_error();
   }
 }
 
 static bool PyTensorType_Check(PyObject* obj) {
   auto it = std::find_if(tensor_types.begin(), tensor_types.end(),
-    [obj](const PyTensorType& x) {
-      return (PyObject*)&x == obj;
+    [obj](const std::unique_ptr<PyTensorType>& x) {
+      return (PyObject*)(x.get()) == obj;
     });
   return it != tensor_types.end();
 }
@@ -371,10 +409,10 @@ void py_set_default_dtype(PyObject* obj) {
     auto scalar_type = ((THPDtype*)obj)->scalar_type;
     auto backend = default_tensor_type->get_backend();
     auto it = std::find_if(tensor_types.begin(), tensor_types.end(),
-      [backend, scalar_type](const PyTensorType& x) {
-        return x.get_backend() == backend && x.get_scalar_type() == scalar_type;
+      [backend, scalar_type](const std::unique_ptr<PyTensorType>& x) {
+        return x->get_backend() == backend && x->get_scalar_type() == scalar_type;
       });
-    set_default_tensor_type(&*it);
+    set_default_tensor_type((*it).get());
   } else {
     throw TypeError("invalid dtype object");
   }
diff --git a/torch/csrc/tensor/python_tensor.h b/torch/csrc/tensor/python_tensor.h
index acf73b9e02..148627f79f 100644
--- a/torch/csrc/tensor/python_tensor.h
+++ b/torch/csrc/tensor/python_tensor.h
@@ -3,6 +3,7 @@
 #include <torch/csrc/python_headers.h>
 #include <c10/core/ScalarType.h>
 #include <c10/core/DispatchKey.h>
+#include <c10/core/Backend.h>
 
 namespace c10 {
 struct Device;
@@ -24,6 +25,9 @@ void py_set_default_tensor_type(PyObject* type_obj);
 // Same as py_set_default_tensor_type, but only changes the dtype (ScalarType).
 void py_set_default_dtype(PyObject* dtype_obj);
 
+// Register the tensor type for specific Backend and dtype (ScalarType).
+PyObject* register_python_tensor_type(c10::Backend backend, c10::ScalarType scalar_type);
+
 // Gets the DispatchKey for the default tensor type.
 //
 // TODO: This is nuts!  There is no reason to let the default tensor type id
diff --git a/torch/csrc/utils/tensor_types.cpp b/torch/csrc/utils/tensor_types.cpp
index e29fb93c52..c1563732f2 100644
--- a/torch/csrc/utils/tensor_types.cpp
+++ b/torch/csrc/utils/tensor_types.cpp
@@ -15,7 +15,7 @@ using namespace at;
 
 namespace torch { namespace utils {
 
-static const char* backend_to_string(const at::Backend& backend) {
+const char* backend_to_string(const at::Backend& backend) {
   switch (backend) {
     case at::Backend::CPU: return "torch";
     case at::Backend::CUDA: return "torch.cuda";
diff --git a/torch/csrc/utils/tensor_types.h b/torch/csrc/utils/tensor_types.h
index 86258320b7..a65daf2003 100644
--- a/torch/csrc/utils/tensor_types.h
+++ b/torch/csrc/utils/tensor_types.h
@@ -6,6 +6,7 @@
 
 namespace torch { namespace utils {
 
+const char* backend_to_string(const at::Backend& backend);
 std::string options_to_string(const at::TensorOptions options);
 std::string type_to_string(const at::DeprecatedTypeProperties& type);
 at::TensorOptions options_from_string(const std::string& str);
-- 
2.25.1

