From 5138f7c2e176885fa034ccc059478b99c4172c10 Mon Sep 17 00:00:00 2001
From: chengjun <chengjun.lu@intel.com>
Date: Fri, 6 Nov 2020 00:12:49 +0800
Subject: [PATCH 03/14] Add intel XPU hooks interface

---
 aten/src/ATen/Context.h                    | 57 ++++++++++++---
 aten/src/ATen/Version.cpp                  |  4 ++
 aten/src/ATen/detail/XPUHooksInterface.cpp | 29 ++++++++
 aten/src/ATen/detail/XPUHooksInterface.h   | 83 ++++++++++++++++++++++
 aten/src/ATen/native/Memory.cpp            | 17 ++++-
 aten/src/ATen/native/TensorFactories.cpp   |  7 +-
 6 files changed, 183 insertions(+), 14 deletions(-)
 create mode 100644 aten/src/ATen/detail/XPUHooksInterface.cpp
 create mode 100644 aten/src/ATen/detail/XPUHooksInterface.h

diff --git a/aten/src/ATen/Context.h b/aten/src/ATen/Context.h
index fed5e88e53..2a6cf6cdca 100644
--- a/aten/src/ATen/Context.h
+++ b/aten/src/ATen/Context.h
@@ -1,6 +1,6 @@
 #pragma once
 
-#include <ATen/core/ATenGeneral.h>
+#include <ATen/CPUGeneratorImpl.h>
 #include <ATen/Tensor.h>
 #include <ATen/Utils.h>
 #include <ATen/core/ATenGeneral.h>
@@ -9,6 +9,7 @@
 #include <ATen/core/LegacyTypeDispatch.h>
 #include <ATen/detail/CUDAHooksInterface.h>
 #include <ATen/detail/HIPHooksInterface.h>
+#include <ATen/detail/XPUHooksInterface.h>
 #include <c10/util/Exception.h>
 #include <c10/core/impl/DeviceGuardImplInterface.h>
 #include <c10/core/QEngine.h>
@@ -27,29 +28,35 @@ class CAFFE2_API Context {
 
   const Generator& defaultGenerator(Device device) {
     DeviceType device_type = device.type();
-    initCUDAIfNeeded(device_type);
-    initHIPIfNeeded(device_type);
+    initDeviceIfNeeded(device_type);
     if (device_type == at::kCPU) {
       return at::detail::getDefaultCPUGenerator();
     } else if (device_type == at::kCUDA) {
       return at::detail::getCUDAHooks().getDefaultCUDAGenerator(device.index());
+    } else if (device_type == at::kXPU) {
+      return at::detail::getXPUHooks().getDefaultXPUGenerator(device.index());
     } else {
       AT_ERROR(DeviceTypeName(device_type), " device type not enabled.");
     }
   }
   Device getDeviceFromPtr(void* data, DeviceType device_type) {
-    initCUDAIfNeeded(device_type);
-    initHIPIfNeeded(device_type);
+    initDeviceIfNeeded(device_type);
     if (device_type == at::kCPU) {
       return DeviceType::CPU;
     } else if (device_type == at::kCUDA) {
       return at::detail::getCUDAHooks().getDeviceFromPtr(data);
+    } else if (device_type == at::kXPU) {
+      return at::detail::getXPUHooks().getDeviceFromPtr(data);
     } else {
       AT_ERROR(DeviceTypeName(device_type), " device type not enabled.");
     }
   }
   bool isPinnedPtr(void* data) {
-    return detail::getCUDAHooks().isPinnedPtr(data);
+    deviceExclusiveCheck();
+    if (hasCUDA()) {
+      return detail::getCUDAHooks().isPinnedPtr(data);
+    }
+    return detail::getXPUHooks().isPinnedPtr(data);
   }
   bool hasOpenMP() const;
   bool hasMKL() const;
@@ -70,6 +77,28 @@ class CAFFE2_API Context {
   bool hasHIP() const {
     return detail::getHIPHooks().hasHIP();
   }
+  bool hasXPU() const {
+    return detail::getXPUHooks().hasXPU();
+  }
+  void deviceExclusiveCheck() {
+    int count = 0;
+    if (hasCUDA()) {
+      count++;
+    }
+    if (hasHIP()) {
+      count++;
+    }
+    if (hasXPU()) {
+      count++;
+    }
+    if (count > 1) {
+      throw std::runtime_error(
+          "Enabling CUDA, HIP and XPU at same time in ATen is not supported,"
+          "as HIP masqueradesto be CUDA (e.g., when you say CUDA, on a HIP build of ATen,"
+          "this actually means HIP, while XPU cannot work with two others."
+          "Rebuild PyTorch with one or the other disabled.");
+    }
+  }
   bool hasXLA() const {
     return c10::impl::hasDeviceGuardImpl(at::DeviceType::XLA);
   }
@@ -195,13 +224,10 @@ class CAFFE2_API Context {
   bool releaseWeightsWhenPrepacking() const;
 
  private:
-  void initCUDAIfNeeded(DeviceType p) {
+  void initDeviceIfNeeded(DeviceType p) {
     if (p == DeviceType::CUDA) {
       lazyInitCUDA();
-    }
-  }
-  void initHIPIfNeeded(DeviceType p) {
-    if (p == DeviceType::HIP) {
+    } else if (p == DeviceType::HIP) {
       lazyInitHIP();
     }
   }
@@ -261,10 +287,19 @@ static inline bool hasHIP() {
   return globalContext().hasHIP();
 }
 
+static inline bool hasXPU() {
+  return globalContext().hasXPU();
+}
+
 static inline bool hasXLA() {
   return globalContext().hasXLA();
 }
 
+
+static void deviceExclusiveCheck() {
+  globalContext().deviceExclusiveCheck();
+}
+
 // Despite its name, this function returns the number of *CUDA* GPUs.
 static inline size_t getNumGPUs() {
   // WARNING: DO NOT ADD LOGIC TO HANDLE OTHER DEVICE TYPES TO THIS
diff --git a/aten/src/ATen/Version.cpp b/aten/src/ATen/Version.cpp
index 0fcf38470a..ee49595e46 100644
--- a/aten/src/ATen/Version.cpp
+++ b/aten/src/ATen/Version.cpp
@@ -171,6 +171,10 @@ std::string show_config() {
     ss << detail::getCUDAHooks().showConfig();
   }
 
+  if (hasXPU()) {
+    ss << detail::getXPUHooks().showConfig();
+  }
+
   ss << "  - Build settings: ";
   for (const std::pair<std::string, std::string>& pair : caffe2::GetBuildOptions()) {
     if (!pair.second.empty()) {
diff --git a/aten/src/ATen/detail/XPUHooksInterface.cpp b/aten/src/ATen/detail/XPUHooksInterface.cpp
new file mode 100644
index 0000000000..d740068379
--- /dev/null
+++ b/aten/src/ATen/detail/XPUHooksInterface.cpp
@@ -0,0 +1,29 @@
+#include <ATen/detail/XPUHooksInterface.h>
+
+#include <c10/util/Exception.h>
+
+#include <cstddef>
+#include <memory>
+#include <mutex>
+
+namespace at {
+namespace detail {
+
+static XPUHooksInterface *xpu_hooks = nullptr;
+
+const XPUHooksInterface &getXPUHooks() {
+  static std::once_flag once;
+  std::call_once(once, [] {
+    xpu_hooks =
+        XPUHooksRegistry()->Create("XPUHooks", XPUHooksArgs{}).release();
+    if (!xpu_hooks) {
+      xpu_hooks = new XPUHooksInterface();
+    }
+  });
+  return *xpu_hooks;
+}
+} // namespace detail
+
+C10_DEFINE_REGISTRY(XPUHooksRegistry, XPUHooksInterface, XPUHooksArgs)
+
+} // namespace at
diff --git a/aten/src/ATen/detail/XPUHooksInterface.h b/aten/src/ATen/detail/XPUHooksInterface.h
new file mode 100644
index 0000000000..35ba452714
--- /dev/null
+++ b/aten/src/ATen/detail/XPUHooksInterface.h
@@ -0,0 +1,83 @@
+#pragma once
+
+#include <ATen/core/Generator.h>
+#include <c10/core/Allocator.h>
+#include <c10/util/Exception.h>
+
+#include <c10/util/Registry.h>
+
+#include <cstddef>
+#include <functional>
+#include <memory>
+
+namespace at {
+class Context;
+}
+
+namespace at {
+
+constexpr const char* XPU_HELP =
+  "PyTorch splits its backend into two shared libraries: a CPU library "
+  "and a XPU library; this error has occurred because you are trying "
+  "to use some XPU functionality, but the XPU library has not been "
+  "loaded by the dynamic linker for some reason.  The XPU library MUST "
+  "be loaded, EVEN IF you don't directly use any symbols from the XPU library!";
+
+struct CAFFE2_API XPUHooksInterface {
+  virtual ~XPUHooksInterface() {}
+
+  virtual void initXPU() const {
+    TORCH_CHECK(false, "Cannot initialize XPU without XPU library.", XPU_HELP);
+  }
+
+  virtual bool hasXPU() const {
+    return false;
+  }
+
+  virtual bool hasOneMKL() const {
+    return false;
+  }
+
+  virtual bool hasOneDNN() const {
+    return false;
+  }
+
+  virtual std::string showConfig() const {
+    TORCH_CHECK(false, "Cannot query detailed XPU version without XPU library. ", XPU_HELP);
+  }
+
+  virtual int64_t getCurrentDevice() const {
+    return -1;
+  }
+
+  virtual int getDeviceCount() const {
+    return 0;
+  }
+
+  virtual Device getDeviceFromPtr(void* data) const {
+    TORCH_CHECK(false, "Cannot get device of pointer on XPU without XPU library.", XPU_HELP);
+  }
+
+  virtual bool isPinnedPtr(void* data) const {
+    return false;
+  }
+
+  virtual Allocator* getPinnedMemoryAllocator() const {
+    TORCH_CHECK(false, "Pinned Memory requires XPU library support", XPU_HELP);
+  }
+
+  virtual const Generator& getDefaultXPUGenerator(DeviceIndex device_index = -1) const {
+    TORCH_CHECK(false, "Cannot get default XPU generator without XPU library.", XPU_HELP);
+  }
+};
+
+struct CAFFE2_API XPUHooksArgs {};
+
+C10_DECLARE_REGISTRY(XPUHooksRegistry, XPUHooksInterface, XPUHooksArgs);
+#define REGISTER_XPU_HOOKS(clsname) \
+  C10_REGISTER_CLASS(XPUHooksRegistry, clsname, clsname)
+
+namespace detail {
+CAFFE2_API const XPUHooksInterface& getXPUHooks();
+} // namespace detail
+} // namespace at
diff --git a/aten/src/ATen/native/Memory.cpp b/aten/src/ATen/native/Memory.cpp
index 400a20d67a..ec264dc005 100644
--- a/aten/src/ATen/native/Memory.cpp
+++ b/aten/src/ATen/native/Memory.cpp
@@ -10,17 +10,30 @@ namespace at {
 namespace native {
 
 bool is_pinned(const Tensor& self) {
-  return detail::getCUDAHooks().isPinnedPtr(self.storage().data());
+  deviceExclusiveCheck();
+  if (hasCUDA()) {
+    return detail::getCUDAHooks().isPinnedPtr(self.storage().data());
+  }
+  return detail::getXPUHooks().isPinnedPtr(self.storage().data());
 }
 
 Tensor pin_memory(const Tensor& self) {
   if (self.options().backend() != Backend::CPU) {
     AT_ERROR("cannot pin '", self.toString(), "' only dense CPU tensors can be pinned");
   }
+
+  deviceExclusiveCheck();
   if (self.is_pinned()) {
     return self;
   }
-  auto* allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+
+  c10::Allocator* allocator;
+  if (hasCUDA()) {
+    allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+  } else if (hasXPU()) {
+    allocator = detail::getXPUHooks().getPinnedMemoryAllocator();
+  }
+
   auto storage = Storage(
       Storage::use_byte_size_t(),
       detail::computeStorageNbytes(
diff --git a/aten/src/ATen/native/TensorFactories.cpp b/aten/src/ATen/native/TensorFactories.cpp
index 149aab7cfc..53f481172e 100644
--- a/aten/src/ATen/native/TensorFactories.cpp
+++ b/aten/src/ATen/native/TensorFactories.cpp
@@ -179,7 +179,12 @@ Tensor empty_cpu(IntArrayRef size, const TensorOptions& options_, c10::optional<
 
   c10::Allocator* allocator;
   if (options.pinned_memory()) {
-    allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+    deviceExclusiveCheck();
+    if (hasCUDA()) {
+      allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+    } else if (hasXPU()) {
+      allocator = detail::getXPUHooks().getPinnedMemoryAllocator();
+    }
   } else {
     allocator = at::getCPUAllocator();
   }
-- 
2.25.1

