From 0b882e441dd8c6914bc622146b94f9bd5560708d Mon Sep 17 00:00:00 2001
From: Ye Ting <ting.ye@intel.com>
Date: Mon, 11 Jan 2021 23:11:01 +0800
Subject: [PATCH 05/17] 5.Add intel XPU JIT support

---
 c10/core/Device.h                          |  5 ++++
 test/cpp/jit/test_argument_spec.cpp        |  4 +--
 torch/csrc/jit/codegen/fuser/executor.cpp  |  2 ++
 torch/csrc/jit/passes/decompose_ops.cpp    |  2 +-
 torch/csrc/jit/passes/shape_analysis.cpp   | 10 +-------
 torch/csrc/jit/passes/tensorexpr_fuser.cpp |  2 ++
 torch/csrc/jit/runtime/argument_spec.h     | 29 +++++++++++++---------
 7 files changed, 30 insertions(+), 24 deletions(-)

diff --git a/c10/core/Device.h b/c10/core/Device.h
index f1249e865f..52bb22cc00 100644
--- a/c10/core/Device.h
+++ b/c10/core/Device.h
@@ -86,6 +86,11 @@ struct C10_API Device final {
     return type_ == DeviceType::CPU;
   }
 
+  /// Return true if the device is of XPU type.
+  bool is_xpu() const noexcept {
+    return type_ == DeviceType::XPU;
+  }
+
   /// Same string as returned from operator<<.
   std::string str() const;
 
diff --git a/test/cpp/jit/test_argument_spec.cpp b/test/cpp/jit/test_argument_spec.cpp
index bf40761fc4..800a4767dc 100644
--- a/test/cpp/jit/test_argument_spec.cpp
+++ b/test/cpp/jit/test_argument_spec.cpp
@@ -9,8 +9,8 @@ namespace jit {
 
 namespace {
 
-int device(const autograd::Variable& v) {
-  return v.device().is_cuda() ? v.get_device() : -1;
+at::Device device(const autograd::Variable& v) {
+  return v.device();
 }
 
 bool isEqual(at::IntArrayRef lhs, at::IntArrayRef rhs) {
diff --git a/torch/csrc/jit/codegen/fuser/executor.cpp b/torch/csrc/jit/codegen/fuser/executor.cpp
index da085f267c..e59d579b21 100644
--- a/torch/csrc/jit/codegen/fuser/executor.cpp
+++ b/torch/csrc/jit/codegen/fuser/executor.cpp
@@ -362,6 +362,8 @@ bool runFusion(const int64_t key, Stack& stack, std::string* code_out) {
     return false;
   if (device.is_cpu() && !canFuseOnCPU())
     return false;
+  if (device.is_xpu())
+    return false;
 
   // Validates sizes and expands inputs as needed
   auto maybe_map_size = canRunKernel(spec, inputs);
diff --git a/torch/csrc/jit/passes/decompose_ops.cpp b/torch/csrc/jit/passes/decompose_ops.cpp
index 2d0f70a011..605cd59d4d 100644
--- a/torch/csrc/jit/passes/decompose_ops.cpp
+++ b/torch/csrc/jit/passes/decompose_ops.cpp
@@ -41,7 +41,7 @@ bool isDecomposableNorm(Node* normalize_op) {
   auto device = input->type()->expect<TensorType>()->device();
   // As of now, we do the decomposition for batchnorm/layernorm on GPU device
   // only
-  if (!device || (*device).is_cpu()) {
+  if (!device || (*device).is_cpu() || (*device).is_xpu()) {
     return false;
   }
 
diff --git a/torch/csrc/jit/passes/shape_analysis.cpp b/torch/csrc/jit/passes/shape_analysis.cpp
index cf9a93b7f5..d77a08d771 100644
--- a/torch/csrc/jit/passes/shape_analysis.cpp
+++ b/torch/csrc/jit/passes/shape_analysis.cpp
@@ -146,12 +146,6 @@ class ShapePropagator {
     return dim;
   }
 
-  // TODO: Would be better to make JIT not assume that CUDA devices
-  // are the only thing that exist.
-  static at::Device jitDeviceIndexToDevice(int device) {
-    return device == -1 ? at::kCPU : at::Device(at::kCUDA, device);
-  }
-
   IValue representativeValue(Value* v) {
     TypePtr type_ = v->type();
     // if the value is actually constant, just use it!
@@ -160,13 +154,11 @@ class ShapePropagator {
     }
     if (TensorTypePtr type = type_->cast<TensorType>()) {
       if (type->isComplete()) {
-        auto attype = type->device()->is_cpu() ? at::CPU(*type->scalarType())
-                                               : at::CUDA(*type->scalarType());
         at::DeviceGuard device_guard(*type->device());
         return at::empty_strided(
                    *type->sizes().concrete_sizes(),
                    *type->strides().concrete_sizes(),
-                   attype.options())
+                   at::TensorOptions(*type->device()).dtype(*type->scalarType()))
             .zero_();
       }
       // fallthrough
diff --git a/torch/csrc/jit/passes/tensorexpr_fuser.cpp b/torch/csrc/jit/passes/tensorexpr_fuser.cpp
index 76711c76c7..dc618f4c73 100644
--- a/torch/csrc/jit/passes/tensorexpr_fuser.cpp
+++ b/torch/csrc/jit/passes/tensorexpr_fuser.cpp
@@ -696,6 +696,8 @@ class TensorExprFuser {
       return canFuseOnCPU();
     } else if (device->is_cuda()) {
       return canFuseOnGPU();
+    } else if (device->is_xpu()) {
+      return false;
     }
     throw std::runtime_error("Unknown device");
   }
diff --git a/torch/csrc/jit/runtime/argument_spec.h b/torch/csrc/jit/runtime/argument_spec.h
index 401933c6d6..e289029d97 100644
--- a/torch/csrc/jit/runtime/argument_spec.h
+++ b/torch/csrc/jit/runtime/argument_spec.h
@@ -15,18 +15,15 @@ namespace jit {
 // GraphExecutor creates specializations of Graphs for different
 // dimensionalitities and types of inputs.
 
-inline static at::Device ConvertIntToCPUOrCUDA(int device) {
-  return device < 0 ? at::kCPU : at::Device(DeviceType::CUDA, device);
-}
 struct ArgumentInfo {
   friend struct ArgumentSpec;
-  using plain_data_type = uint32_t;
+  using plain_data_type = uint64_t;
 
   bool defined() const {
     return defined_;
   }
-  int device() const {
-    return device_;
+  at::Device device() const {
+    return at::Device(DeviceType(dev_type_), device_);
   }
   // XXX: It is guaranteed that this will return false when called on non-tensor
   // arguments
@@ -45,7 +42,7 @@ struct ArgumentInfo {
 
     return TensorType::create(
         type(),
-        ConvertIntToCPUOrCUDA(device()),
+        device(),
         c10::optional<size_t>(dim()),
         requires_grad());
   }
@@ -61,6 +58,8 @@ struct ArgumentInfo {
   int device_ : 8; // NOTE: this needs to be signed because we use -1 to
                    // represent CPU
   unsigned type_ : 8;
+  unsigned dev_type_ : 16;
+  unsigned : 16;
 };
 
 static_assert(
@@ -101,7 +100,9 @@ struct ArgumentSpec {
     if ((arg.defined_ = t->defined())) {
       arg.requires_grad_ = with_grad && autograd::Variable(*t).requires_grad();
       arg.dim_ = t->dim();
-      arg.device_ = t->is_cuda() ? t->get_device() : -1;
+      at::Device device = t->device();
+      arg.dev_type_ = static_cast<std::underlying_type<DeviceType>::type>(device.type());
+      arg.device_ = device.index();
       arg.type_ = static_cast<unsigned>(t->scalar_type());
     }
     combineHash(arg);
@@ -214,7 +215,8 @@ struct CompleteArgumentInfoPOD {
   unsigned defined : 1;
   unsigned requires_grad : 1;
   signed device : 14;
-  uint32_t total_dims; // all TensorInfoPODs are in CompleteArgumentSpec's
+  unsigned dev_type : 16;
+  uint16_t total_dims; // all TensorInfoPODs are in CompleteArgumentSpec's
                        // tensor_info() array. total_dims is the total number of
                        // dimensions seen so far in all previous members of
                        // tensor_info(), including this tensor 2*total_dims
@@ -256,6 +258,9 @@ struct CompleteArgumentSpec {
         if (pod.defined) {
           pod.type = static_cast<int>(t.scalar_type());
           pod.device = (!t.is_cuda()) ? -1 : t.get_device();
+          at::Device device = t.device();
+          pod.dev_type = static_cast<std::underlying_type<DeviceType>::type>(device.type());
+          pod.device = device.index();
           pod.requires_grad = with_grad && t.requires_grad();
           total_dims += t.ndimension();
           auto sizes = t.sizes();
@@ -332,8 +337,8 @@ struct CompleteArgumentInfo {
   bool requires_grad() const {
     return pod(i).requires_grad;
   }
-  int device() const {
-    return pod(i).device;
+  at::Device device() const {
+    return at::Device(DeviceType(pod(i).dev_type), pod(i).device);
   }
   int ndimension() const {
     // See [valid range], it is always valid to ask for offset for (i + 1)
@@ -353,7 +358,7 @@ struct CompleteArgumentInfo {
       return TensorType::get();
     return TensorType::create(
         type(),
-        ConvertIntToCPUOrCUDA(device()),
+        device(),
         c10::VaryingShape<int64_t>{sizes()},
         c10::VaryingShape<int64_t>{strides()},
         requires_grad());
-- 
2.25.1

