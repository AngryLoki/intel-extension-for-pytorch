From e80dae14711376bd01ac34dadf1d9b0b01068dd2 Mon Sep 17 00:00:00 2001
From: leizhenyuan <110007460+leizhenyuan@users.noreply.github.com>
Date: Sat, 8 Oct 2022 17:24:32 +0800
Subject: [PATCH 33/35] Revert "enable pin memory on xpu (#67)" (#69)

This reverts commit ec76f1d45f4affe39aa8a284a368f7f92eb06401.
---
 torch/_utils.py                       |  4 ++--
 torch/utils/data/_utils/pin_memory.py | 18 +++++-------------
 torch/utils/data/dataloader.py        | 16 +++++-----------
 3 files changed, 12 insertions(+), 26 deletions(-)

diff --git a/torch/_utils.py b/torch/_utils.py
index 7f245e352f..bb018664eb 100644
--- a/torch/_utils.py
+++ b/torch/_utils.py
@@ -437,7 +437,7 @@ class ExceptionWrapper(object):
 def _get_available_device_type():
     if torch.cuda.is_available():
         return "cuda"
-    if hasattr(torch, "xpu") and torch.xpu.is_available():  # type: ignore[attr-defined]
+    if hasattr(torch, "xpu") and torch.xpu.is_available():
         return "xpu"
     # add more available device types here
     return None
@@ -448,7 +448,7 @@ def _get_device_attr(get_member):
     if device_type and device_type.lower() == "cuda":
         return get_member(torch.cuda)
     elif device_type and device_type.lower() == "xpu":
-        return get_member(torch.xpu)  # type: ignore[attr-defined]
+        return get_member(torch.xpu)
     # add more available device types here
     return None
 
diff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py
index f621d44af4..f17aae9223 100644
--- a/torch/utils/data/_utils/pin_memory.py
+++ b/torch/utils/data/_utils/pin_memory.py
@@ -19,13 +19,8 @@ def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
     # consuming all CPU cores.
     torch.set_num_threads(1)
 
-    device_type = torch._utils._get_available_device_type()
+    torch.cuda.set_device(device_id)
 
-    if device_type == "cuda":
-        torch.cuda.set_device(device_id)
-    elif device_type == "xpu":
-        torch.xpu.set_device(device_id)  # type: ignore[attr-defined]
-    
     # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the
     # logic of this function.
     while not done_event.is_set():
@@ -38,8 +33,8 @@ def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
             try:
                 data = pin_memory(data)
             except Exception:
-                    data = ExceptionWrapper(
-                    where="in pin memory thread for device_type {} device_id {}".format(device_type, device_id))
+                data = ExceptionWrapper(
+                    where="in pin memory thread for device {}".format(device_id))
             r = (idx, data)
         while not done_event.is_set():
             try:
@@ -51,11 +46,8 @@ def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
 
 
 def pin_memory(data):
-    
-    device_type = torch._utils._get_available_device_type()
-    
     if isinstance(data, torch.Tensor):
-        return data.pin_memory(device_type)
+        return data.pin_memory()
     elif isinstance(data, string_classes):
         return data
     elif isinstance(data, collections.abc.Mapping):
@@ -65,6 +57,6 @@ def pin_memory(data):
     elif isinstance(data, collections.abc.Sequence):
         return [pin_memory(sample) for sample in data]
     elif hasattr(data, "pin_memory"):
-        return data.pin_memory(device_type)
+        return data.pin_memory()
     else:
         return data
diff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py
index 886a45a116..0f46ad283e 100644
--- a/torch/utils/data/dataloader.py
+++ b/torch/utils/data/dataloader.py
@@ -5,7 +5,6 @@ functions to be run in multiprocessing. E.g., the data loading worker loop is
 in `./_utils/worker.py`.
 """
 
-from ast import IsNot
 import os
 import threading
 import itertools
@@ -492,8 +491,7 @@ class _BaseDataLoaderIter(object):
         self._index_sampler = loader._index_sampler
         self._num_workers = loader.num_workers
         self._prefetch_factor = loader.prefetch_factor
-        self._device_type = torch._utils._get_available_device_type()
-        self._pin_memory = loader.pin_memory and not (self._device_type is None)
+        self._pin_memory = loader.pin_memory and torch.cuda.is_available()
         self._timeout = loader.timeout
         self._collate_fn = loader.collate_fn
         self._sampler_iter = iter(self._index_sampler)
@@ -894,6 +892,7 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
         self._worker_pids_set = False
         self._shutdown = False
         self._workers_done_event = multiprocessing_context.Event()
+
         self._index_queues = []
         self._workers = []
         for i in range(self._num_workers):
@@ -925,16 +924,11 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
 
             # Queue is not type-annotated
             self._data_queue = queue.Queue()  # type: ignore[var-annotated]
-            if self._device_type == "cuda":
-                current_device = torch.cuda.current_device()
-            elif self._device_type == "xpu":
-                current_device = torch.xpu.current_device()  # type: ignore[attr-defined]
-            pin_args=(self._worker_result_queue, self._data_queue, current_device, 
-                    self._pin_memory_thread_done_event)
-            
             pin_memory_thread = threading.Thread(
                 target=_utils.pin_memory._pin_memory_loop,
-                args=pin_args)
+                args=(self._worker_result_queue, self._data_queue,
+                      torch.cuda.current_device(),
+                      self._pin_memory_thread_done_event))
             pin_memory_thread.daemon = True
             pin_memory_thread.start()
             # Similar to workers (see comment above), we only register
-- 
2.25.1

