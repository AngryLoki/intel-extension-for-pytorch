diff --git a/CMakeLists.txt b/CMakeLists.txt
index 7d65067..211cdaa 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -221,6 +221,7 @@ set(SELECTED_OP_LIST "" CACHE STRING
     "Path to the yaml file that contains the list of operators to include for custom build. Include all operators by default.")
 set(OP_DEPENDENCY "" CACHE STRING
     "Path to the yaml file that contains the op dependency graph for custom build.")
+set(USE_ITT "None" CACHE STRING "Use Intel(R) VTune Profiler ITT functionality")
 
 # This is a fix for a rare build issue on Ubuntu:
 # symbol lookup error: miniconda3/envs/pytorch-py3.7/lib/libmkl_intel_lp64.so: undefined symbol: mkl_blas_dsyrk
diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index 8025a7d..6caf249 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -732,6 +732,14 @@ if (NOT INTERN_BUILD_MOBILE OR NOT BUILD_CAFFE2_MOBILE)
       ${TORCH_SRC_DIR}/csrc/api
       ${TORCH_SRC_DIR}/csrc/api/include)
   endif()
+  if(NOT ${USE_ITT} STREQUAL "None")
+    message(STATUS "USE_ITT: ${USE_ITT}")
+    list(APPEND TORCH_SRCS
+      ${TORCH_SRC_DIR}/csrc/itt_wrapper.cpp
+      ${TORCH_SRC_DIR}/csrc/autograd/profiler_itt.cpp)
+    list(APPEND Caffe2_DEPENDENCY_INCLUDE "${USE_ITT}/include")
+    list(APPEND Caffe2_DEPENDENCY_LIBS "${USE_ITT}/lib64/libittnotify.a")
+  endif()
 
   if(USE_CUDA)
     if(MSVC)
diff --git a/cmake/Summary.cmake b/cmake/Summary.cmake
index 83fa7bd..3cb95b2 100644
--- a/cmake/Summary.cmake
+++ b/cmake/Summary.cmake
@@ -103,6 +103,7 @@ function (caffe2_print_configuration_summary)
   message(STATUS "  USE_METAL             : ${USE_METAL}")
   message(STATUS "  USE_MKL               : ${CAFFE2_USE_MKL}")
   message(STATUS "  USE_MKLDNN            : ${USE_MKLDNN}")
+  message(STATUS "  USE_ITT               : ${USE_ITT}")
   if(${CAFFE2_USE_MKLDNN})
     message(STATUS "  USE_MKLDNN_CBLAS      : ${USE_MKLDNN_CBLAS}")
   endif()
diff --git a/setup.py b/setup.py
index 7352d3b..4fc8d47 100644
--- a/setup.py
+++ b/setup.py
@@ -42,6 +42,9 @@
 #   USE_MKLDNN=0
 #     disables use of MKLDNN
 #
+#   USE_ITT=<Root folder of Intel(R) VTune Profiler>
+#     enable use of Intel(R) VTune Profiler's ITT functionality
+#
 #   MKLDNN_THREADING
 #     MKL-DNN threading mode: TBB or OMP (default)
 #
@@ -411,6 +414,11 @@ class build_ext(setuptools.command.build_ext.build_ext):
         else:
             report('-- Building without distributed package')
 
+        if cmake_cache_vars['USE_ITT'] != 'None':
+            report('-- Using ITT')
+        else:
+            report('-- Not using ITT')
+
         # Do not use clang to compile exensions if `-fstack-clash-protection` is defined
         # in system CFLAGS
         system_c_flags = distutils.sysconfig.get_config_var('CFLAGS')
diff --git a/torch/CMakeLists.txt b/torch/CMakeLists.txt
index c61291b..9644b28 100644
--- a/torch/CMakeLists.txt
+++ b/torch/CMakeLists.txt
@@ -175,6 +175,17 @@ else()
       -Wno-strict-aliasing)
 endif()
 
+if(NOT ${USE_ITT} STREQUAL "None")
+  list(APPEND TORCH_PYTHON_SRCS
+    ${TORCH_SRC_DIR}/csrc/itt_wrapper.cpp
+    ${TORCH_SRC_DIR}/csrc/itt.cpp
+    ${TORCH_SRC_DIR}/csrc/autograd/profiler_itt.cpp
+  )
+  list(APPEND TORCH_PYTHON_INCLUDE_DIRECTORIES "${USE_ITT}/include")
+  list(APPEND TORCH_PYTHON_LINK_LIBRARIES "${USE_ITT}/lib64/libittnotify.a")
+  list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_ITT)
+endif()
+
 if (USE_CUDA)
     list(APPEND TORCH_PYTHON_SRCS
       ${TORCH_SRC_DIR}/csrc/cuda/Module.cpp
diff --git a/torch/autograd/profiler.py b/torch/autograd/profiler.py
index 718b7c5..daa36b8 100644
--- a/torch/autograd/profiler.py
+++ b/torch/autograd/profiler.py
@@ -375,6 +375,33 @@ class record_function(ContextDecorator):
         return False
 
 
+class emit_itt(object):
+    def __init__(self, enabled=True, record_shapes=False):
+        self.enabled = enabled
+        self.entered = False
+        self.record_shapes = record_shapes
+
+    def __enter__(self):
+        if not self.enabled:
+            return
+        if self.entered:
+            raise RuntimeError("ITT annotation context manager is not reentrant")
+        self.entered = True
+        torch.autograd._enable_profiler(
+            torch.autograd.ProfilerConfig(
+                torch.autograd.ProfilerState.ITT,
+                self.record_shapes
+            )
+        )
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        if not self.enabled:
+            return
+        torch.autograd._disable_profiler()
+        return False
+
+
 class emit_nvtx(object):
     """Context manager that makes every autograd operation emit an NVTX range.
 
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index 8955d0c..19ac7ef 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -589,6 +589,12 @@ void initModule(PyObject *module);
 }} // namespace torch::cuda
 #endif
 
+#ifdef USE_ITT
+namespace torch {
+void initIttBindings(PyObject *module);
+} // namespace torch
+#endif
+
 bool THDPDoubleStorage_init(PyObject *module);
 bool THDPFloatStorage_init(PyObject *module);
 // TODO: fix
@@ -675,6 +681,9 @@ PyObject* initModule() {
   torch::autograd::initNNFunctions(module);
   torch::autograd::init_legacy_variable(module);
   torch::python::init_bindings(module);
+#ifdef USE_ITT
+  torch::initIttBindings(module);
+#endif
 #ifdef USE_CUDA
   torch::cuda::initModule(module);
 #endif
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index 96c3263..a467d20 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -33,7 +33,8 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
       .value("Disabled", ProfilerState::Disabled)
       .value("CPU", ProfilerState::CPU)
       .value("CUDA", ProfilerState::CUDA)
-      .value("NVTX", ProfilerState::NVTX);
+      .value("NVTX", ProfilerState::NVTX)
+      .value("ITT", ProfilerState::ITT);
 
   py::class_<ProfilerConfig>(m, "ProfilerConfig")
       .def(py::init<ProfilerState, bool>());
diff --git a/torch/csrc/autograd/profiler.cpp b/torch/csrc/autograd/profiler.cpp
index ee6f9b8..0aa3e1a 100644
--- a/torch/csrc/autograd/profiler.cpp
+++ b/torch/csrc/autograd/profiler.cpp
@@ -12,11 +12,14 @@ namespace torch { namespace autograd { namespace profiler {
 
 namespace {
 
-CUDAStubs default_stubs;
-constexpr CUDAStubs* default_stubs_addr = &default_stubs;
+CUDAStubs default_cuda_stubs;
+ITTStubs default_itt_stubs;
+constexpr CUDAStubs* default_cuda_stubs_addr = &default_cuda_stubs;
+constexpr ITTStubs* default_itt_stubs_addr = &default_itt_stubs;
 // constant initialization, so it is guaranteed to be initialized before
 // static initialization calls which may invoke registerCUDAMethods
-static CUDAStubs* cuda_stubs = default_stubs_addr;
+static CUDAStubs* cuda_stubs = default_cuda_stubs_addr;
+static ITTStubs* itt_stubs = default_itt_stubs_addr;
 
 ProfilerState state = ProfilerState::Disabled;
 // Protects access all_event_lists_map.
@@ -32,6 +35,10 @@ void registerCUDAMethods(CUDAStubs* stubs) {
   cuda_stubs = stubs;
 }
 
+void registerITTMethods(ITTStubs* stubs) {
+  itt_stubs = stubs;
+}
+
 ProfilerConfig::~ProfilerConfig() = default;
 
 RangeEventList& getEventList() {
@@ -50,6 +57,8 @@ void mark(std::string name, bool include_cuda /* = true */) {
   }
   if (state == ProfilerState::NVTX) {
     cuda_stubs->nvtxMarkA(name.c_str());
+  } else if (state == ProfilerState::ITT) {
+	  itt_stubs->ittMark(name.c_str());
   } else {
     getEventList().record(
         EventKind::Mark,
@@ -99,6 +108,34 @@ void pushRangeImpl(
     } else {
       cuda_stubs->nvtxRangePushA(name.str());
     }
+  } else if (state == ProfilerState::ITT) {
+    if(sequence_nr >= 0 || shapes.size() > 0) {
+      std::stringstream s;
+      if(sequence_nr >= 0)
+        s << name.str() << msg << sequence_nr;
+      if(shapes.size() > 0) {
+        s << ", sizes = [";
+        for(int i = 0; i < shapes.size(); i++) {
+          if(shapes[i].size() > 0) {
+            s << "[";
+            for(int dim = 0; dim < shapes[i].size(); dim++) {
+              s << shapes[i][dim];
+              if(dim < shapes[i].size() - 1)
+                s << ", ";
+            }
+            s << "]";
+          }
+          else
+            s << "[]";
+          if(i < shapes.size() - 1)
+            s << ", ";
+        }
+        s << "]";
+      }
+      itt_stubs->ittRangePush(s.str().c_str());
+    } else {
+      itt_stubs->ittRangePush(name.str());
+    }
   } else {
     getEventList().record(
         EventKind::PushRange,
@@ -119,6 +156,8 @@ void popRange() {
   }
   if (state == ProfilerState::NVTX) {
     cuda_stubs->nvtxRangePop();
+  }  else if (state == ProfilerState::ITT) {
+    itt_stubs->ittRangePop();
   } else {
     getEventList().record(
         EventKind::PopRange,
@@ -133,6 +172,8 @@ void enableProfiler(ProfilerConfig config) {
   AT_ASSERT(new_state != ProfilerState::Disabled);
   if (new_state == ProfilerState::NVTX && !cuda_stubs->enabled())
     throw std::runtime_error("Can't use NVTX profiler - PyTorch was compiled without CUDA");
+  if (new_state == ProfilerState::ITT && !itt_stubs->enabled())
+    throw std::runtime_error("Can't use Intel(R) VTune Profiler's ITT functionality - PyTorch was compiled without ITT");
   if (state != ProfilerState::Disabled && new_state != state) {
     throw std::runtime_error("can't change kind of profiling (e.g. NVTX to CPU) while profiler is running");
   }
diff --git a/torch/csrc/autograd/profiler.h b/torch/csrc/autograd/profiler.h
index 01e5c38..99d8602 100644
--- a/torch/csrc/autograd/profiler.h
+++ b/torch/csrc/autograd/profiler.h
@@ -61,6 +61,28 @@ private:
 
 TORCH_API void registerCUDAMethods(CUDAStubs* stubs);
 
+struct TORCH_API ITTStubs {
+  virtual void ittMark(const char* name) {
+    fail();
+  }
+  virtual void ittRangePush(const char* name) {
+    fail();
+  }
+  virtual void ittRangePop() {
+    fail();
+  }
+  virtual bool enabled() {
+    return false;
+  }
+
+private:
+  void fail() {
+    AT_ERROR("ITT used in profiler but not enabled.");
+  }
+};
+
+TORCH_API void registerITTMethods(ITTStubs* stubs);
+
 constexpr inline size_t ceilToMultiple(size_t a, size_t b) {
   return ((a + b - 1) / b) * b;
 }
@@ -102,6 +124,7 @@ enum class TORCH_API ProfilerState {
     CPU, // CPU-only profiling
     CUDA, // CPU + CUDA events
     NVTX,  // only emit NVTX markers
+    ITT, // only emit ITT markers
 };
 
 struct TORCH_API ProfilerConfig {
diff --git a/torch/csrc/autograd/profiler_itt.cpp b/torch/csrc/autograd/profiler_itt.cpp
new file mode 100644
index 0000000..2c0476b
--- /dev/null
+++ b/torch/csrc/autograd/profiler_itt.cpp
@@ -0,0 +1,33 @@
+#include <torch/csrc/autograd/profiler.h>
+#include <torch/csrc/itt_wrapper.h>
+
+namespace torch { namespace autograd { namespace profiler {
+
+namespace {
+struct ITTMethods : public ITTStubs {
+  void ittMark(const char* name) override {
+	torch::itt_mark(name);
+  }
+  void ittRangePush(const char* name) override {
+    torch::itt_range_push(name);
+  }
+  void ittRangePop() override {
+    torch::itt_range_pop();
+  }
+  bool enabled() override {
+    return true;
+  }
+};
+
+struct RegisterITTMethods {
+  RegisterITTMethods() {
+    static ITTMethods methods;
+    registerITTMethods(&methods);
+  }
+};
+RegisterITTMethods reg;
+
+} // namespaces
+} // namespace profiler
+} // namespace autograd
+} // namespace torch
\ No newline at end of file
diff --git a/torch/csrc/itt.cpp b/torch/csrc/itt.cpp
new file mode 100644
index 0000000..2a0d2ff
--- /dev/null
+++ b/torch/csrc/itt.cpp
@@ -0,0 +1,13 @@
+#include <torch/csrc/utils/pybind.h>
+#include <torch/csrc/itt_wrapper.h>
+
+namespace torch {
+void initIttBindings(PyObject* module) {
+  auto m = py::handle(module).cast<py::module>();
+
+  auto itt = m.def_submodule("_itt", "VTune ITT bindings");
+  itt.def("rangePush", itt_range_push);
+  itt.def("rangePop", itt_range_pop);
+  itt.def("mark", itt_mark);
+}
+} // namespace torch
\ No newline at end of file
diff --git a/torch/csrc/itt_wrapper.cpp b/torch/csrc/itt_wrapper.cpp
new file mode 100644
index 0000000..8e6efc0
--- /dev/null
+++ b/torch/csrc/itt_wrapper.cpp
@@ -0,0 +1,21 @@
+#include "ittnotify.h"
+
+namespace torch {
+__itt_domain* _itt_domain = __itt_domain_create("PyTorch");
+
+void itt_range_push(const char* msg) {
+	__itt_string_handle* hsMsg = __itt_string_handle_create(msg);
+	__itt_task_begin(_itt_domain, __itt_null, __itt_null, hsMsg);
+}
+
+void itt_range_pop() {
+	__itt_task_end(_itt_domain);
+}
+
+void itt_mark(const char* msg) {
+	__itt_string_handle* hsMsg = __itt_string_handle_create(msg);
+	__itt_task_begin(_itt_domain, __itt_null, __itt_null, hsMsg);
+	__itt_task_end(_itt_domain);
+}
+
+} // namespace torch
\ No newline at end of file
diff --git a/torch/csrc/itt_wrapper.h b/torch/csrc/itt_wrapper.h
new file mode 100644
index 0000000..9dff601
--- /dev/null
+++ b/torch/csrc/itt_wrapper.h
@@ -0,0 +1,10 @@
+#ifndef PROFILER_ITT_H
+#define PROFILER_ITT_H
+
+namespace torch {
+void itt_range_push(const char* msg);
+void itt_range_pop();
+void itt_mark(const char* msg);
+} // namespace torch
+
+#endif // PROFILER_ITT_H
diff --git a/torch/itt.py b/torch/itt.py
new file mode 100644
index 0000000..853ce61
--- /dev/null
+++ b/torch/itt.py
@@ -0,0 +1,39 @@
+try:
+    from torch._C import _itt
+except ImportError:
+    class _ITTStub(object):
+        @staticmethod
+        def _fail(*args, **kwargs):
+            raise RuntimeError("ITT functions not installed. Are you sure you have a ITT build?")
+
+        rangePush = _fail
+        rangePop = _fail
+        mark = _fail
+
+    _itt = _ITTStub()
+
+
+__all__ = ['range_push', 'range_pop', 'mark']
+
+
+def range_push(msg):
+    """
+    Arguments:
+        msg (string): ASCII message to associate with range
+    """
+    return _itt.rangePush(msg)
+
+
+def range_pop():
+    """
+    """
+    return _itt.rangePop()
+
+
+def mark(msg):
+    """
+    Describe an instantaneous event that occurred at some point.
+    Arguments:
+        msg (string): ASCII message to associate with the event.
+    """
+    return _itt.mark(msg)
