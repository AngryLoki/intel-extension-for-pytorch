From 15ed3e894b3d11f67d4ee7d3e6d94696674d40fb Mon Sep 17 00:00:00 2001
From: Jinghui <jinghui.gu@intel.com>
Date: Mon, 25 Jul 2022 12:01:01 +0800
Subject: [PATCH 16/28] Add fused addmm path in linear for contiguous 3D input
 (#72728) (#57)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72728

If the input is 3D and contiguous, we can get a fused addmm by reshaping.
ghstack-source-id: 152278479

Test Plan: existing tests?

Reviewed By: zrphercule

Differential Revision: D34176407

fbshipit-source-id: 899f216cadcd782c3b1b046025228df04228c740
(cherry picked from commit e601c5a512baac3791aa6d514d784234275a8f03)

Co-authored-by: Scott Wolchok <swolchok@fb.com>
---
 aten/src/ATen/native/Linear.cpp | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/aten/src/ATen/native/Linear.cpp b/aten/src/ATen/native/Linear.cpp
index 3a4a8e1fd7..847a2dab5e 100644
--- a/aten/src/ATen/native/Linear.cpp
+++ b/aten/src/ATen/native/Linear.cpp
@@ -34,6 +34,12 @@ Tensor linear(const Tensor& input, const Tensor& weight, const c10::optional<Ten
     // Fused op is marginally faster.
     return at::addmm(*bias, input, weight.t());
   }
+  if (input.dim() == 3 && bias->defined() && input.is_contiguous()) {
+    // Also hit the fused path for contiguous 3D input.
+    const auto input_sizes = input.sizes();
+    const auto result = at::addmm(*bias, input.view({input_sizes[0] * input_sizes[1], input_sizes[2]}), weight.t());
+    return result.view({input_sizes[0], input_sizes[1], result.size(1)});
+  }
   auto output = at::matmul(input, weight.t());
   if (bias->defined()) {
     output.add_(*bias);
-- 
2.25.1

