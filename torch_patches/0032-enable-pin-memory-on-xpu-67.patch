From ec76f1d45f4affe39aa8a284a368f7f92eb06401 Mon Sep 17 00:00:00 2001
From: leizhenyuan <110007460+leizhenyuan@users.noreply.github.com>
Date: Sat, 8 Oct 2022 14:39:39 +0800
Subject: [PATCH 32/36] enable pin memory on xpu (#67)

* enable pin memory on xpu
---
 torch/_utils.py                       |  4 ++--
 torch/utils/data/_utils/pin_memory.py | 18 +++++++++++++-----
 torch/utils/data/dataloader.py        | 16 +++++++++++-----
 3 files changed, 26 insertions(+), 12 deletions(-)

diff --git a/torch/_utils.py b/torch/_utils.py
index bb018664eb..7f245e352f 100644
--- a/torch/_utils.py
+++ b/torch/_utils.py
@@ -437,7 +437,7 @@ class ExceptionWrapper(object):
 def _get_available_device_type():
     if torch.cuda.is_available():
         return "cuda"
-    if hasattr(torch, "xpu") and torch.xpu.is_available():
+    if hasattr(torch, "xpu") and torch.xpu.is_available():  # type: ignore[attr-defined]
         return "xpu"
     # add more available device types here
     return None
@@ -448,7 +448,7 @@ def _get_device_attr(get_member):
     if device_type and device_type.lower() == "cuda":
         return get_member(torch.cuda)
     elif device_type and device_type.lower() == "xpu":
-        return get_member(torch.xpu)
+        return get_member(torch.xpu)  # type: ignore[attr-defined]
     # add more available device types here
     return None
 
diff --git a/torch/utils/data/_utils/pin_memory.py b/torch/utils/data/_utils/pin_memory.py
index f17aae9223..f621d44af4 100644
--- a/torch/utils/data/_utils/pin_memory.py
+++ b/torch/utils/data/_utils/pin_memory.py
@@ -19,8 +19,13 @@ def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
     # consuming all CPU cores.
     torch.set_num_threads(1)
 
-    torch.cuda.set_device(device_id)
+    device_type = torch._utils._get_available_device_type()
 
+    if device_type == "cuda":
+        torch.cuda.set_device(device_id)
+    elif device_type == "xpu":
+        torch.xpu.set_device(device_id)  # type: ignore[attr-defined]
+    
     # See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for details on the
     # logic of this function.
     while not done_event.is_set():
@@ -33,8 +38,8 @@ def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
             try:
                 data = pin_memory(data)
             except Exception:
-                data = ExceptionWrapper(
-                    where="in pin memory thread for device {}".format(device_id))
+                    data = ExceptionWrapper(
+                    where="in pin memory thread for device_type {} device_id {}".format(device_type, device_id))
             r = (idx, data)
         while not done_event.is_set():
             try:
@@ -46,8 +51,11 @@ def _pin_memory_loop(in_queue, out_queue, device_id, done_event):
 
 
 def pin_memory(data):
+    
+    device_type = torch._utils._get_available_device_type()
+    
     if isinstance(data, torch.Tensor):
-        return data.pin_memory()
+        return data.pin_memory(device_type)
     elif isinstance(data, string_classes):
         return data
     elif isinstance(data, collections.abc.Mapping):
@@ -57,6 +65,6 @@ def pin_memory(data):
     elif isinstance(data, collections.abc.Sequence):
         return [pin_memory(sample) for sample in data]
     elif hasattr(data, "pin_memory"):
-        return data.pin_memory()
+        return data.pin_memory(device_type)
     else:
         return data
diff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py
index 0f46ad283e..886a45a116 100644
--- a/torch/utils/data/dataloader.py
+++ b/torch/utils/data/dataloader.py
@@ -5,6 +5,7 @@ functions to be run in multiprocessing. E.g., the data loading worker loop is
 in `./_utils/worker.py`.
 """
 
+from ast import IsNot
 import os
 import threading
 import itertools
@@ -491,7 +492,8 @@ class _BaseDataLoaderIter(object):
         self._index_sampler = loader._index_sampler
         self._num_workers = loader.num_workers
         self._prefetch_factor = loader.prefetch_factor
-        self._pin_memory = loader.pin_memory and torch.cuda.is_available()
+        self._device_type = torch._utils._get_available_device_type()
+        self._pin_memory = loader.pin_memory and not (self._device_type is None)
         self._timeout = loader.timeout
         self._collate_fn = loader.collate_fn
         self._sampler_iter = iter(self._index_sampler)
@@ -892,7 +894,6 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
         self._worker_pids_set = False
         self._shutdown = False
         self._workers_done_event = multiprocessing_context.Event()
-
         self._index_queues = []
         self._workers = []
         for i in range(self._num_workers):
@@ -924,11 +925,16 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
 
             # Queue is not type-annotated
             self._data_queue = queue.Queue()  # type: ignore[var-annotated]
+            if self._device_type == "cuda":
+                current_device = torch.cuda.current_device()
+            elif self._device_type == "xpu":
+                current_device = torch.xpu.current_device()  # type: ignore[attr-defined]
+            pin_args=(self._worker_result_queue, self._data_queue, current_device, 
+                    self._pin_memory_thread_done_event)
+            
             pin_memory_thread = threading.Thread(
                 target=_utils.pin_memory._pin_memory_loop,
-                args=(self._worker_result_queue, self._data_queue,
-                      torch.cuda.current_device(),
-                      self._pin_memory_thread_done_event))
+                args=pin_args)
             pin_memory_thread.daemon = True
             pin_memory_thread.start()
             # Similar to workers (see comment above), we only register
-- 
2.25.1

