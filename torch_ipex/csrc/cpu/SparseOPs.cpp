// Autogenerated file by gen-cpu-ops.py. Do not edit directly!
#include "SparseOPs.h"
#include "aten_ipex_bridge.h"
#include "ipex_sparse_tensor_impl.h"

namespace torch_ipex {
namespace cpu {

at::Tensor AtenIpexCPUSparse::add(const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::add(_ipex_self, _ipex_other, alpha);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::add_(at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = _ipex_self.add_(_ipex_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::add_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::add_out(_ipex_out, _ipex_self, _ipex_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::div(const at::Tensor & self, const at::Tensor & other) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::div(_ipex_self, _ipex_other);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::div_(at::Tensor & self, const at::Tensor & other) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = _ipex_self.div_(_ipex_other);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::div_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::div_out(_ipex_out, _ipex_self, _ipex_other);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::empty(at::IntArrayRef size, const at::TensorOptions & options, c10::optional<at::MemoryFormat> memory_format) {
  TORCH_INTERNAL_ASSERT(options.device().type() == at::DeviceType::DPCPP);
  at::TensorOptions _ipex_options = options.device(at::DeviceType::CPU);
  if (memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous)
      TORCH_WARN(memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous);
  auto&& _ipex_result = at::empty(size, _ipex_options, memory_format);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::log1p_(at::Tensor & self) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::log1p_(_ipex_self);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::log1p_out(at::Tensor & out, const at::Tensor & self) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::log1p_out(_ipex_out, _ipex_self);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::mm(const at::Tensor & self, const at::Tensor & mat2) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::mm(_ipex_self, _ipex_mat2);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::mm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::mm_out(_ipex_out, _ipex_self, _ipex_mat2);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::mul(const at::Tensor & self, const at::Tensor & other) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::mul(_ipex_self, _ipex_other);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::mul_(at::Tensor & self, const at::Tensor & other) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = _ipex_self.mul_(_ipex_other);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::mul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::mul_out(_ipex_out, _ipex_self, _ipex_other);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = _ipex_self.narrow_copy(dim, start, length);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::sspaddmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::sspaddmm_out(_ipex_out, _ipex_self, _ipex_mat1, _ipex_mat2, beta, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::native_norm(const at::Tensor & self, at::Scalar p) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::native_norm(_ipex_self, p);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor AtenIpexCPUSparse::_sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
  auto&& _ipex_grad = bridge::shallowFallbackToCPUTensor(grad);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::_sparse_sum_backward(_ipex_grad, _ipex_self, dim);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor AtenIpexCPUSparse::clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
  if (memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous)
      TORCH_WARN(memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::clone(_ipex_self, memory_format);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::pow_out(at::Tensor & out, const at::Tensor & self, at::Scalar exponent) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::pow_out(_ipex_out, _ipex_self, exponent);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::pow(const at::Tensor & self, at::Scalar exponent) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::pow(_ipex_self, exponent);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::zero_(at::Tensor & self) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = at::zero_(_ipex_self);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::sub_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::sub_out(_ipex_out, _ipex_self, _ipex_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::sub(const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = at::sub(_ipex_self, _ipex_other, alpha);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::sub_(at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _ipex_result = _ipex_self.sub_(_ipex_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::addmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::addmm_out(_ipex_out, _ipex_self, _ipex_mat1, _ipex_mat2, beta, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::addmm(_ipex_self, _ipex_mat1, _ipex_mat2, beta, alpha);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = _ipex_self.addmm_(_ipex_mat1, _ipex_mat2, beta, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenIpexCPUSparse::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::TensorOptions & options) {
  TORCH_INTERNAL_ASSERT(options.device().type() == at::DeviceType::DPCPP);
  at::TensorOptions _ipex_options = options.device(at::DeviceType::CPU);
  auto&& _ipex_result = at::_sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, _ipex_options);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor AtenIpexCPUSparse::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, const at::TensorOptions & options) {
  TORCH_INTERNAL_ASSERT(options.device().type() == at::DeviceType::DPCPP);
  at::TensorOptions _ipex_options = options.device(at::DeviceType::CPU);
  auto&& _ipex_indices = bridge::shallowFallbackToCPUTensor(indices);
  auto&& _ipex_values = bridge::shallowFallbackToCPUTensor(values);
  auto&& _ipex_result = at::_sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, _ipex_indices, _ipex_values, _ipex_options);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::sparse_resize_(at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = _ipex_self.sparse_resize_(size, sparse_dim, dense_dim);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::sparse_resize_and_clear_(at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = _ipex_self.sparse_resize_and_clear_(size, sparse_dim, dense_dim);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenIpexCPUSparse::sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_mask = bridge::shallowFallbackToCPUTensor(mask);
  auto&& _ipex_result = _ipex_self.sparse_mask(_ipex_mask);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor AtenIpexCPUSparse::to_dense(const at::Tensor & self) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = _ipex_self.to_dense();
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor AtenIpexCPUSparse::coalesce(const at::Tensor & self) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_result = _ipex_self.coalesce();
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::hspmm_out(at::Tensor & out, const at::Tensor & mat1, const at::Tensor & mat2) {
  auto&& _ipex_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _ipex_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::hspmm_out(_ipex_out, _ipex_mat1, _ipex_mat2);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _ipex_out);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
  auto&& _ipex_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _ipex_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _ipex_result = at::hspmm(_ipex_mat1, _ipex_mat2);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}

at::Tensor & AtenIpexCPUSparse::copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_src = bridge::shallowFallbackToCPUTensor(src);
  auto&& _ipex_result = at::copy_sparse_to_sparse_(_ipex_self, _ipex_src, non_blocking);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _ipex_self);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenIpexCPUSparse::index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  auto&& _ipex_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _ipex_index = bridge::shallowFallbackToCPUTensor(index);
  auto&& _ipex_result = at::index_select(_ipex_self, dim, _ipex_index);
  static_cast<void>(_ipex_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_ipex_result);
}



}  // namespace cpu
}  // namespace torch_ipex

