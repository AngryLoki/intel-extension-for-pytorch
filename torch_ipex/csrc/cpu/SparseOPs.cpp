// Autogenerated file by gen-cpu-sparse-ops.py. Do not edit directly!
#include "SparseOPs.h"
#include "aten_ipex_bridge.h"
#include "ipex_sparse_tensor_impl.h"

namespace torch_ipex {
namespace cpu {

at::Tensor AtenIpexCPUSparse::add(const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::add(_cpu_self, _cpu_other, alpha);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::add_(at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = _cpu_self.add_(_cpu_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::add_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::add_out(_cpu_out, _cpu_self, _cpu_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::div(const at::Tensor & self, const at::Tensor & other) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::div(_cpu_self, _cpu_other);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::div_(at::Tensor & self, const at::Tensor & other) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = _cpu_self.div_(_cpu_other);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::div_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::div_out(_cpu_out, _cpu_self, _cpu_other);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::empty(at::IntArrayRef size, const at::TensorOptions & options, c10::optional<at::MemoryFormat> memory_format) {
  TORCH_INTERNAL_ASSERT(options.device().type() == at::DeviceType::DPCPP);
  at::TensorOptions _cpu_options = options.device(at::DeviceType::CPU);
  if (memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous)
      TORCH_WARN(memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous);
  auto&& _cpu_result = at::empty(size, _cpu_options, memory_format);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::log1p_(at::Tensor & self) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::log1p_(_cpu_self);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::log1p_out(at::Tensor & out, const at::Tensor & self) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::log1p_out(_cpu_out, _cpu_self);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::mm(const at::Tensor & self, const at::Tensor & mat2) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::mm(_cpu_self, _cpu_mat2);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::mm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat2) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::mm_out(_cpu_out, _cpu_self, _cpu_mat2);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::mul(const at::Tensor & self, const at::Tensor & other) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::mul(_cpu_self, _cpu_other);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::mul_(at::Tensor & self, const at::Tensor & other) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = _cpu_self.mul_(_cpu_other);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::mul_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::mul_out(_cpu_out, _cpu_self, _cpu_other);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::narrow_copy(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = _cpu_self.narrow_copy(dim, start, length);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::sspaddmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::sspaddmm_out(_cpu_out, _cpu_self, _cpu_mat1, _cpu_mat2, beta, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::native_norm(const at::Tensor & self, at::Scalar p) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::native_norm(_cpu_self, p);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor AtenIpexCPUSparse::_sparse_sum_backward(const at::Tensor & grad, const at::Tensor & self, at::IntArrayRef dim) {
  auto&& _cpu_grad = bridge::shallowFallbackToCPUTensor(grad);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::_sparse_sum_backward(_cpu_grad, _cpu_self, dim);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor AtenIpexCPUSparse::clone(const at::Tensor & self, c10::optional<at::MemoryFormat> memory_format) {
  if (memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous)
      TORCH_WARN(memory_format.value_or(c10::MemoryFormat::Contiguous) != c10::MemoryFormat::Contiguous);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::clone(_cpu_self, memory_format);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::pow_out(at::Tensor & out, const at::Tensor & self, at::Scalar exponent) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::pow_out(_cpu_out, _cpu_self, exponent);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::pow(const at::Tensor & self, at::Scalar exponent) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::pow(_cpu_self, exponent);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::zero_(at::Tensor & self) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = at::zero_(_cpu_self);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::sub_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::sub_out(_cpu_out, _cpu_self, _cpu_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::sub(const at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = at::sub(_cpu_self, _cpu_other, alpha);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::sub_(at::Tensor & self, const at::Tensor & other, at::Scalar alpha) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_other = bridge::shallowFallbackToCPUTensor(other);
  auto&& _cpu_result = _cpu_self.sub_(_cpu_other, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::addmm_out(at::Tensor & out, const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::addmm_out(_cpu_out, _cpu_self, _cpu_mat1, _cpu_mat2, beta, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::addmm(_cpu_self, _cpu_mat1, _cpu_mat2, beta, alpha);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::addmm_(at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, at::Scalar beta, at::Scalar alpha) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = _cpu_self.addmm_(_cpu_mat1, _cpu_mat2, beta, alpha);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenIpexCPUSparse::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::TensorOptions & options) {
  TORCH_INTERNAL_ASSERT(options.device().type() == at::DeviceType::DPCPP);
  at::TensorOptions _cpu_options = options.device(at::DeviceType::CPU);
  auto&& _cpu_result = at::_sparse_coo_tensor_with_dims(sparse_dim, dense_dim, size, _cpu_options);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor AtenIpexCPUSparse::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, const at::Tensor & indices, const at::Tensor & values, const at::TensorOptions & options) {
  TORCH_INTERNAL_ASSERT(options.device().type() == at::DeviceType::DPCPP);
  at::TensorOptions _cpu_options = options.device(at::DeviceType::CPU);
  auto&& _cpu_indices = bridge::shallowFallbackToCPUTensor(indices);
  auto&& _cpu_values = bridge::shallowFallbackToCPUTensor(values);
  auto&& _cpu_result = at::_sparse_coo_tensor_with_dims_and_tensors(sparse_dim, dense_dim, size, _cpu_indices, _cpu_values, _cpu_options);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::sparse_resize_(at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = _cpu_self.sparse_resize_(size, sparse_dim, dense_dim);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor & AtenIpexCPUSparse::sparse_resize_and_clear_(at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = _cpu_self.sparse_resize_and_clear_(size, sparse_dim, dense_dim);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenIpexCPUSparse::sparse_mask(const at::Tensor & self, const at::Tensor & mask) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_mask = bridge::shallowFallbackToCPUTensor(mask);
  auto&& _cpu_result = _cpu_self.sparse_mask(_cpu_mask);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor AtenIpexCPUSparse::to_dense(const at::Tensor & self) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = _cpu_self.to_dense();
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor AtenIpexCPUSparse::coalesce(const at::Tensor & self) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_result = _cpu_self.coalesce();
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::hspmm_out(at::Tensor & out, const at::Tensor & mat1, const at::Tensor & mat2) {
  auto&& _cpu_out = bridge::shallowFallbackToCPUTensor(out);
  auto&& _cpu_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::hspmm_out(_cpu_out, _cpu_mat1, _cpu_mat2);
  bridge::shallowUpgradeToDPCPPTensorAW(out, _cpu_out);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return out;
}

at::Tensor AtenIpexCPUSparse::hspmm(const at::Tensor & mat1, const at::Tensor & mat2) {
  auto&& _cpu_mat1 = bridge::shallowFallbackToCPUTensor(mat1);
  auto&& _cpu_mat2 = bridge::shallowFallbackToCPUTensor(mat2);
  auto&& _cpu_result = at::hspmm(_cpu_mat1, _cpu_mat2);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}

at::Tensor & AtenIpexCPUSparse::copy_sparse_to_sparse_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_src = bridge::shallowFallbackToCPUTensor(src);
  auto&& _cpu_result = at::copy_sparse_to_sparse_(_cpu_self, _cpu_src, non_blocking);
  bridge::shallowUpgradeToDPCPPTensorAW(self, _cpu_self);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return self;
}

at::Tensor AtenIpexCPUSparse::index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
  auto&& _cpu_self = bridge::shallowFallbackToCPUTensor(self);
  auto&& _cpu_index = bridge::shallowFallbackToCPUTensor(index);
  auto&& _cpu_result = at::index_select(_cpu_self, dim, _cpu_index);
  static_cast<void>(_cpu_result); // Avoid warnings in case not used
  return bridge::shallowUpgradeToDPCPPTensor(_cpu_result);
}



}  // namespace cpu
}  // namespace torch_ipex

