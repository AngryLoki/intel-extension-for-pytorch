<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Intel® Extension for PyTorch* optimizations for quantization (Experimental) &mdash; intel_extension_for_pytorch 1.11.0+cpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Releases" href="../releases.html" />
    <link rel="prev" title="Runtime Extension (Experimental)" href="runtime_extension.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../../versions.html">1.11.0+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#ease-of-use-python-api">Ease-of-use Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#channels-last">Channels Last</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#operator-optimization">Operator Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#optimizer-optimization">Optimizer Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#runtime-extension-experimental">Runtime Extension (Experimental)</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#int8-quantization-experimental">INT8 Quantization (Experimental)</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Intel® Extension for PyTorch* optimizations for quantization (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#calibration-step">Calibration Step</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-conversion">Model Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evaluate">Evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deploy-the-converted-model">Deploy the Converted Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#additional-context">Additional context</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#integration-with-onednn-graph-api">Integration with oneDNN graph API</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l6"><a class="reference internal" href="#graph-executor">Graph Executor</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="#limitations">Limitations</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#support-for-dynamic-shapes">Support for dynamic shapes</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../features.html">Features</a> &raquo;</li>
      <li>Intel® Extension for PyTorch* optimizations for quantization (Experimental)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/int8.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="intel-extension-for-pytorch-optimizations-for-quantization-experimental">
<h1>Intel® Extension for PyTorch* optimizations for quantization (Experimental)<a class="headerlink" href="#intel-extension-for-pytorch-optimizations-for-quantization-experimental" title="Permalink to this headline"></a></h1>
<p>The quantization functionality in Intel® Extension for PyTorch* currently only supports post-training static quantization. This tutorial introduces how the static quantization works in the Intel® Extension for PyTorch* side.</p>
<p>Suppose there is a model as below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># user dataset for calibration.</span>
<span class="n">xx_c</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># user dataset for validation.</span>
<span class="n">xx_v</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
<section id="calibration-step">
<h2>Calibration Step<a class="headerlink" href="#calibration-step" title="Permalink to this headline"></a></h2>
<p>Similar to the steps at PyTorch side, the first step is to perform calibration step to collect distributions of different activations. The distributions is then used to divide the entire range of activations into 256 levels.</p>
<p>At first, we need to define the quantization configuration determining which quantization scheme to be used for activation. Two values are supported: <code class="docutils literal notranslate"><span class="pre">torch.per_tensor_affine</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.per_tensor_symmetric</span></code>. The default qscheme is <code class="docutils literal notranslate"><span class="pre">torch.per_tensor_affine</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">)</span>
</pre></div>
</div>
<p>then perform calibration using the calibration dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_c</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">conf</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">conf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In the last line, a <code class="docutils literal notranslate"><span class="pre">.json</span></code> file is saved. The file contains info of quantization, such as observer algorithm, activations, and weights scales:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
        <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;conv2d&quot;</span><span class="p">,</span>
        <span class="nt">&quot;algorithm&quot;</span><span class="p">:</span> <span class="s2">&quot;min_max&quot;</span><span class="p">,</span>
        <span class="nt">&quot;weight_granularity&quot;</span><span class="p">:</span> <span class="s2">&quot;per_channel&quot;</span><span class="p">,</span>
        <span class="nt">&quot;input_scales&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="mf">0.02742583677172661</span>
        <span class="p">],</span>
        <span class="nt">&quot;input_zero_points&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="mi">125</span>
        <span class="p">],</span>
        <span class="nt">&quot;output_scales&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="mf">0.01582648977637291</span>
        <span class="p">],</span>
        <span class="nt">&quot;output_zero_points&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="mi">120</span>
        <span class="p">],</span>
        <span class="nt">&quot;weight_scales&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">[</span>
                <span class="mf">0.0008243077900260687</span><span class="p">,</span>
                <span class="mf">0.0008239267044700682</span><span class="p">,</span>
                <span class="mf">0.0008076696540229023</span><span class="p">,</span>
                <span class="mf">0.000826483650598675</span><span class="p">,</span>
                <span class="mf">0.0008274353458546102</span><span class="p">,</span>
                <span class="mf">0.0008290993282571435</span><span class="p">,</span>
                <span class="mf">0.0007878943579271436</span><span class="p">,</span>
                <span class="mf">0.0008173943497240543</span><span class="p">,</span>
                <span class="mf">0.0008244941127486527</span><span class="p">,</span>
                <span class="mf">0.0008231988758780062</span>
            <span class="p">]</span>
        <span class="p">],</span>
        <span class="nt">&quot;input_quantized_dtypes&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;uint8&quot;</span>
        <span class="p">],</span>
        <span class="nt">&quot;output_quantized_dtypes&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;uint8&quot;</span>
        <span class="p">],</span>
        <span class="nt">&quot;inputs_quantized&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="kc">true</span>
        <span class="p">],</span>
        <span class="nt">&quot;outputs_quantized&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="kc">false</span>
        <span class="p">],</span>
        <span class="nt">&quot;inputs_flow&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;conv2d0.input0&quot;</span>
        <span class="p">],</span>
        <span class="nt">&quot;outputs_flow&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;conv2d0.output0&quot;</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Description of the json file can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/blob/master/intel_extension_for_pytorch/quantization/conf.py">conf.py</a>.</p>
</section>
<section id="model-conversion">
<h2>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline"></a></h2>
<p>After doing calibration steps, distributions of activations and weights are collected. The model can be converted to a quantized model with these info. Quantization in Intel® Extension for PyTorch* takes advantage of <a class="reference external" href="https://spec.oneapi.io/onednn-graph/latest/introduction.html">oneDNN graph API</a>. This requires to be executed with TorchScript graph, thus, we need to convert the eager model to Torchscript model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">trace_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
</pre></div>
</div>
<p>This step inserts some quantizer(<code class="docutils literal notranslate"><span class="pre">aten::quantize_per_tensor</span></code> or <code class="docutils literal notranslate"><span class="pre">aten::dequantize</span></code>) in the model. Meanwhile, <a class="reference external" href="https://spec.oneapi.io/onednn-graph/latest/introduction.html">oneDNN graph API</a> will do graph optimization to replace some quantization pattens with quantization operators. More details can be found at <a class="reference internal" href="graph_optimization.html"><span class="doc">graph_optimization.md</span></a>.</p>
</section>
<section id="evaluate">
<h2>Evaluate<a class="headerlink" href="#evaluate" title="Permalink to this headline"></a></h2>
<p>After doing model conversion, we can do the evaluation step with your dataset by using the converted model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_v</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">trace_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deploy-the-converted-model">
<h2>Deploy the Converted Model<a class="headerlink" href="#deploy-the-converted-model" title="Permalink to this headline"></a></h2>
<p>If you want to deploy your model on another device, you need to save the converted model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">trace_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;quantization_model.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>and then load the saved model on your target device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="n">loaded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;quantization_model.pt&#39;</span><span class="p">)</span>
<span class="c1"># running the model using your dataset</span>
</pre></div>
</div>
</section>
<section id="additional-context">
<h2>Additional context<a class="headerlink" href="#additional-context" title="Permalink to this headline"></a></h2>
<section id="integration-with-onednn-graph-api">
<h3>Integration with oneDNN graph API<a class="headerlink" href="#integration-with-onednn-graph-api" title="Permalink to this headline"></a></h3>
<p>The quantization in Intel® Extension for PyTorch* integrates <a class="reference external" href="https://spec.oneapi.io/onednn-graph/latest/introduction.html">oneDNN graph API</a> with TorchScript graph of PyTorch.</p>
<p>The integration is mainly composed of the Graph Optimization part and the Graph Executor part:</p>
<section id="graph-optimization">
<h4>Graph Optimization<a class="headerlink" href="#graph-optimization" title="Permalink to this headline"></a></h4>
<p>We have registered quantization-related optimization passes in the Custom Pre-passes set of PyTorch:</p>
<ol>
<li><p>Alias and mutation reduction</p>
<p>Operators of oneDNN graph are pure functional, while PyTorch has operators in in-place forms or create views for buffer sharing.
Due to the semantic gaps between the backend operators and the PyTorch operators, we have a pass to reduce mutation with best effort at the beginning.</p>
</li>
<li><p>Graph passing</p>
<p>With a PyTorch TorchScript graph, the integration maps PyTorch operators in the graph to the corresponding backend operators to form a backend graph.</p>
</li>
<li><p>Partitioning</p>
<p>The backend selects regions to be fused in the graph and return a list of partitions. Each partition corresponds to a fusion operator.</p>
</li>
<li><p>Graph rewriting</p>
<p>The original PyTorch graph will be re-written based on the partitions returned from the backend. The operators in one partition will be grouped together to form a JIT operator.</p>
</li>
</ol>
<p>The below diagram demonstrates the process of <code class="docutils literal notranslate"><span class="pre">Graph</span> <span class="pre">passing</span> <span class="pre">-</span> <span class="pre">Partitioning</span> <span class="pre">-</span> <span class="pre">Graph</span> <span class="pre">rewriting</span></code>:</p>
<p><img alt="image" src="../../_images/integration_diagram.PNG" /></p>
<ol>
<li><p>Layout propagation</p>
<p>This pass is to eliminate unnecessary layout conversions at boundaries. We set different formats to the output of a partition so that the backend could perform layout conversion internally. When <code class="docutils literal notranslate"><span class="pre">ANY</span></code> is set, the layout at boundaries will be fully decided by the backend. Otherwise, the backend should follow the layout set by the Framework.</p>
</li>
</ol>
<p><img alt="image" src="../../_images/layout_propagation.png" /></p>
</section>
<section id="graph-executor">
<h4>Graph Executor<a class="headerlink" href="#graph-executor" title="Permalink to this headline"></a></h4>
<p>During runtime execution of a PyTorch TorchScript graph, oneDNN graph partition will be dispatched to the oneDNN graph JIT variadic Operator.</p>
<p>Inside the oneDNN graph JIT Op, input PyTorch tensors of each partition will be mapped to oneDNN graph tensors. The partition will then be <a class="reference external" href="https://spec.oneapi.io/onednn-graph/latest/programming_model.html#partition">compiled</a> and <a class="reference external" href="https://spec.oneapi.io/onednn-graph/latest/programming_model.html#compiled-partition">executed</a>. The output oneDNN graph tensor will be mapped back to PyTorch tensors to be fed to the next operator on the TorchScript graph.</p>
</section>
</section>
<section id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline"></a></h3>
<section id="support-for-dynamic-shapes">
<h4>Support for dynamic shapes<a class="headerlink" href="#support-for-dynamic-shapes" title="Permalink to this headline"></a></h4>
<p>The support for dynamic shapes in Intel® Extension for PyTorch* int8 integration is still working in progress.</p>
<p>For the use cases where the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* int8 path may slow down the model inference.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_extension.html" class="btn btn-neutral float-left" title="Runtime Extension (Experimental)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../releases.html" class="btn btn-neutral float-right" title="Releases" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>