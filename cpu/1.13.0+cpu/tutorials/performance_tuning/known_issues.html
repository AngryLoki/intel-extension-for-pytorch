<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Known Issues &mdash; intel_extension_for_pytorch 1.13.0+cpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Blogs &amp; Publications" href="../blogs_publications.html" />
    <link rel="prev" title="TorchServe with Intel® Extension for PyTorch*" href="torchserve.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="/intel-extension-for-pytorch/">1.13.0+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../performance_tuning.html">Performance Tuning Guide</a> &raquo;</li>
      <li>Known Issues</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/known_issues.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="known-issues">
<h1>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading"></a></h1>
<ul>
<li><p>If you found the workload runs with Intel® Extension for PyTorch* occupies a remarkably large amount of memory, you can try to reduce the occupied memory size by setting the <code class="docutils literal notranslate"><span class="pre">--weights_prepack</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>Supporting of EmbeddingBag with INT8 when bag size &gt; 1 is working in progress.</p></li>
<li><p>Compiling with gcc 11 might result in <code class="docutils literal notranslate"><span class="pre">illegal</span> <span class="pre">instruction</span></code> error.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Overflow</span> <span class="pre">when</span> <span class="pre">unpacking</span> <span class="pre">long</span></code> when a tensor’s min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.</p></li>
<li><p>For models with dynamic control flow, please try dynamic quantization. Users are likely to get performance gain for GEMM models.</p></li>
<li><p>Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP* thread, results might be incorrect with large tensors (find more detailed info <a class="reference external" href="https://github.com/pytorch/pytorch/issues/80501">here</a>. Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.</p>
<p>Workaround pseudocode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># perform convert/trace/freeze with omp_num_threads &gt; 1(N)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">converted_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">freezed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># run freezed model to apply optimization pass</span>
<span class="n">freezed_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># benchmarking with omp_num_threads = 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">run_benchmark</span><span class="p">(</span><span class="n">freezed_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains <code class="docutils literal notranslate"><span class="pre">Conv</span></code>, <code class="docutils literal notranslate"><span class="pre">Matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, and <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code></p></li>
<li><p>Runtime extension of MultiStreamModule doesn’t support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can’t be simplely batch split.</p></li>
<li><p>Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, <code class="docutils literal notranslate"><span class="pre">joint_net</span></code> is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.</p></li>
<li><p>Incorrect Conv and Linear result if the number of OMP threads is changed at runtime</p>
<p>The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.</p>
</li>
<li><p>Low performance with INT8 support for dynamic shapes</p>
<p>The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.</p>
<p><strong>Note</strong>: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.</p>
</li>
<li><p>Low throughput with DLRM FP32 Train</p>
<p>A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending on review. The issue will be fixed when the PR is merged.</p>
</li>
<li><p>If inference is done with a custom function, <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding feature of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function doesn’t work.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_pytorch_extension</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="k">class</span> <span class="nc">Module</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Module</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">m</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a PyTorch FX limitation. You can avoid this error by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O0&quot;)</span></code>, which doesn’t apply ipex optimization, or disable <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O1&quot;,</span> <span class="pre">conv_bn_folding=False)</span></code>.</p>
</li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchserve.html" class="btn btn-neutral float-left" title="TorchServe with Intel® Extension for PyTorch*" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../blogs_publications.html" class="btn btn-neutral float-right" title="Blogs &amp; Publications" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>