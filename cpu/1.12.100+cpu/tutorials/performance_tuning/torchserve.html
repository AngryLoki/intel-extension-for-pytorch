<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TorchServe with Intel® Extension for PyTorch* &mdash; intel_extension_for_pytorch 1.12.100 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Known Issues" href="known_issues.html" />
    <link rel="prev" title="Launch Script Usage Guide" href="launch_script.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="/intel-extension-for-pytorch/versions.html">1.12.100+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">TorchServe with Intel® Extension for PyTorch*</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#contents-of-this-document">Contents of this Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-intel-extension-for-pytorch">Install Intel Extension for PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#serving-model-with-intel-extension-for-pytorch">Serving model with Intel Extension for PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#torchserve-with-launcher">TorchServe with Launcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-and-exporting-int8-model-for-ipex">Creating and Exporting INT8 model for IPEX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-serialized-file">1. Creating a serialized file</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#bert">BERT</a></li>
<li class="toctree-l5"><a class="reference internal" href="#resnet50">ResNet50</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-model-archive">2. Creating a Model Archive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#start-torchserve-to-serve-the-model">3. Start TorchServe to serve the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#registering-and-deploying-model">4. Registering and Deploying model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#benchmarking-with-launcher">Benchmarking with Launcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-boost-with-ipex-and-launcher">Performance Boost with IPEX and Launcher</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="known_issues.html">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../performance_tuning.html">Performance Tuning Guide</a> &raquo;</li>
      <li>TorchServe with Intel® Extension for PyTorch*</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/torchserve.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="torchserve-with-intel-extension-for-pytorch">
<h1>TorchServe with Intel® Extension for PyTorch*<a class="headerlink" href="#torchserve-with-intel-extension-for-pytorch" title="Permalink to this headline"></a></h1>
<p>TorchServe can be used with Intel® Extension for PyTorch* (IPEX) to give a performance boost on Intel hardware<sup>1</sup>.
Here we show how to use TorchServe with IPEX.</p>
<p><sup>1. While Intel® Extension for PyTorch* benefits all platforms, those with AVX512 benefit the most. </sup></p>
<section id="contents-of-this-document">
<h2>Contents of this Document<a class="headerlink" href="#contents-of-this-document" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#install-intel-extension-for-pytorch">Install Intel Extension for PyTorch</a></p></li>
<li><p><a class="reference external" href="#serving-model-with-intel-extension-for-pytorch">Serving model with Intel Extension for PyTorch</a></p></li>
<li><p><a class="reference external" href="#torchserve-with-launcher">TorchServe with Launcher</a></p></li>
<li><p><a class="reference external" href="#creating-and-exporting-int8-model-for-ipex">Creating and Exporting INT8 model for IPEX</a></p></li>
<li><p><a class="reference external" href="#benchmarking-with-launcher">Benchmarking with Launcher</a></p></li>
<li><p><a class="reference external" href="#performance-boost-with-ipex-and-launcher">Performance Boost with IPEX and Launcher</a></p></li>
</ul>
</section>
<section id="install-intel-extension-for-pytorch">
<h2>Install Intel Extension for PyTorch<a class="headerlink" href="#install-intel-extension-for-pytorch" title="Permalink to this headline"></a></h2>
<p>Refer to the <a class="reference internal" href="../installation.html"><span class="doc">installation documentation</span></a>.</p>
</section>
<section id="serving-model-with-intel-extension-for-pytorch">
<h2>Serving model with Intel Extension for PyTorch<a class="headerlink" href="#serving-model-with-intel-extension-for-pytorch" title="Permalink to this headline"></a></h2>
<p>After installation, use TorchServe with IPEX by enabling it in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>Once IPEX is enabled, deploying PyTorch model follows the same procedure shown in the <a class="reference external" href="https://pytorch.org/serve/use_cases.html">PyTorch use cases documentation</a>. TorchServe with IPEX can deploy any model and do inference.</p>
</section>
<section id="torchserve-with-launcher">
<h2>TorchServe with Launcher<a class="headerlink" href="#torchserve-with-launcher" title="Permalink to this headline"></a></h2>
<p>Launcher is a script to automate tuning configuration setting on Intel hardware to boost performance. Tuning configurations such as OMP_NUM_THREADS, thread affinity, and memory allocator can have a dramatic effect on performance. Refer to the <a class="reference internal" href="tuning_guide.html"><span class="doc">Performance Tuning Guide</span></a> and <a class="reference internal" href="launch_script.html"><span class="doc">performance tuning launch script</span></a> documentation for details.</p>
<p>Enable TorchServe with launcher by setting its configuration in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<p>Add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> to use launcher with its default configuration.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>Launcher uses <code class="docutils literal notranslate"><span class="pre">numactl</span></code> if it’s installed to ensure a socket is pinned and thus memory is allocated from local numa node. To use launcher without numactl, add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">disable_numactl</span>
</pre></div>
</div>
<p>Launcher by default uses only non-hyperthreaded cores to avoid core compute resource sharing. To use launcher with all cores, both physical and logical (hyperthreaded), add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">use_logical_core</span>
</pre></div>
</div>
<p>Below is an example of passing multiple args to <code class="docutils literal notranslate"><span class="pre">cpu_launcher_args</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">use_logical_core</span> <span class="o">--</span><span class="n">disable_numactl</span>
</pre></div>
</div>
<p>Some useful <code class="docutils literal notranslate"><span class="pre">cpu_launcher_args</span></code> to note are:</p>
<ol class="simple">
<li><p>Memory Allocator: [ PTMalloc <code class="docutils literal notranslate"><span class="pre">--use_default_allocator</span></code> | <em>TCMalloc <code class="docutils literal notranslate"><span class="pre">--enable_tcmalloc</span></code></em> | JeMalloc <code class="docutils literal notranslate"><span class="pre">--enable_jemalloc</span></code>]</p>
<ul class="simple">
<li><p>PyTorch by default uses PTMalloc. TCMalloc/JeMalloc generally gives better performance.</p></li>
</ul>
</li>
<li><p>OpenMP library: [GNU OpenMP <code class="docutils literal notranslate"><span class="pre">--disable_iomp</span></code> | <em>Intel OpenMP</em>]</p>
<ul class="simple">
<li><p>PyTorch by default uses GNU OpenMP. Launcher by default uses Intel OpenMP. Intel OpenMP library generally gives better performance.</p></li>
</ul>
</li>
<li><p>Node id: [<code class="docutils literal notranslate"><span class="pre">--node_id</span></code>]</p>
<ul class="simple">
<li><p>Launcher by default uses all physical cores. Limit memory access to local memories on the Nth socket to avoid Non-Uniform Memory Access (NUMA).</p></li>
</ul>
</li>
</ol>
<p>Refer to the <a class="reference internal" href="launch_script.html"><span class="doc">performance tuning launch script</span></a> for a full list of tunable configuration of launcher.</p>
<p>Some notable launcher configurations are:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--ninstances</span></code>: Number of instances for multi-instance inference/training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--instance_idx</span></code>: Launcher by default runs all <code class="docutils literal notranslate"><span class="pre">ninstances</span></code> when running multiple instances. Specifying <code class="docutils literal notranslate"><span class="pre">instance_idx</span></code> runs a single instance among <code class="docutils literal notranslate"><span class="pre">ninstances</span></code>. This is useful when running each instance independently.</p></li>
</ol>
<p>Refer to the <a class="reference internal" href="launch_script.html"><span class="doc">performance tuning launch script</span></a> for more details.</p>
</section>
<section id="creating-and-exporting-int8-model-for-ipex">
<h2>Creating and Exporting INT8 model for IPEX<a class="headerlink" href="#creating-and-exporting-int8-model-for-ipex" title="Permalink to this headline"></a></h2>
<p>Intel® Extension for PyTorch* supports both eager and torchscript mode. In this section, we show how to deploy INT8 model for IPEX.</p>
<section id="creating-a-serialized-file">
<h3>1. Creating a serialized file<a class="headerlink" href="#creating-a-serialized-file" title="Permalink to this headline"></a></h3>
<p>First create <code class="docutils literal notranslate"><span class="pre">.pt</span></code> serialized file using IPEX INT8 inference. Here we show two examples with BERT and ResNet50.</p>
<section id="bert">
<h4>BERT<a class="headerlink" href="#bert" title="Permalink to this headline"></a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoConfig</span>

<span class="c1"># load the model</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">torchscript</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># define dummy input tensor to use for the model&#39;s forward call to record operations in the model for tracing</span>
<span class="n">N</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">384</span>
<span class="n">dummy_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

<span class="c1"># calibration</span>
<span class="c1"># ipex supports two quantization schemes to be used for activation: torch.per_tensor_affine and torch.per_tensor_symmetric</span>
<span class="c1"># default qscheme is torch.per_tensor_affine</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">)</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">conf</span><span class="p">):</span>
            <span class="n">model</span><span class="p">(</span><span class="n">dummy_tensor</span><span class="p">,</span> <span class="n">dummy_tensor</span><span class="p">,</span> <span class="n">dummy_tensor</span><span class="p">)</span>

<span class="c1"># optionally save the configuration for later use</span>
<span class="c1"># save:</span>
<span class="c1"># conf.save(&quot;model_conf.json&quot;)</span>
<span class="c1"># load:</span>
<span class="c1"># conf = ipex.quantization.QuantConf(&quot;model_conf.json&quot;)</span>

<span class="c1"># conversion</span>
<span class="n">jit_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dummy_tensor</span><span class="p">,</span> <span class="n">dummy_tensor</span><span class="p">,</span> <span class="n">dummy_tensor</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">jit_inputs</span><span class="p">)</span>

<span class="c1"># save to .pt</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;bert_int8_jit.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="resnet50">
<h4>ResNet50<a class="headerlink" href="#resnet50" title="Permalink to this headline"></a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.fx.experimental.optimization</span> <span class="k">as</span> <span class="nn">optimization</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>

<span class="c1"># load the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">optimization</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># define dummy input tensor to use for the model&#39;s forward call to record operations in the model for tracing</span>
<span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span>
<span class="n">dummy_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>

<span class="c1"># calibration</span>
<span class="c1"># ipex supports two quantization schemes to be used for activation: torch.per_tensor_affine and torch.per_tensor_symmetric</span>
<span class="c1"># default qscheme is torch.per_tensor_affine</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span><span class="p">)</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">conf</span><span class="p">):</span>
           <span class="n">model</span><span class="p">(</span><span class="n">dummy_tensor</span><span class="p">)</span>

<span class="c1"># optionally save the configuration for later use</span>
<span class="c1"># save:</span>
<span class="c1"># conf.save(&quot;model_conf.json&quot;)</span>
<span class="c1"># load:</span>
<span class="c1"># conf = ipex.quantization.QuantConf(&quot;model_conf.json&quot;)</span>

<span class="c1"># conversion</span>
<span class="n">jit_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dummy_tensor</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">jit_inputs</span><span class="p">)</span>

<span class="c1"># save to .pt</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;rn50_int8_jit.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="creating-a-model-archive">
<h3>2. Creating a Model Archive<a class="headerlink" href="#creating-a-model-archive" title="Permalink to this headline"></a></h3>
<p>Once the serialized file ( <code class="docutils literal notranslate"><span class="pre">.pt</span></code>) is created, it can be used with <code class="docutils literal notranslate"><span class="pre">torch-model-archiver</span></code> as usual. Use the following command to package the model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">archiver</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">name</span> <span class="n">rn50_ipex_int8</span> <span class="o">--</span><span class="n">version</span> <span class="mf">1.0</span> <span class="o">--</span><span class="n">serialized</span><span class="o">-</span><span class="n">file</span> <span class="n">rn50_int8_jit</span><span class="o">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">handler</span> <span class="n">image_classifier</span>
</pre></div>
</div>
</section>
<section id="start-torchserve-to-serve-the-model">
<h3>3. Start TorchServe to serve the model<a class="headerlink" href="#start-torchserve-to-serve-the-model" title="Permalink to this headline"></a></h3>
<p>Make sure to set <code class="docutils literal notranslate"><span class="pre">ipex_enable=true</span></code> in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>. Use the following command to start TorchServe with IPEX.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torchserve</span> <span class="o">--</span><span class="n">start</span> <span class="o">--</span><span class="n">ncs</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">store</span> <span class="n">model_store</span> <span class="o">--</span><span class="n">ts</span><span class="o">-</span><span class="n">config</span> <span class="n">config</span><span class="o">.</span><span class="n">properties</span>
</pre></div>
</div>
</section>
<section id="registering-and-deploying-model">
<h3>4. Registering and Deploying model<a class="headerlink" href="#registering-and-deploying-model" title="Permalink to this headline"></a></h3>
<p>Registering and deploying the model follows the same steps shown in the <a class="reference external" href="https://pytorch.org/serve/use_cases.html">PyTorch Use Case documentation</a>.</p>
</section>
</section>
<section id="benchmarking-with-launcher">
<h2>Benchmarking with Launcher<a class="headerlink" href="#benchmarking-with-launcher" title="Permalink to this headline"></a></h2>
<p>Launcher can be used with TorchServe official <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks">benchmark</a> to launch server and benchmark requests with optimal configuration on Intel hardware.</p>
<p>In this section we provide examples of benchmarking with launcher with its default configuration.</p>
<p>Add the following lines to <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> in the benchmark directory to use launcher with its default setting.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>The rest of the steps for benchmarking follows the same steps shown in the <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks">benchmark documentation</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">model_log.log</span></code> contains information and command that were used for this execution launch.</p>
<p>CPU usage on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core is shown as below:
<img alt="launcher_default_2sockets" src="https://user-images.githubusercontent.com/93151422/144373537-07787510-039d-44c4-8cfd-6afeeb64ac78.gif" /></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat logs/model_log.log
2021-12-01 21:22:40,096 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/&lt;user&gt;/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
2021-12-01 21:22:40,096 - __main__ - INFO - OMP_NUM_THREADS=56
2021-12-01 21:22:40,096 - __main__ - INFO - Using Intel OpenMP
2021-12-01 21:22:40,096 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2021-12-01 21:22:40,096 - __main__ - INFO - KMP_BLOCKTIME=1
2021-12-01 21:22:40,096 - __main__ - INFO - LD_PRELOAD=&lt;VIRTUAL_ENV&gt;/lib/libiomp5.so
2021-12-01 21:22:40,096 - __main__ - WARNING - Numa Aware: cores:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55] in different NUMA node
</pre></div>
</div>
<p>CPU usage on a machine with Intel(R) Xeon(R) Platinum 8375C CPU, 1 socket, 2 cores per socket, 2 threads per socket is shown as below:
<img alt="launcher_default_1socket" src="https://user-images.githubusercontent.com/93151422/144372993-92b2ca96-f309-41e2-a5c8-bf2143815c93.gif" /></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat logs/model_log.log
2021-12-02 06:15:03,981 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/&lt;user&gt;/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
2021-12-02 06:15:03,981 - __main__ - INFO - OMP_NUM_THREADS=2
2021-12-02 06:15:03,982 - __main__ - INFO - Using Intel OpenMP
2021-12-02 06:15:03,982 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2021-12-02 06:15:03,982 - __main__ - INFO - KMP_BLOCKTIME=1
2021-12-02 06:15:03,982 - __main__ - INFO - LD_PRELOAD=&lt;VIRTUAL_ENV&gt;/lib/libiomp5.so
</pre></div>
</div>
</section>
<section id="performance-boost-with-ipex-and-launcher">
<h2>Performance Boost with IPEX and Launcher<a class="headerlink" href="#performance-boost-with-ipex-and-launcher" title="Permalink to this headline"></a></h2>
<p><img alt="pdt_perf" src="https://github.com/min-jean-cho/frameworks.ai.pytorch.ipex-cpu-1/assets/93151422/a158ba6c-a151-4115-befb-39acb7545936" /></p>
<p>Above shows performance improvement of Torchserve with IPEX and launcher on ResNet50 and BERT-base-uncased. Torchserve official <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-apache-bench">apache-bench benchmark</a> on Amazon EC2 m6i.24xlarge was used to collect the results. Add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> to reproduce the results. Notice that launcher is configured such that a single instance uses all physical cores on a single socket to avoid cross socket communication and core overlap.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">node_id</span> <span class="mi">0</span> <span class="o">--</span><span class="n">ninstance</span> <span class="mi">1</span> <span class="o">--</span><span class="n">enable_jemalloc</span>
</pre></div>
</div>
<p>Use the following command to reproduce the results.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">url</span> <span class="p">{</span><span class="n">modelUrl</span><span class="p">}</span> <span class="o">--</span><span class="nb">input</span> <span class="p">{</span><span class="n">inputPath</span><span class="p">}</span> <span class="o">--</span><span class="n">concurrency</span> <span class="mi">1</span>
</pre></div>
</div>
<p>For example, run the following command to reproduce latency performance of ResNet50 with data type of IPEX int8 and batch size of 1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">url</span> <span class="s1">&#39;file:///model_store/rn50_ipex_int8.mar&#39;</span> <span class="o">--</span><span class="n">concurrency</span> <span class="mi">1</span>
</pre></div>
</div>
<p>For example, run the following command to reproduce latency performance of BERT with data type of IPEX int8 and batch size of 1.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">url</span> <span class="s1">&#39;file:///model_store/bert_ipex_int8.mar&#39;</span> <span class="o">--</span><span class="nb">input</span> <span class="s1">&#39;../examples/Huggingface_Transformers/Seq_classification_artifacts/sample_text_captum_input.txt&#39;</span> <span class="o">--</span><span class="n">concurrency</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="launch_script.html" class="btn btn-neutral float-left" title="Launch Script Usage Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="known_issues.html" class="btn btn-neutral float-right" title="Known Issues" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>