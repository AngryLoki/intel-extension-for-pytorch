Index: pytorch-1.7/aten/src/ATen/SparseTensorImpl.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/SparseTensorImpl.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/aten/src/ATen/SparseTensorImpl.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -9,6 +9,8 @@
   DeviceType sparseTensorSetToDeviceType(DispatchKeySet key_set) {
     if (key_set.has(DispatchKey::SparseCPU)) {
       return kCPU;
+    } else if (key_set.has(DispatchKey::SparseXPU)) {
+      return kXPU;
     } else if (key_set.has(DispatchKey::SparseCUDA)) {
       return kCUDA;
     } else {
Index: pytorch-1.7/aten/src/ATen/native/TensorIterator.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/native/TensorIterator.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/aten/src/ATen/native/TensorIterator.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -374,7 +374,7 @@
     // Checks all tensors are on the same device, if requested
     if (config.check_all_same_device_) {
       // Handles CPU scalars on CUDA kernels that support them
-      if (common_device.is_cuda() &&
+      if ((common_device.is_cuda() || common_device.type() == at::kXPU)&&
           config.allow_cpu_scalars_ &&
           !op.is_output &&
           op.tensor.dim() == 0 &&
Index: pytorch-1.7/aten/src/ATen/native/sparse/SparseTensor.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/native/sparse/SparseTensor.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/aten/src/ATen/native/sparse/SparseTensor.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -76,6 +76,8 @@
   DispatchKey dispatch_key;
   if (options.device().is_cuda()) {
     dispatch_key = DispatchKey::SparseCUDA;
+  } else if (options.device().type() == DeviceType::XPU) {
+    dispatch_key = DispatchKey::SparseXPU;
   } else {
     dispatch_key = DispatchKey::SparseCPU;
   }
Index: pytorch-1.7/aten/src/ATen/templates/TensorBody.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/templates/TensorBody.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/aten/src/ATen/templates/TensorBody.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -4,6 +4,7 @@
 #include <c10/core/Layout.h>
 #include <c10/core/MemoryFormat.h>
 #include <c10/core/QScheme.h>
+#include <c10/core/Stream.h>
 #include <c10/core/Scalar.h>
 #include <c10/core/ScalarType.h>
 #include <c10/core/Storage.h>
@@ -49,6 +50,8 @@
 class Tensor;
 using TensorList = ArrayRef<Tensor>;
 
+using Stream = c10::Stream;
+
 namespace impl {
 inline bool variable_excluded_from_dispatch() {
 #ifdef C10_MOBILE
@@ -317,6 +320,9 @@
   /// Returns if a `Tensor` has CUDA backend.
   bool is_cuda() const;
 
+  /// Returns if a `Tensor` has XPU backend.
+  bool is_xpu() const;
+
   /// Returns if a `Tensor` has HIP backend.
   bool is_hip() const;
 
Index: pytorch-1.7/aten/src/ATen/templates/TensorMethods.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/templates/TensorMethods.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/aten/src/ATen/templates/TensorMethods.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -1,6 +1,7 @@
 #include <c10/core/Scalar.h>
 #include <c10/core/MemoryFormat.h>
 #include <c10/core/QScheme.h>
+#include <c10/core/Stream.h>
 #include <c10/macros/Macros.h>
 #include <c10/core/TensorOptions.h>
 #include <c10/util/intrusive_ptr.h>
@@ -14,6 +15,8 @@
 
 namespace at {
 
+using Stream = c10::Stream;
+
 Tensor Tensor::cpu() const {
   return to(options().device(DeviceType::CPU), /*non_blocking*/ false, /*copy*/ false);
 }
@@ -74,6 +77,16 @@
   return impl_->is_cuda();
 }
 
+bool Tensor::is_xpu() const {
+  // NB: this is not a native function to avoid dispatching overhead.
+  return impl_->is_xpu();
+}
+
+bool is_xpu(Tensor self) {
+  // NB: this is not a native function to avoid dispatching overhead.
+  return self.is_xpu();
+}
+
 NamedTensorMeta* Tensor::get_named_tensor_meta() {
   return static_cast<NamedTensorMeta*>(impl_->named_tensor_meta());
 }
Index: pytorch-1.7/c10/core/Backend.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/Backend.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/Backend.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -31,14 +31,17 @@
   CUDA,
   HIP,
   FPGA,
+  XPU,
   SparseCPU,
   SparseCUDA,
   SparseHIP,
+  SparseXPU,
   MSNPU,
   XLA,
   Vulkan,
   QuantizedCPU,
   QuantizedCUDA,
+  QuantizedXPU,
   Undefined,
   MkldnnCPU,
   NumOptions
@@ -48,6 +51,8 @@
   switch (b) {
     case Backend::CPU:
       return Backend::SparseCPU;
+    case Backend::XPU:
+      return Backend::SparseXPU;
     case Backend::CUDA:
       return Backend::SparseCUDA;
     case Backend::HIP:
@@ -77,6 +82,10 @@
       return Backend::MSNPU;
     case Backend::XLA:
       return Backend::XLA;
+    case Backend::XPU:
+      return Backend::XPU;
+    case Backend::SparseXPU:
+      return Backend::XPU;
     case Backend::SparseCPU:
       return Backend::CPU;
     case Backend::SparseCUDA:
@@ -87,6 +96,8 @@
       return Backend::QuantizedCPU;
     case Backend::QuantizedCUDA:
       return Backend::QuantizedCUDA;
+    case Backend::QuantizedXPU:
+      return Backend::QuantizedXPU;
     default:
       throw std::runtime_error("Unknown backend");
   }
@@ -119,6 +130,12 @@
     return Backend::QuantizedCPU;
   } else if (t == DispatchKey::QuantizedCUDA) {
     return Backend::QuantizedCUDA;
+  } else if (t == DispatchKey::XPU) {
+    return Backend::XPU;
+  } else if (t == DispatchKey::SparseXPU) {
+    return Backend::SparseXPU;
+  } else if (t == DispatchKey::QuantizedXPU) {
+    return Backend::QuantizedXPU;
   } else if (t == DispatchKey::Undefined) {
     return Backend::Undefined;
   } else {
@@ -140,6 +157,10 @@
       return DispatchKey::MSNPU;
     case Backend::XLA:
       return DispatchKey::XLA;
+    case Backend::XPU:
+      return DispatchKey::XPU;
+    case Backend::SparseXPU:
+      return DispatchKey::SparseXPU;
     case Backend::SparseCPU:
       return DispatchKey::SparseCPU;
     case Backend::SparseCUDA:
@@ -154,6 +175,8 @@
       return DispatchKey::QuantizedCPU;
     case Backend::QuantizedCUDA:
       return DispatchKey::QuantizedCUDA;
+    case Backend::QuantizedXPU:
+      return DispatchKey::QuantizedXPU;
     case Backend::Undefined:
       return DispatchKey::Undefined;
     default:
@@ -181,6 +204,10 @@
       return DeviceType::CUDA;
     case Backend::SparseHIP:
       return DeviceType::HIP;
+    case Backend::XPU:
+    case Backend::SparseXPU:
+    case Backend::QuantizedXPU:
+      return DeviceType::XPU;
     case Backend::MkldnnCPU:
     case Backend::QuantizedCPU:
       return DeviceType::CPU;
@@ -204,6 +231,8 @@
     case Backend::HIP:
       return Backend::CPU;
     case Backend::FPGA:
+      return Backend::CPU;
+    case Backend::XPU:
       return Backend::CPU;
     case Backend::SparseCPU:
       return Backend::SparseCPU;
@@ -211,6 +240,8 @@
       return Backend::SparseCPU;
     case Backend::SparseHIP:
       return Backend::SparseCPU;
+    case Backend::SparseXPU:
+      return Backend::SparseCPU;
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::CPU;
@@ -219,6 +250,8 @@
     case Backend::QuantizedCPU:
       return Backend::QuantizedCPU;
     case Backend::QuantizedCUDA:
+      return Backend::QuantizedCPU;
+    case Backend::QuantizedXPU:
       return Backend::QuantizedCPU;
     case Backend::Undefined:
       return Backend::Undefined;
@@ -227,8 +260,37 @@
   }
 }
 
+static inline Backend backendToXPU(Backend b) {
+  switch (b) {
+    case Backend::CPU:
+    case Backend::CUDA:
+    case Backend::HIP:
+    case Backend::FPGA:
+    case Backend::XPU:
+    case Backend::MSNPU:
+    case Backend::XLA:
+    case Backend::MkldnnCPU:
+    case Backend::Vulkan:
+      return Backend::XPU;
+    case Backend::SparseCPU:
+    case Backend::SparseCUDA:
+    case Backend::SparseXPU:
+    case Backend::SparseHIP:
+      return Backend::SparseXPU;
+    case Backend::QuantizedCPU:
+    case Backend::QuantizedCUDA:
+    case Backend::QuantizedXPU:
+      return Backend::QuantizedXPU;
+    case Backend::Undefined:
+      return Backend::Undefined;
+    default:
+      AT_ERROR("Unknown backend");
+  }
+}
+
 static inline Backend backendToCUDA(Backend b) {
   switch (b) {
+    case Backend::XPU:
     case Backend::CPU:
     case Backend::CUDA:
     case Backend::HIP:
@@ -236,6 +298,7 @@
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::CUDA;
+    case Backend::SparseXPU:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
@@ -249,6 +312,7 @@
 
 static inline Backend backendToHIP(Backend b) {
   switch (b) {
+    case Backend::XPU:
     case Backend::CPU:
     case Backend::CUDA:
     case Backend::HIP:
@@ -256,6 +320,7 @@
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::HIP;
+    case Backend::SparseXPU:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
@@ -278,6 +343,8 @@
       return "HIP";
     case Backend::FPGA:
       return "FPGA";
+    case Backend::XPU:
+      return "XPU";
     case Backend::MSNPU:
       return "MSNPU";
     case Backend::XLA:
@@ -288,6 +355,8 @@
       return "SparseCUDA";
     case Backend::SparseHIP:
       return "SparseHIP";
+    case Backend::SparseXPU:
+      return "SparseXPU";
     case Backend::MkldnnCPU:
       return "MkldnnCPU";
     case Backend::Vulkan:
@@ -296,6 +365,8 @@
       return "QuantizedCPU";
     case Backend::QuantizedCUDA:
       return "QuantizedCUDA";
+    case Backend::QuantizedXPU:
+      return "QuantizedXPU";
     default:
       return "UNKNOWN_BACKEND";
   }
@@ -303,6 +374,7 @@
 
 static inline bool isSparse(Backend b) {
   switch (b) {
+    case Backend::SparseXPU:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
Index: pytorch-1.7/c10/core/Device.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/Device.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/Device.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -30,9 +30,11 @@
 namespace c10 {
 namespace {
 DeviceType parse_type(const std::string& device_string) {
-  static const std::array<std::pair<std::string, DeviceType>, 10> types = {{
+  static const std::array<std::pair<std::string, DeviceType>,
+                          static_cast<size_t>(DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES)> types = {{
       {"cpu", DeviceType::CPU},
       {"cuda", DeviceType::CUDA},
+      {"xpu", DeviceType::XPU},
       {"mkldnn", DeviceType::MKLDNN},
       {"opengl", DeviceType::OPENGL},
       {"opencl", DeviceType::OPENCL},
@@ -52,7 +54,7 @@
     return device->second;
   }
   AT_ERROR(
-      "Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu, xla device type at start of device string: ", device_string);
+      "Expected one of cpu, cuda, xpu, mkldnn, opengl, opencl, ideep, hip, msnpu, xla device type at start of device string: ", device_string);
 }
 } // namespace
 
Index: pytorch-1.7/c10/core/DeviceType.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/DeviceType.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/DeviceType.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -29,6 +29,8 @@
       return lower_case ? "xla" : "XLA";
     case DeviceType::Vulkan:
       return lower_case ? "vulkan" : "VULKAN";
+    case DeviceType::XPU:
+      return lower_case ? "xpu" : "XPU";
     default:
       AT_ERROR(
           "Unknown device: ",
@@ -62,6 +64,7 @@
     case DeviceType::MSNPU:
     case DeviceType::XLA:
     case DeviceType::Vulkan:
+    case DeviceType::XPU:
       return true;
     default:
       return false;
Index: pytorch-1.7/c10/core/DeviceType.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/DeviceType.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/DeviceType.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -24,11 +24,12 @@
   MSNPU = 8, // MSNPU
   XLA = 9, // XLA / TPU
   Vulkan = 10, // Vulkan
+  XPU = 11, // XPU
   // NB: If you add more devices:
   //  - Change the implementations of DeviceTypeName and isValidDeviceType
   //    in DeviceType.cpp
   //  - Change the number below
-  COMPILE_TIME_MAX_DEVICE_TYPES = 11,
+  COMPILE_TIME_MAX_DEVICE_TYPES = 12,
   ONLY_FOR_TEST = 20901, // This device type is only for test.
 };
 
@@ -39,6 +40,7 @@
 constexpr DeviceType kMSNPU = DeviceType::MSNPU;
 constexpr DeviceType kXLA = DeviceType::XLA;
 constexpr DeviceType kVulkan = DeviceType::Vulkan;
+constexpr DeviceType kXPU = DeviceType::XPU;
 
 // define explicit int constant
 constexpr int COMPILE_TIME_MAX_DEVICE_TYPES =
Index: pytorch-1.7/c10/core/DispatchKey.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/DispatchKey.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/DispatchKey.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -15,6 +15,8 @@
       return "HIP";
     case DispatchKey::FPGA:
       return "FPGA";
+    case DispatchKey::XPU:
+      return "XPU";
     case DispatchKey::MSNPU:
       return "MSNPU";
     case DispatchKey::XLA:
@@ -35,6 +37,8 @@
       return "QuantizedCPU";
     case DispatchKey::QuantizedCUDA:
       return "QuantizedCUDA";
+    case DispatchKey::QuantizedXPU:
+      return "QuantizedXPU";
 
     case DispatchKey::ComplexCPU:
       return "ComplexCPU";
@@ -52,6 +56,8 @@
       return "SparseCUDA";
     case DispatchKey::SparseHIP:
       return "SparseHIP";
+    case DispatchKey::SparseXPU:
+      return "SparseXPU";
 
     case DispatchKey::PrivateUse1:
       return "PrivateUse1";
Index: pytorch-1.7/c10/core/DispatchKey.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/DispatchKey.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/DispatchKey.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -60,6 +60,7 @@
          // test/cpp_extensions/msnpu_extension.cpp
   XLA, // lives out of tree at https://github.com/pytorch/xla
   Vulkan,
+  XPU, // For out of tree Intel's heterogeneous computing plug-in
 
   // These are Caffe2 device types which we grandfathered into
   // DispatchKey.
@@ -74,6 +75,7 @@
   // based on the dtype of the tensor.
   QuantizedCPU, // registered at build/aten/src/ATen/QuantizedCPUType.cpp
   QuantizedCUDA, // registered at build/aten/src/ATen/QuantizedCUDAType.cpp
+  QuantizedXPU, // For out of tree Intel's heterogeneous computing plug-in
   ComplexCPU, // lives out of tree at
               // https://gitlab.com/pytorch-complex/pytorch-cpu-strided-complex
   ComplexCUDA, // and
@@ -102,6 +104,7 @@
   SparseCUDA, // registered at build/aten/src/ATen/SparseCUDAType.cpp
   SparseHIP, // TODO: I think this is not actually used, due to Note
              // [Masquerading as CUDA]
+  SparseXPU, // For out of tree Intel's heterogeneous computing plug-in
 
   // Here are reserved backends for user-defined backends, see Note [Private use
   // DispatchKey]
@@ -216,6 +219,7 @@
   AutogradCPU,
   AutogradCUDA,
   AutogradXLA,
+  AutogradXPU,
   // Here are some reserved pre-autograd keys for user-defined backends, see
   // Note [Private use DispatchKey]
   AutogradPrivateUse1,
Index: pytorch-1.7/c10/core/DispatchKeySet.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/DispatchKeySet.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/DispatchKeySet.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -28,6 +28,7 @@
   DispatchKey::CPU,
   DispatchKey::CUDA,
   DispatchKey::XLA,
+  DispatchKey::XPU,
   DispatchKey::PrivateUse1,
   DispatchKey::PrivateUse2,
   DispatchKey::PrivateUse3,
Index: pytorch-1.7/c10/core/DispatchKeySet.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/DispatchKeySet.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/DispatchKeySet.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -192,6 +192,7 @@
   DispatchKey::AutogradCPU,
   DispatchKey::AutogradCUDA,
   DispatchKey::AutogradXLA,
+  DispatchKey::AutogradXPU,
   DispatchKey::AutogradPrivateUse1,
   DispatchKey::AutogradPrivateUse2,
   DispatchKey::AutogradPrivateUse3,
Index: pytorch-1.7/c10/core/Layout.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/Layout.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/Layout.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -17,6 +17,7 @@
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
+    case Backend::SparseXPU:
       return Layout::Sparse;
     case Backend::MkldnnCPU:
       return Layout::Mkldnn;
Index: pytorch-1.7/c10/core/TensorImpl.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/TensorImpl.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/TensorImpl.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -434,13 +434,15 @@
     // NB: This method is not virtual and avoid dispatches for performance reasons.
     return key_set_.has(DispatchKey::SparseCPU) ||
            key_set_.has(DispatchKey::SparseCUDA) ||
-           key_set_.has(DispatchKey::SparseHIP);
+           key_set_.has(DispatchKey::SparseHIP) ||
+           key_set_.has(DispatchKey::SparseXPU);
   }
 
   bool is_quantized() const {
     // NB: This method is not virtual and avoid dispatches for performance reasons.
     return key_set_.has(DispatchKey::QuantizedCPU) ||
-        key_set_.has(DispatchKey::QuantizedCUDA);
+        key_set_.has(DispatchKey::QuantizedCUDA) ||
+        key_set_.has(DispatchKey::QuantizedXPU);
   }
 
   bool is_meta() const {
@@ -455,6 +457,13 @@
         key_set_.has(DispatchKey::QuantizedCUDA);
   }
 
+  bool is_xpu() const {
+    // NB: This method is not virtual and avoid dispatches for performance reasons.
+    return key_set_.has(DispatchKey::XPU) ||
+           key_set_.has(DispatchKey::SparseXPU) ||
+           key_set_.has(DispatchKey::QuantizedXPU);
+  }
+
   bool is_hip() const {
     // NB: This method is not virtual and avoid dispatches for performance reasons.
     return key_set_.has(DispatchKey::HIP) ||
@@ -913,12 +922,14 @@
     auto is_dense = [](DispatchKeySet ts) {
       return ts.has(DispatchKey::CPU) ||
              ts.has(DispatchKey::CUDA) ||
-             ts.has(DispatchKey::HIP);
+             ts.has(DispatchKey::HIP) ||
+             ts.has(DispatchKey::XPU);
     };
     auto is_sparse = [](DispatchKeySet ts) {
       return ts.has(DispatchKey::SparseCPU) ||
              ts.has(DispatchKey::SparseCUDA) ||
-             ts.has(DispatchKey::SparseHIP);
+             ts.has(DispatchKey::SparseHIP) ||
+             ts.has(DispatchKey::SparseXPU);
     };
     return (key_set_ == from) || (is_dense(key_set_) && is_dense(from)) || (is_sparse(key_set_) && is_sparse(from));
   }
Index: pytorch-1.7/c10/core/TensorOptions.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/TensorOptions.h	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/c10/core/TensorOptions.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -595,6 +595,12 @@
             }
             return DispatchKey::CUDA;
           }
+          case DeviceType::XPU: {
+            if (isQIntType(dtype_)) {
+              return DispatchKey::QuantizedXPU;
+            }
+            return DispatchKey::XPU;
+          }
           case DeviceType::MKLDNN:
             return DispatchKey::MKLDNN;
           case DeviceType::OPENGL:
@@ -625,6 +631,8 @@
             return DispatchKey::SparseCUDA;
           case DeviceType::HIP:
             return DispatchKey::SparseHIP;
+          case DeviceType::XPU:
+            return DispatchKey::SparseXPU;
           default:
             AT_ERROR("Unsupported device type for sparse layout: ", device_.type());
         }
@@ -675,6 +683,12 @@
     return DeviceType::CPU;
   } else if (tid == DispatchKey::Vulkan) {
     return DeviceType::Vulkan;
+  } else if (tid == DispatchKey::XPU) {
+    return DeviceType::XPU;
+  } else if (tid == DispatchKey::SparseXPU) {
+    return DeviceType::XPU;
+  } else if (tid == DispatchKey::QuantizedXPU) {
+    return DeviceType::XPU;
   } else {
     AT_ASSERTM(false, "Unknown DispatchKey: ", tid);
   }
Index: pytorch-1.7/setup.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/setup.py	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/setup.py	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -881,6 +881,7 @@
                 'include/torch/csrc/jit/testing/*.h',
                 'include/torch/csrc/onnx/*.h',
                 'include/torch/csrc/utils/*.h',
+                'include/torch/csrc/generic/*.h',
                 'include/pybind11/*.h',
                 'include/pybind11/detail/*.h',
                 'include/TH/*.h*',
Index: pytorch-1.7/tools/autograd/templates/python_variable_methods.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/autograd/templates/python_variable_methods.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/tools/autograd/templates/python_variable_methods.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -12,7 +12,6 @@
 #include "torch/csrc/autograd/utils/wrap_outputs.h"
 #include "torch/csrc/jit/frontend/tracer.h"
 #ifdef USE_CUDA
-#include "torch/csrc/cuda/Stream.h"
 #include "torch/csrc/cuda/Event.h"
 #endif
 #include "torch/csrc/utils/cuda_lazy_init.h"
@@ -30,6 +29,7 @@
 
 #include <ATen/ATen.h>
 #include "c10/util/Optional.h"
+#include "c10/core/Stream.h"
 
 #include <stdexcept>
 
@@ -40,6 +40,7 @@
 using at::Scalar;
 using at::ScalarType;
 using at::Tensor;
+using c10::Stream;
 using namespace torch::autograd::utils;
 
 namespace torch { namespace autograd {
@@ -502,6 +503,28 @@
   END_HANDLE_TH_ERRORS
 }
 
+static PyObject * THPVariable_xpu(PyObject* self, PyObject* args, PyObject* kwargs)
+{
+  HANDLE_TH_ERRORS
+  static PythonArgParser parser({
+    "xpu(Device? device=None, bool non_blocking=False, *, MemoryFormat? memory_format=None)",
+    "xpu(Device? device=None, bool async=False, *, MemoryFormat? memory_format=None)|deprecated"
+  });
+  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;
+  ParsedArgs<3> parsed_args;
+  auto r = parser.parse(self, args, kwargs, parsed_args);
+
+  if(r.has_torch_function()){
+    return handle_torch_function(r, self, args, kwargs, THPVariableClass, "torch.Tensor");
+  }
+
+  auto device = r.isNone(0) ? at::Device(at::DeviceType::XPU) : r.device(0);
+  auto opt_memory_format = r.memoryformatOptional(2);
+  TORCH_CHECK(device.type() == at::DeviceType::XPU, "Invalid device, must be xpu device");
+  return THPVariable_Wrap(dispatch_to(self_, device, r.toBool(1), false, opt_memory_format));
+  END_HANDLE_TH_ERRORS
+}
+
 static PyObject * THPVariable_to_type(PyObject* self, ScalarType scalarType, c10::optional<c10::MemoryFormat> optional_memory_format) {
   HANDLE_TH_ERRORS
   auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;
@@ -703,27 +726,6 @@
   return torch::utils::tensor_to_numpy(self_);
   END_HANDLE_TH_ERRORS
 }
-
-// TODO: move this to ATen. We would need to expose Stream objects in ATen.
-static PyObject * THPVariable_record_stream(PyObject* self, PyObject* arg)
-{
-  HANDLE_TH_ERRORS
-  if (check_has_torch_function(self)) {
-    auto args = py::make_tuple(py::handle(arg));
-    return handle_torch_function(self, "record_stream", args.ptr());
-  }
-#ifdef USE_CUDA
-  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;
-  if (!THCPStream_Check(arg)) {
-    return PyErr_Format(PyExc_TypeError, "expected Stream object");
-  }
-  c10::cuda::CUDACachingAllocator::recordStream(self_.storage().data_ptr(), at::cuda::CUDAStream::unpack(((THCPStream*)arg)->cdata));
-  Py_RETURN_NONE;
-#else
-  throw std::runtime_error("PyTorch compiled without CUDA support");
-#endif
-  END_HANDLE_TH_ERRORS
-}
 
 static PyObject * THPVariable_requires_grad_(PyObject* self, PyObject* args, PyObject* kwargs)
 {
@@ -1158,6 +1160,7 @@
   {"copy_", (PyCFunction)(void(*)(void))THPVariable_copy_, METH_VARARGS | METH_KEYWORDS, NULL},
   {"cpu", (PyCFunction)(void(*)(void))THPVariable_cpu, METH_VARARGS | METH_KEYWORDS, NULL},
   {"cuda", (PyCFunction)(void(*)(void))THPVariable_cuda, METH_VARARGS | METH_KEYWORDS, NULL},
+  {"xpu", (PyCFunction)(void(*)(void))THPVariable_xpu, METH_VARARGS | METH_KEYWORDS, NULL},
   {"data_ptr", (PyCFunction)THPVariable_data_ptr, METH_NOARGS, NULL},
   {"dim", (PyCFunction)THPVariable_dim, METH_NOARGS, NULL},
   {"has_names", (PyCFunction)THPVariable_has_names, METH_NOARGS, NULL},
@@ -1181,7 +1184,6 @@
   {"nonzero", (PyCFunction)(void(*)(void))THPVariable_nonzero, METH_VARARGS | METH_KEYWORDS, NULL},
   {"numel", (PyCFunction)THPVariable_numel, METH_NOARGS, NULL},
   {"numpy", (PyCFunction)THPVariable_numpy, METH_NOARGS, NULL},
-  {"record_stream", (PyCFunction)THPVariable_record_stream, METH_O, NULL},
   {"requires_grad_", (PyCFunction)(void(*)(void))THPVariable_requires_grad_, METH_VARARGS | METH_KEYWORDS, NULL},
   {"set_", (PyCFunction)(void (*)(void))THPVariable_set_, METH_VARARGS | METH_KEYWORDS, NULL},
   {"short", (PyCFunction)(void(*)(void))THPVariable_short, METH_VARARGS | METH_KEYWORDS, NULL},
Index: pytorch-1.7/torch/csrc/autograd/python_variable.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/autograd/python_variable.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/csrc/autograd/python_variable.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -544,6 +544,17 @@
   END_HANDLE_TH_ERRORS
 }
 
+PyObject *THPVariable_is_xpu(THPVariable *self, void *unused)
+{
+  HANDLE_TH_ERRORS
+  if (check_has_torch_function((PyObject *)self)) {
+    return handle_torch_function_getter(self, "is_xpu");
+  }
+  auto& self_ = self->cdata;
+  return torch::autograd::utils::wrap(self_.is_xpu());
+  END_HANDLE_TH_ERRORS
+}
+
 PyObject *THPVariable_is_sparse(THPVariable *self, void *unused)
 {
   HANDLE_TH_ERRORS
@@ -693,6 +704,7 @@
   {"name", (getter)THPVariable_get_name, nullptr, nullptr, nullptr},
   {"shape", (getter)THPVariable_get_shape, nullptr, nullptr, nullptr},
   {"is_cuda", (getter)THPVariable_is_cuda, nullptr, nullptr, nullptr},
+  {"is_xpu", (getter)THPVariable_is_xpu, nullptr, nullptr, nullptr},
   {"is_sparse", (getter)THPVariable_is_sparse, nullptr, nullptr, nullptr},
   {"is_mkldnn", (getter)THPVariable_is_mkldnn, nullptr, nullptr, nullptr},
   {"is_complex", (getter)THPVariable_is_complex, nullptr, nullptr, nullptr},
Index: pytorch-1.7/torch/csrc/jit/passes/graph_fuser.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/passes/graph_fuser.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/csrc/jit/passes/graph_fuser.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -184,6 +184,8 @@
       return canFuseOnCPU();
     } else if ((*device).is_cuda()) {
       return canFuseOnGPU();
+    } else if ((*device).is_xpu()) {
+      return false;
     }
     throw std::runtime_error("Unknown device");
   }
Index: pytorch-1.7/torch/csrc/jit/serialization/pickler.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/serialization/pickler.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/csrc/jit/serialization/pickler.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -603,7 +603,8 @@
   result.tensor_ = tensor;
   result.size_ = tensor.storage().nbytes();
   // TODO HIP support
-  if (tensor.storage().device_type() == DeviceType::CUDA) {
+  if (tensor.storage().device_type() == DeviceType::CUDA ||
+      tensor.storage().device_type() == DeviceType::XPU) {
     // NB: This new tensor is created to support cuda tensors.
     // Storages can be mutated when converting tensors from cuda to cpu,
     // and we need a cpu tensor to copy data from.
Index: pytorch-1.7/torch/csrc/jit/serialization/unpickler.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/serialization/unpickler.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/csrc/jit/serialization/unpickler.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -67,6 +67,7 @@
       case StringType::Kind:
       case FunctionType::Kind:
       case DeviceObjType::Kind:
+      case StreamObjType::Kind:
       case QSchemeType::Kind:
       case LayoutType::Kind:
       case ScalarTypeType::Kind:
@@ -425,6 +426,8 @@
       }
 
       if (device.type() == DeviceType::CUDA) {
+        tensor = tensor.to(device, tensor.scalar_type());
+      } else if (device.type() == DeviceType::XPU) {
         tensor = tensor.to(device, tensor.scalar_type());
       } else if (device.type() != DeviceType::CPU) {
         AT_ERROR(
Index: pytorch-1.7/torch/csrc/utils/tensor_new.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/utils/tensor_new.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/csrc/utils/tensor_new.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -59,6 +59,8 @@
     case DeviceType::XLA:
       TORCH_CHECK(!isSparse(b), "Sparse not implemented for XLA");
       return Backend::XLA;
+    case DeviceType::XPU:
+      return backendToXPU(b);
     default:
       AT_ERROR("Unknown device type");
   }
@@ -337,20 +339,24 @@
     TORCH_CHECK(dispatch_key == c10::DispatchKey::CPU
                 || dispatch_key == c10::DispatchKey::CUDA
                 || dispatch_key == c10::DispatchKey::HIP
-                || dispatch_key == c10::DispatchKey::XLA,
+                || dispatch_key == c10::DispatchKey::XLA
+                || dispatch_key == c10::DispatchKey::XPU,
                 "new(): expected DispatchKey: ", c10::DispatchKey::CPU,
                 " or ", c10::DispatchKey::CUDA,
                 " or ", c10::DispatchKey::HIP,
                 " or ", c10::DispatchKey::XLA,
+                " or ", c10::DispatchKey::XPU,
                 " but got: ", dispatch_key);
   } else if(expected_layout == c10::kSparse) {
     // NOTE: no sparse XLA
     TORCH_CHECK(dispatch_key == c10::DispatchKey::SparseCPU
                 || dispatch_key == c10::DispatchKey::SparseCUDA
-                || dispatch_key == c10::DispatchKey::SparseHIP,
+                || dispatch_key == c10::DispatchKey::SparseHIP
+                || dispatch_key == c10::DispatchKey::SparseXPU,
                 "new(): expected DispatchKey: ", c10::DispatchKey::SparseCPU,
                 " or ", c10::DispatchKey::SparseCUDA,
                 " or ", c10::DispatchKey::SparseHIP,
+                " or ", c10::DispatchKey::SparseXPU,
                 " but got: ", dispatch_key);
   } else {
     TORCH_INTERNAL_ASSERT(false, "unexpected layout");
Index: pytorch-1.7/torch/csrc/utils/tensor_types.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/utils/tensor_types.cpp	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/csrc/utils/tensor_types.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -19,8 +19,10 @@
   switch (backend) {
     case at::Backend::CPU: return "torch";
     case at::Backend::CUDA: return "torch.cuda";
+    case at::Backend::XPU: return "torch.xpu";
     case at::Backend::SparseCPU: return "torch.sparse";
     case at::Backend::SparseCUDA: return "torch.cuda.sparse";
+    case at::Backend::SparseXPU: return "torch.xpu.sparse";
     default: AT_ERROR("Unimplemented backend ", backend);
   }
 }
Index: pytorch-1.7/torch/distributed/nn/api/remote_module.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/distributed/nn/api/remote_module.py	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/distributed/nn/api/remote_module.py	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -219,6 +219,9 @@
     def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:
         _raise_not_supported(self.cuda.__name__)
 
+    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:
+        _raise_not_supported(self.xpu.__name__)
+
     def cpu(self: T) -> T:
         _raise_not_supported(self.cpu.__name__)
 
Index: pytorch-1.7/torch/nn/modules/module.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/nn/modules/module.py	(revision 60b44264377251f098e5b9cc1e1688e5d787675c)
+++ pytorch-1.7/torch/nn/modules/module.py	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
@@ -462,6 +462,22 @@
         """
         return self._apply(lambda t: t.cuda(device))
 
+    def xpu(self: T, device: Optional[Union[int, device]] = None) -> T:
+        r"""Moves all model parameters and buffers to the XPU.
+
+        This also makes associated parameters and buffers different objects. So
+        it should be called before constructing optimizer if the module will
+        live on XPU while being optimized.
+
+        Arguments:
+            device (int, optional): if specified, all parameters will be
+                copied to that device
+
+        Returns:
+            Module: self
+        """
+        return self._apply(lambda t: t.xpu(device))
+
     def cpu(self: T) -> T:
         r"""Moves all model parameters and buffers to the CPU.
 
Index: pytorch-1.7/torch/autograd/profiler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/autograd/profiler.py	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
+++ pytorch-1.7/torch/autograd/profiler.py	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
@@ -701,10 +701,13 @@
     """
     cpu_time_str = attr_formatter('cpu_time')
     cuda_time_str = attr_formatter('cuda_time')
+    xpu_time_str = attr_formatter('xpu_time')
     cpu_time_total_str = attr_formatter('cpu_time_total')
     cuda_time_total_str = attr_formatter('cuda_time_total')
+    xpu_time_total_str = attr_formatter('xpu_time_total')
     self_cpu_time_total_str = attr_formatter('self_cpu_time_total')
     self_cuda_time_total_str = attr_formatter('self_cuda_time_total')
+    self_xpu_time_total_str = attr_formatter('self_xpu_time_total')
 
     @property
     def cpu_time(self):
@@ -714,6 +717,11 @@
     def cuda_time(self):
         return 0.0 if self.count == 0 else 1.0 * self.cuda_time_total / self.count  # type: ignore
 
+    @property
+    def xpu_time(self):
+        return 0.0 if self.count == 0 else 1.0 * self.xpu_time_total / self.count
+
+
 
 class Interval(object):
     def __init__(self, start, end):
@@ -730,8 +738,8 @@
 class FunctionEvent(FormattedTimesMixin):
     """Profiling information about a single function."""
     def __init__(
-            self, id, node_id, name, thread, cpu_start, cpu_end, fwd_thread=None, input_shapes=None,
-            stack=None, scope=0, cpu_memory_usage=0, cuda_memory_usage=0, is_async=False,
+            self, id, node_id, name, thread, cpu_start=0, cpu_end=0, fwd_thread=None, input_shapes=None,
+            stack=None, scope=0, cpu_memory_usage=0, cuda_memory_usage=0, xpu_memory_usage=0, is_async=False,
             is_remote=True, sequence_nr=-1):
         self.id: int = id
         self.node_id: int = node_id
@@ -740,6 +748,7 @@
         self.thread: int = thread
         self.fwd_thread: Optional[int] = fwd_thread
         self.kernels: List[Kernel] = []
+        self.xpu_kernels: List[Kernel] = []
         self.count: int = 1
         self.cpu_children: List[FunctionEvent] = []
         self.cpu_parent: Optional[FunctionEvent] = None
@@ -748,6 +757,7 @@
         self.scope: int = scope
         self.cpu_memory_usage: int = cpu_memory_usage
         self.cuda_memory_usage: int = cuda_memory_usage
+        self.xpu_memory_usage: int = xpu_memory_usage
         self.is_async: bool = is_async
         self.is_remote: bool = is_remote
         self.sequence_nr: int = sequence_nr
@@ -755,6 +765,9 @@
     def append_kernel(self, name, device, start, end):
         self.kernels.append(Kernel(name, device, Interval(start, end)))
 
+    def append_xpu_kernel(self, name, device, duration):
+        self.xpu_kernels.append(Kernel(name, device, Interval(0, duration)))
+
     def append_cpu_child(self, child):
         """Append a CPU child of type FunctionEvent.
 
@@ -792,6 +805,14 @@
             [child.cuda_memory_usage for child in self.cpu_children]
         )
 
+    @property
+    def self_xpu_memory_usage(self):
+        if self.is_async:
+            return 0
+        return self.xpu_memory_usage - sum(
+            [child.xpu_memory_usage for child in self.cpu_children]
+        )
+
     @property
     def self_cpu_time_total(self):
         if self.is_async:
@@ -813,6 +834,15 @@
     def cpu_time_total(self):
         return self.cpu_interval.elapsed_us()
 
+    @property
+    def xpu_time_total(self):
+        return sum(kinfo.interval.elapsed_us() for kinfo in self.xpu_kernels)
+
+    @property
+    def self_xpu_time_total(self):
+        return sum(kinfo.interval.elapsed_us() for kinfo in self.xpu_kernels) - \
+               sum([child.xpu_time_total for child in self.cpu_children])
+
     @property
     def key(self):
         return self.name
@@ -820,8 +850,8 @@
     def __repr__(self):
         return (
             '<FunctionEvent id={} node_id={} cpu_time={} cpu_start={} cpu_end={} '
-            'cpu_children={} cuda_time={} name={} thread={} input_shapes={} '
-            'cpu_memory_usage={} cuda_memory_usage={} is_async={} is_remote={} seq_nr={}>'.format(
+            'cpu_children={} cuda_time={} xpu_time={} name={} thread={} input_shapes={} '
+            'cpu_memory_usage={} cuda_memory_usage={} xpu_memory_usage={} xpu_kernels={} is_async={} is_remote={} seq_nr={}>'.format(
                 self.id,
                 self.node_id,
                 self.cpu_time_str,
@@ -829,11 +859,14 @@
                 self.cpu_interval.end,
                 str([child.id for child in self.cpu_children]),
                 self.cuda_time_str,
+                self.xpu_time_str,
                 self.name,
                 self.thread,
                 str(self.input_shapes),
                 self.cpu_memory_usage,
                 self.cuda_memory_usage,
+                self.xpu_memory_usage,
+                len(self.xpu_kernels),
                 self.is_async,
                 self.is_remote,
                 self.sequence_nr,
@@ -851,15 +884,19 @@
         self.is_remote: bool = False
         self.cpu_time_total: int = 0
         self.cuda_time_total: int = 0
+        self.xpu_time_total: int = 0
         self.self_cpu_time_total: int = 0
         self.self_cuda_time_total: int = 0
+        self.self_xpu_time_total: int = 0
         self.input_shapes: Optional[List[List[int]]] = None
         self.stack: Optional[List] = None
         self.scope: Optional[int] = None
         self.cpu_memory_usage: int = 0
         self.cuda_memory_usage: int = 0
+        self.xpu_memory_usage: int = 0
         self.self_cpu_memory_usage: int = 0
         self.self_cuda_memory_usage: int = 0
+        self.self_xpu_memory_usage: int = 0
         self.cpu_children: Optional[List[FunctionEvent]] = None
         self.cpu_parent: Optional[FunctionEvent] = None
 
@@ -882,12 +919,16 @@
         assert other.key == self.key
         self.cpu_time_total += other.cpu_time_total
         self.cuda_time_total += other.cuda_time_total
+        self.xpu_time_total += other.xpu_time
         self.self_cpu_time_total += other.self_cpu_time_total
         self.self_cuda_time_total += other.self_cuda_time_total
+        self.self_xpu_time_total += other.self_xpu_time_total
         self.cpu_memory_usage += other.cpu_memory_usage
         self.cuda_memory_usage += other.cuda_memory_usage
+        self.xpu_memory_usage += other.xpu_memory_usage
         self.self_cpu_memory_usage += other.self_cpu_memory_usage
         self.self_cuda_memory_usage += other.self_cuda_memory_usage
+        self.self_xpu_memory_usage += other.self_xpu_memory_usage
         self.count += other.count
         return self
 
@@ -897,16 +938,19 @@
     def __repr__(self):
         return (
             '<FunctionEventAvg key={} self_cpu_time={} cpu_time={} '
-            ' self_cuda_time={} cuda_time={} input_shapes={} '
-            'cpu_memory_usage={} cuda_memory_usage={}>'.format(
+            ' self_cuda_time={} cuda_time={} self_xpu_time={} xpu_time={} input_shapes={} '
+            'cpu_memory_usage={} cuda_memory_usage={} xpu_memory_usage={}>'.format(
                 self.key,
                 self.self_cpu_time_total_str,
                 self.cpu_time_str,
                 self.self_cuda_time_total_str,
                 self.cuda_time_str,
+                self.self_xpu_time_str,
+                self.xpu_time_str,
                 str(self.input_shapes),
                 self.cpu_memory_usage,
                 self.cuda_memory_usage,
+                self.xpu_memory_usage,
             )
         )
 
@@ -934,6 +978,7 @@
     start_record = None
     cuda_records = {}
     functions = []
+    function_stack = []
     record_stack = []
     string_table = StringTable()
 
@@ -986,8 +1031,10 @@
         # accumulated memory allocations per handle
         cpu_memory_allocs = {}
         cuda_memory_allocs = {}
+        xpu_memory_allocs = {}
         # ranges per handle
         range_starts = {}
+        function_stack = []
 
         filtered_handles = set()
         prev_record = None
@@ -1014,6 +1061,16 @@
                 range_starts[record_key] = record
                 cpu_memory_allocs[record_key] = 0
                 cuda_memory_allocs[record_key] = 0
+                xpu_memory_allocs[record_key] = 0
+
+                fe = FunctionEvent(
+                    id=record.handle(),
+                    node_id=record.node_id(),
+                    name=string_table[record.name()],
+                    thread=record.thread_id(),
+                    input_shapes=record.shapes(),
+                )
+                function_stack.append(fe)
             elif record.kind() == 'pop':
                 assert (
                     record_key in range_starts
@@ -1023,29 +1080,25 @@
                 )
 
                 start = range_starts[record_key]
+                fe = function_stack.pop()
 
                 cpu_memory_usage = cpu_memory_allocs[record_key]
                 cuda_memory_usage = cuda_memory_allocs[record_key]
+                xpu_memory_usage = xpu_memory_allocs[record_key]
                 is_async = start.thread_id() != record.thread_id()
                 is_remote_event = record.is_remote()
 
-                fe = FunctionEvent(
-                    id=record.handle(),
-                    node_id=record.node_id(),
-                    name=string_table[start.name()],
-                    thread=start.thread_id(),
-                    cpu_start=start_record.cpu_elapsed_us(start),
-                    cpu_end=start_record.cpu_elapsed_us(record),
-                    fwd_thread=start.fwd_thread_id(),
-                    input_shapes=start.shapes(),
-                    stack=[entry for entry in start.stack() if filter_stack_entry(entry)],
-                    scope=start.scope(),
-                    cpu_memory_usage=cpu_memory_usage,
-                    cuda_memory_usage=cuda_memory_usage,
-                    is_async=is_async,
-                    is_remote=is_remote_event,
-                    sequence_nr=start.sequence_nr(),
-                )
+                fe.cpu_interval = Interval(start_record.cpu_elapsed_us(start), start_record.cpu_elapsed_us(record))
+                fe.cpu_memory_usage = cpu_memory_usage
+                fe.cuda_memory_usage = cuda_memory_usage
+                fe.xpu_memory_usage = xpu_memory_usage
+                fe.is_async = is_async
+                fe.is_remote = is_remote_event
+                fe.fwd_thread=start.fwd_thread_id()
+                fe.stack=[entry for entry in start.stack() if filter_stack_entry(entry)]
+                fe.scope=start.scope()
+                fe.sequence_nr=start.sequence_nr()
+
                 # note: async events have only cpu total time
                 if not is_async and start.has_cuda():
                     cuda_start = adjusted_time(start, cuda_records)
@@ -1060,11 +1113,33 @@
                 del range_starts[record_key]
                 del cpu_memory_allocs[record_key]
                 del cuda_memory_allocs[record_key]
+                del xpu_memory_allocs[record_key]
             elif record.kind() == 'memory_alloc':
                 for handle in cpu_memory_allocs.keys():
                     cpu_memory_allocs[handle] += record.cpu_memory_usage()
                 for handle in cuda_memory_allocs.keys():
                     cuda_memory_allocs[handle] += record.cuda_memory_usage()
+                for handle in xpu_memory_allocs.keys():
+                    xpu_memory_allocs[handle] += record.xpu_memory_usage()
+            elif record.kind() == 'mark':
+                if record.has_xpu():
+                    if len(function_stack) > 0:
+                        fe = function_stack[-1]
+                        fe.append_xpu_kernel(record.name,
+                                               record.device,
+                                               record.xpu_elapsed_us())
+                    else:
+                        # a xpu event is submitted but no parent function was recorded.
+                        fe = FunctionEvent(
+                            id=record.handle(),
+                            node_id=record.node_id(),
+                            name=string_table[record.name()],
+                            thread=record.thread_id(),
+                            input_shapes=record.shapes())
+                        fe.append_xpu_kernel(record.name,
+                                               record.device,
+                                               record.xpu_elapsed_us())
+                        functions.append(fe)
             prev_record = record
 
     # Sort functions by start time then by end time ascending.
@@ -1204,6 +1279,9 @@
         'CPU total %',
         'CPU total',
         'CPU time avg',
+        'XPU total %',
+        'XPU total',
+        'XPU time avg',
     ]
     if use_cuda:
         headers.extend([
@@ -1217,6 +1295,10 @@
             'CPU Mem',
             'Self CPU Mem',
         ])
+        headers.extend([
+            'XPU Mem',
+            'Self XPU Mem',
+        ])
         if torch.cuda.is_available():
             headers.extend([
                 'CUDA Mem',
@@ -1268,6 +1350,7 @@
 
     self_cpu_time_total = sum([event.self_cpu_time_total for event in events])
     cuda_time_total = sum([evt.self_cuda_time_total for evt in events])
+    xpu_time_total = sum([evt.xpu_time_total for evt in events])
     # Actual printing
     if header is not None:
         append('=' * line_length)
@@ -1298,6 +1381,10 @@
             format_time_share(evt.cpu_time_total, self_cpu_time_total) if not evt.is_async else 0,
             evt.cpu_time_total_str,  # CPU total
             evt.cpu_time_str,  # CPU time avg
+            # XPU time total %
+            format_time_share(evt.xpu_time_total, xpu_time_total),
+            evt.xpu_time_total_str,
+            evt.xpu_time_str,  # Sycl time avg
         ]
         if use_cuda:
             row_values.extend([
@@ -1314,6 +1401,12 @@
                 # Self CPU Mem Total
                 format_memory(evt.self_cpu_memory_usage),
             ])
+            row_values.extend([
+                # CPU Mem Total
+                format_memory(evt.xpu_memory_usage),
+                # Self CPU Mem Total
+                format_memory(evt.self_xpu_memory_usage),
+            ])
             if torch.cuda.is_available():
                 row_values.extend([
                     # CUDA Mem Total
@@ -1345,6 +1438,7 @@
 
     append(header_sep)
     append("Self CPU time total: {}".format(format_time(self_cpu_time_total)))
+    append("XPU time total: {}".format(format_time(xpu_time_total)))
     if use_cuda:
         append("CUDA time total: {}".format(format_time(cuda_time_total)))
     return ''.join(result)
Index: pytorch-1.7/torch/csrc/autograd/init.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/autograd/init.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
+++ pytorch-1.7/torch/csrc/autograd/init.cpp	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
@@ -39,6 +39,7 @@
       .value("Disabled", ProfilerState::Disabled)
       .value("CPU", ProfilerState::CPU)
       .value("CUDA", ProfilerState::CUDA)
+      .value("XPU", ProfilerState::XPU)
       .value("NVTX", ProfilerState::NVTX);
 
   py::class_<ProfilerConfig>(m, "ProfilerConfig")
@@ -52,10 +53,13 @@
       .def("device", &Event::device)
       .def("cpu_elapsed_us", &Event::cpuElapsedUs)
       .def("cuda_elapsed_us", &Event::cudaElapsedUs)
+      .def("xpu_elapsed_us", &Event::xpu_elapsed_us)
       .def("has_cuda", &Event::hasCuda)
+      .def("has_xpu", &Event::has_xpu)
       .def("shapes", &Event::shapes)
       .def("cpu_memory_usage", &Event::cpuMemoryUsage)
       .def("cuda_memory_usage", &Event::cudaMemoryUsage)
+      .def("xpu_memory_usage", &Event::xpuMemoryUsage)
       .def("handle", &Event::handle)
       .def("node_id", &Event::nodeId)
       .def("is_remote", &Event::isRemote)
Index: pytorch-1.7/torch/csrc/autograd/profiler.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/autograd/profiler.cpp	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
+++ pytorch-1.7/torch/csrc/autograd/profiler.cpp	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
@@ -67,10 +67,14 @@
       };
 
 CUDAStubs default_stubs;
+XPUStubs default_xpu_stubs;
 constexpr CUDAStubs* default_stubs_addr = &default_stubs;
+constexpr XPUStubs* default_xpu_stubs_addr = &default_xpu_stubs;
+// constant initialization, so it is guaranteed to be initialized before
 // Constant initialization, so it is guaranteed to be initialized before
 // static initialization calls which may invoke registerCUDAMethods
 static CUDAStubs* cuda_stubs = default_stubs_addr;
+static XPUStubs* xpu_stubs = default_xpu_stubs_addr;
 
 // We decompose the profiler logic into the following components:
 //
@@ -196,7 +200,7 @@
     return result;
   }
 
-  void mark(std::string name, bool include_cuda = true) {
+  void mark(std::string name, bool not_cpu = true) {
     if (config_.state == ProfilerState::Disabled) {
       return;
     }
@@ -207,12 +211,24 @@
           EventKind::Mark,
           at::StringView(std::move(name)),
           at::RecordFunction::currentThreadId(),
-          include_cuda && config_.state == ProfilerState::CUDA);
+        config_.state);
       evt.setNodeId(at::RecordFunction::getDefaultNodeId());
       getEventList().record(std::move(evt));
     }
   }
 
+  void mark_xpu(std::string name, XPUEventStub& xpu_event) {
+    if (config_.state == ProfilerState::Disabled) {
+      return;
+    }
+    getEventList().record(
+      EventKind::Mark,
+      at::StringView(std::move(name)),
+      at::RecordFunction::currentThreadId(),
+      config_.state,
+      xpu_event);
+  }
+
   void setOrAddRemoteProfiledEvents(
       std::vector<Event>&& remoteProfiledEvents) {
     // Lock to serialize access from multiple callback threads.
@@ -240,7 +256,7 @@
           EventKind::PushRange,
           fn.name(),
           at::RecordFunction::currentThreadId(),
-          record_cuda,
+          config_.state,
           fn.handle(),
           std::move(shapes),
           at::RecordFunction::getDefaultNodeId());
@@ -277,7 +293,7 @@
           EventKind::PopRange,
           at::StringView(""),
           at::RecordFunction::currentThreadId(),
-          record_cuda,
+          config_.state,
           fn.handle());
       evt.setNodeId(at::RecordFunction::getDefaultNodeId());
       getEventList(fn.threadId()).record(std::move(evt));
@@ -302,7 +318,7 @@
           EventKind::MemoryAlloc,
           at::StringView(""),
           thread_id,
-          config_.state == ProfilerState::CUDA);
+          config_.state);
       evt.updateMemoryStats(alloc_size, device);
       getEventList(thread_id).record(std::move(evt));
     }
@@ -477,6 +493,10 @@
   cuda_stubs = stubs;
 }
 
+void registerXPUMethods(XPUStubs* stubs) {
+  xpu_stubs = stubs;
+}
+
 ProfilerConfig::~ProfilerConfig() = default;
 
 at::IValue ProfilerConfig::toIValue() const {
@@ -550,6 +570,11 @@
   state->mark("__start_profile", false);
 }
 
+void mark_xpu(std::string name, XPUEventStub& xpu_event) {
+  auto state_ptr = getProfilerTLSState();
+  state_ptr->mark_xpu(name, xpu_event);
+}
+
 thread_event_lists disableProfiler(c10::optional<ProfilerDisableOptions> profilerDisableOptions) {
   auto cleanupTLSState = profilerDisableOptions ? profilerDisableOptions->cleanupTLSState : true;
   auto consolidate = profilerDisableOptions ? profilerDisableOptions->consolidate : true;
@@ -584,12 +609,17 @@
   state_ptr->setOrAddRemoteProfiledEvents(std::move(profiledEvents));
 }
 
-void Event::record(bool record_cuda) {
-  if (record_cuda) {
-    cuda_stubs->record(&device_, &cuda_event, &cpu_ns_);
-    return;
-  }
-  cpu_ns_ = getTime();
+void Event::record(ProfilerState state) {
+  switch(state) {
+    case ProfilerState::CUDA:
+      cuda_stubs->record(&device_, &cuda_event, &cpu_ns_);
+      break;
+      /*case ProfilerState::XPU:
+        break;*/
+    default:
+      cpu_ns_ = getTime();
+      break;
+  }
 }
 
 /* static */ Event Event::fromIValue(const at::IValue& eventIValue) {
@@ -693,8 +723,16 @@
   return cuda_stubs->elapsed(&cuda_event, &e.cuda_event);
 }
 
+double Event::xpu_elapsed_us() {
+  if(!has_xpu()) {
+    throw std::logic_error("Events were not recorded for XPU");
+  }
+  return xpu_stubs->elapsed(xpu_event);
+}
+
 CUDAStubs::~CUDAStubs() = default;
 
+XPUStubs::~XPUStubs() = default;
 
 static jit::CodeTemplate event_template(R"(
 {
Index: pytorch-1.7/torch/csrc/autograd/profiler.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/autograd/profiler.h	(revision f70bb7f5622b9703f0cf5c2c397d830b77c2f9ef)
+++ pytorch-1.7/torch/csrc/autograd/profiler.h	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
@@ -23,6 +23,14 @@
 struct CUevent_st;
 typedef std::shared_ptr<CUevent_st> CUDAEventStub;
 
+struct XPUEventStubBase {
+  virtual float elapsed() {
+    TORCH_CHECK(0, "no XPU instance ...");
+  }
+  virtual ~XPUEventStubBase() = default;
+} ;
+typedef std::shared_ptr<XPUEventStubBase> XPUEventStub;
+
 namespace torch { namespace autograd {
 
 struct Node;
@@ -65,6 +73,24 @@
 
 TORCH_API void registerCUDAMethods(CUDAStubs* stubs);
 
+struct TORCH_API XPUStubs {
+  virtual float elapsed(XPUEventStub event) {
+    fail();
+    return 0.f;
+  }
+  virtual bool enabled() {
+    return false;
+  }
+  virtual ~XPUStubs();
+
+private:
+  void fail() {
+    AT_ERROR("XPU used in profiler but not enabled.");
+  }
+};
+
+TORCH_API void registerXPUMethods(XPUStubs* stubs);
+
 constexpr inline size_t ceilToMultiple(size_t a, size_t b) {
   return ((a + b - 1) / b) * b;
 }
@@ -107,6 +133,7 @@
     Disabled,
     CPU, // CPU-only profiling
     CUDA, // CPU + CUDA events
+    XPU, // CPU + XPU events
     NVTX,  // only emit NVTX markers
 };
 
@@ -147,7 +174,7 @@
       EventKind kind,
       at::StringView name,
       uint16_t thread_id,
-      bool record_cuda,
+      ProfilerState state,
       at::RecordFunctionHandle handle = 0,
       std::vector<std::vector<int64_t>>&& shapes = {},
       int node_id = -1)
@@ -155,9 +182,25 @@
         kind_(kind),
         thread_id_(thread_id),
         handle_(handle),
+        xpu_kernel(false),
         shapes_(shapes),
         node_id_(node_id) {
-    record(record_cuda);
+    record(state);
+  }
+
+
+  Event(
+    EventKind kind,
+    at::StringView name,
+    uint16_t thread_id,
+    ProfilerState state,
+    XPUEventStub& event)
+    : name_(std::move(name)),
+      kind_(kind),
+      thread_id_(thread_id),
+      xpu_kernel(true),
+      xpu_event(event) {
+    record(state);
   }
 
   // Constructor to be used in conjunction with Event::fromIValue.
@@ -180,6 +223,7 @@
         kind_(kind),
         thread_id_(thread_id),
         handle_(handle),
+        xpu_kernel(false),
         shapes_(shapes),
         cpu_memory_usage_(cpu_memory_usage),
         cuda_memory_usage_(cuda_memory_usage),
@@ -202,7 +246,7 @@
   // Reconstructs an event from IValues given by toIValue.
   static Event fromIValue(const at::IValue& eventIValue);
 
-  void record(bool record_cuda);
+  void record(ProfilerState state);
   std::string kind() const {
     switch(kind_) {
       case EventKind::Mark: return "mark";
@@ -240,10 +284,16 @@
 
   double cudaElapsedUs(const Event& e) const;
 
+  double xpu_elapsed_us();
+
   bool hasCuda() const {
     return cuda_event != nullptr || (isRemote() && device_ != -1);
   }
 
+  bool has_xpu() const {
+    return xpu_kernel;
+  }
+
   int device() const {
     return device_;
   }
@@ -256,6 +306,8 @@
         device.type() == c10::DeviceType::MKLDNN ||
         device.type() == c10::DeviceType::IDEEP) {
       cpu_memory_usage_ = alloc_size;
+    } else if (device.type() == c10::DeviceType::XPU) {
+      xpu_memory_usage_ = alloc_size;
     } else {
       LOG(WARNING) << "Unsupported memory profiling device: " << device;
     }
@@ -269,6 +321,10 @@
     return cuda_memory_usage_;
   }
 
+  int64_t xpuMemoryUsage() const {
+    return xpu_memory_usage_;
+  }
+
   at::RecordFunctionHandle handle() const {
     return handle_;
   }
@@ -335,9 +391,11 @@
   uint64_t thread_id_;
   uint64_t fwd_thread_id_;
   at::RecordFunctionHandle handle_ {0};
+  bool xpu_kernel;
   std::vector<std::vector<int64_t>> shapes_;
   int64_t cpu_memory_usage_ = 0;
   int64_t cuda_memory_usage_ = 0;
+  int64_t xpu_memory_usage_ = 0;
   int device_ = -1;
   CUDAEventStub cuda_event = nullptr;
   int node_id_ = 0;
@@ -347,6 +405,7 @@
 
   std::vector<std::string> stack_;
   uint8_t scope_;
+  XPUEventStub xpu_event;
 };
 
 // a linked-list of fixed sized vectors, to avoid
@@ -403,6 +462,8 @@
 // Writes profiled events to a stream.
 TORCH_API void writeProfilerEventsToStream(std::ostream& out, const std::vector<Event*>& events);
 
+TORCH_API void mark_xpu(std::string name, XPUEventStub& xpu_event);
+
 // Usage:
 //   {
 //     RecordProfile guard("filename.trace");
Index: pytorch-1.7/aten/src/ATen/Context.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/Context.h	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
+++ pytorch-1.7/aten/src/ATen/Context.h	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
@@ -1,6 +1,6 @@
 #pragma once
 
-#include <ATen/core/ATenGeneral.h>
+#include <ATen/CPUGeneratorImpl.h>
 #include <ATen/Tensor.h>
 #include <ATen/Utils.h>
 #include <ATen/core/ATenGeneral.h>
@@ -9,6 +9,7 @@
 #include <ATen/core/LegacyTypeDispatch.h>
 #include <ATen/detail/CUDAHooksInterface.h>
 #include <ATen/detail/HIPHooksInterface.h>
+#include <ATen/detail/XPUHooksInterface.h>
 #include <c10/util/Exception.h>
 #include <c10/core/impl/DeviceGuardImplInterface.h>
 #include <c10/core/QEngine.h>
@@ -27,29 +28,35 @@
 
   const Generator& defaultGenerator(Device device) {
     DeviceType device_type = device.type();
-    initCUDAIfNeeded(device_type);
-    initHIPIfNeeded(device_type);
+    initDeviceIfNeeded(device_type);
     if (device_type == at::kCPU) {
       return at::detail::getDefaultCPUGenerator();
     } else if (device_type == at::kCUDA) {
       return at::detail::getCUDAHooks().getDefaultCUDAGenerator(device.index());
+    } else if (device_type == at::kXPU) {
+      return at::detail::getXPUHooks().getDefaultXPUGenerator(device.index());
     } else {
       AT_ERROR(DeviceTypeName(device_type), " device type not enabled.");
     }
   }
   Device getDeviceFromPtr(void* data, DeviceType device_type) {
-    initCUDAIfNeeded(device_type);
-    initHIPIfNeeded(device_type);
+    initDeviceIfNeeded(device_type);
     if (device_type == at::kCPU) {
       return DeviceType::CPU;
     } else if (device_type == at::kCUDA) {
       return at::detail::getCUDAHooks().getDeviceFromPtr(data);
+    } else if (device_type == at::kXPU) {
+      return at::detail::getXPUHooks().getDeviceFromPtr(data);
     } else {
       AT_ERROR(DeviceTypeName(device_type), " device type not enabled.");
     }
   }
   bool isPinnedPtr(void* data) {
-    return detail::getCUDAHooks().isPinnedPtr(data);
+    deviceExclusiveCheck();
+    if (hasCUDA()) {
+      return detail::getCUDAHooks().isPinnedPtr(data);
+    }
+    return detail::getXPUHooks().isPinnedPtr(data);
   }
   bool hasOpenMP() const;
   bool hasMKL() const;
@@ -70,6 +77,28 @@
   bool hasHIP() const {
     return detail::getHIPHooks().hasHIP();
   }
+  bool hasXPU() const {
+    return detail::getXPUHooks().hasXPU();
+  }
+  void deviceExclusiveCheck() {
+    int count = 0;
+    if (hasCUDA()) {
+      count++;
+    }
+    if (hasHIP()) {
+      count++;
+    }
+    if (hasXPU()) {
+      count++;
+    }
+    if (count > 1) {
+      throw std::runtime_error(
+          "Enabling CUDA, HIP and XPU at same time in ATen is not supported,"
+          "as HIP masqueradesto be CUDA (e.g., when you say CUDA, on a HIP build of ATen,"
+          "this actually means HIP, while XPU cannot work with two others."
+          "Rebuild PyTorch with one or the other disabled.");
+    }
+  }
   bool hasXLA() const {
     return c10::impl::hasDeviceGuardImpl(at::DeviceType::XLA);
   }
@@ -195,13 +224,10 @@
   bool releaseWeightsWhenPrepacking() const;
 
  private:
-  void initCUDAIfNeeded(DeviceType p) {
+  void initDeviceIfNeeded(DeviceType p) {
     if (p == DeviceType::CUDA) {
       lazyInitCUDA();
-    }
-  }
-  void initHIPIfNeeded(DeviceType p) {
-    if (p == DeviceType::HIP) {
+    } else if (p == DeviceType::HIP) {
       lazyInitHIP();
     }
   }
@@ -261,10 +287,19 @@
   return globalContext().hasHIP();
 }
 
+static inline bool hasXPU() {
+  return globalContext().hasXPU();
+}
+
 static inline bool hasXLA() {
   return globalContext().hasXLA();
 }
 
+
+static void deviceExclusiveCheck() {
+  globalContext().deviceExclusiveCheck();
+}
+
 // Despite its name, this function returns the number of *CUDA* GPUs.
 static inline size_t getNumGPUs() {
   // WARNING: DO NOT ADD LOGIC TO HANDLE OTHER DEVICE TYPES TO THIS
Index: pytorch-1.7/aten/src/ATen/Version.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/Version.cpp	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
+++ pytorch-1.7/aten/src/ATen/Version.cpp	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
@@ -171,6 +171,10 @@
     ss << detail::getCUDAHooks().showConfig();
   }
 
+  if (hasXPU()) {
+    ss << detail::getXPUHooks().showConfig();
+  }
+
   ss << "  - Build settings: ";
   for (const std::pair<std::string, std::string>& pair : caffe2::GetBuildOptions()) {
     if (!pair.second.empty()) {
Index: pytorch-1.7/aten/src/ATen/detail/XPUHooksInterface.cpp
===================================================================
--- pytorch-1.7/aten/src/ATen/detail/XPUHooksInterface.cpp	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
+++ pytorch-1.7/aten/src/ATen/detail/XPUHooksInterface.cpp	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
@@ -0,0 +1,29 @@
+#include <ATen/detail/XPUHooksInterface.h>
+
+#include <c10/util/Exception.h>
+
+#include <cstddef>
+#include <memory>
+#include <mutex>
+
+namespace at {
+namespace detail {
+
+static XPUHooksInterface *xpu_hooks = nullptr;
+
+const XPUHooksInterface &getXPUHooks() {
+  static std::once_flag once;
+  std::call_once(once, [] {
+    xpu_hooks =
+        XPUHooksRegistry()->Create("XPUHooks", XPUHooksArgs{}).release();
+    if (!xpu_hooks) {
+      xpu_hooks = new XPUHooksInterface();
+    }
+  });
+  return *xpu_hooks;
+}
+} // namespace detail
+
+C10_DEFINE_REGISTRY(XPUHooksRegistry, XPUHooksInterface, XPUHooksArgs)
+
+} // namespace at
Index: pytorch-1.7/aten/src/ATen/detail/XPUHooksInterface.h
===================================================================
--- pytorch-1.7/aten/src/ATen/detail/XPUHooksInterface.h	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
+++ pytorch-1.7/aten/src/ATen/detail/XPUHooksInterface.h	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
@@ -0,0 +1,83 @@
+#pragma once
+
+#include <ATen/core/Generator.h>
+#include <c10/core/Allocator.h>
+#include <c10/util/Exception.h>
+
+#include <c10/util/Registry.h>
+
+#include <cstddef>
+#include <functional>
+#include <memory>
+
+namespace at {
+class Context;
+}
+
+namespace at {
+
+constexpr const char* XPU_HELP =
+  "PyTorch splits its backend into two shared libraries: a CPU library "
+  "and a XPU library; this error has occurred because you are trying "
+  "to use some XPU functionality, but the XPU library has not been "
+  "loaded by the dynamic linker for some reason.  The XPU library MUST "
+  "be loaded, EVEN IF you don't directly use any symbols from the XPU library!";
+
+struct CAFFE2_API XPUHooksInterface {
+  virtual ~XPUHooksInterface() {}
+
+  virtual void initXPU() const {
+    TORCH_CHECK(false, "Cannot initialize XPU without XPU library.", XPU_HELP);
+  }
+
+  virtual bool hasXPU() const {
+    return false;
+  }
+
+  virtual bool hasOneMKL() const {
+    return false;
+  }
+
+  virtual bool hasOneDNN() const {
+    return false;
+  }
+
+  virtual std::string showConfig() const {
+    TORCH_CHECK(false, "Cannot query detailed XPU version without XPU library. ", XPU_HELP);
+  }
+
+  virtual int64_t getCurrentDevice() const {
+    return -1;
+  }
+
+  virtual int getDeviceCount() const {
+    return 0;
+  }
+
+  virtual Device getDeviceFromPtr(void* data) const {
+    TORCH_CHECK(false, "Cannot get device of pointer on XPU without XPU library.", XPU_HELP);
+  }
+
+  virtual bool isPinnedPtr(void* data) const {
+    return false;
+  }
+
+  virtual Allocator* getPinnedMemoryAllocator() const {
+    TORCH_CHECK(false, "Pinned Memory requires XPU library support", XPU_HELP);
+  }
+
+  virtual const Generator& getDefaultXPUGenerator(DeviceIndex device_index = -1) const {
+    TORCH_CHECK(false, "Cannot get default XPU generator without XPU library.", XPU_HELP);
+  }
+};
+
+struct CAFFE2_API XPUHooksArgs {};
+
+C10_DECLARE_REGISTRY(XPUHooksRegistry, XPUHooksInterface, XPUHooksArgs);
+#define REGISTER_XPU_HOOKS(clsname) \
+  C10_REGISTER_CLASS(XPUHooksRegistry, clsname, clsname)
+
+namespace detail {
+CAFFE2_API const XPUHooksInterface& getXPUHooks();
+} // namespace detail
+} // namespace at
Index: pytorch-1.7/aten/src/ATen/native/Memory.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/native/Memory.cpp	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
+++ pytorch-1.7/aten/src/ATen/native/Memory.cpp	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
@@ -10,17 +10,30 @@
 namespace native {
 
 bool is_pinned(const Tensor& self) {
-  return detail::getCUDAHooks().isPinnedPtr(self.storage().data());
+  deviceExclusiveCheck();
+  if (hasCUDA()) {
+    return detail::getCUDAHooks().isPinnedPtr(self.storage().data());
+  }
+  return detail::getXPUHooks().isPinnedPtr(self.storage().data());
 }
 
 Tensor pin_memory(const Tensor& self) {
   if (self.options().backend() != Backend::CPU) {
     AT_ERROR("cannot pin '", self.toString(), "' only dense CPU tensors can be pinned");
   }
+
+  deviceExclusiveCheck();
   if (self.is_pinned()) {
     return self;
   }
-  auto* allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+
+  c10::Allocator* allocator;
+  if (hasCUDA()) {
+    allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+  } else if (hasXPU()) {
+    allocator = detail::getXPUHooks().getPinnedMemoryAllocator();
+  }
+
   auto storage = Storage(
       Storage::use_byte_size_t(),
       detail::computeStorageNbytes(
Index: pytorch-1.7/aten/src/ATen/native/TensorFactories.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/native/TensorFactories.cpp	(revision a9da004a99516d26d4a0326a4ba87f57aa796c5e)
+++ pytorch-1.7/aten/src/ATen/native/TensorFactories.cpp	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
@@ -179,7 +179,12 @@
 
   c10::Allocator* allocator;
   if (options.pinned_memory()) {
-    allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+    deviceExclusiveCheck();
+    if (hasCUDA()) {
+      allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();
+    } else if (hasXPU()) {
+      allocator = detail::getXPUHooks().getPinnedMemoryAllocator();
+    }
   } else {
     allocator = at::getCPUAllocator();
   }
Index: pytorch-1.7/torch/_utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/_utils.py	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
+++ pytorch-1.7/torch/_utils.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
@@ -1,11 +1,15 @@
 import torch
 import torch._six
+from .runtime import current_runtime
 from typing import Optional
 import warnings
 from collections import defaultdict
 import sys
 import traceback
-
+from torch.runtime import (
+    get_runtime_attr,
+    get_device_type
+)
 
 def _type(self, dtype=None, non_blocking=False, **kwargs):
     """Returns the type if `dtype` is not provided, else casts this object to
@@ -429,33 +433,24 @@
 
 
 def _get_available_device_type():
-    if torch.cuda.is_available():
-        return "cuda"
-    # add more available device types here
-    return None
-
-
-def _get_device_attr(get_member):
-    device_type = _get_available_device_type()
-    if device_type.lower() == "cuda":
-        return get_member(torch.cuda)
-    # add more available device types here
+    if get_runtime_attr(lambda m: m.is_available()):
+        return get_device_type()
     return None
 
 
 def _get_current_device_index():
     # current device index
-    return _get_device_attr(lambda m: m.current_device())
+    return get_runtime_attr(lambda m: m.current_device())
 
 
 def _get_all_device_indices():
     # all device index
-    return _get_device_attr(lambda m: list(range(m.device_count())))
+    return get_runtime_attr(lambda m: list(range(m.device_count())))
 
 
 def _get_devices_properties(device_ids):
     # all device properties
-    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]
+    return [get_runtime_attr(lambda m: m.get_device_properties(i)) for i in device_ids]
 
 
 def _get_device_index(device, optional=False, allow_cpu=False) -> int:
@@ -471,8 +466,8 @@
     If :attr:`device` is a Python integer, it is returned as is.
 
     If :attr:`device` is ``None``, this will return the current default
-    device of the supported runtime platform if :attr:`optional` is ``True``.
-    i.e., the current default CUDA device will be returned if CUDA runtime is supported.
+    device of the current runtime platform if :attr:`optional` is ``True``.
+    i.e., the current default CUDA device will be returned if current runtime is CUDA.
     """
     if isinstance(device, str):
         device = torch.device(device)
@@ -491,3 +486,25 @@
             raise ValueError('Expected a torch.device with a specified index '
                              'or an integer, but got:{}'.format(device))
     return device_idx
+
+
+def _get_device_type(device, optional=False, allow_cpu=False) -> str:
+    r"""TODO
+    """
+    if isinstance(device, str):
+        device = torch.device(device)
+    device_type: Optional[str]
+    device_type = None
+    if isinstance(device, torch.device):
+        if not allow_cpu and device.type == 'cpu':
+            raise ValueError('Expected a non cpu device, but got: {}'.format(device))
+        device_type = device.type
+    if isinstance(device, int):
+        device_type = _get_available_device_type()
+    if device_type is None:
+        if optional:
+            device_type = _get_available_device_type()
+        else:
+            raise ValueError('Expected a torch.device with a specified index '
+                             'or an integer or string, but got:{}'.format(device))
+    return device_type
Index: pytorch-1.7/torch/cuda/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/cuda/__init__.py	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
+++ pytorch-1.7/torch/cuda/__init__.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
@@ -8,6 +8,7 @@
 :ref:`cuda-semantics` has more details about working with CUDA.
 """
 
+import sys
 import contextlib
 import os
 import torch
@@ -536,3 +537,10 @@
 from . import profiler
 from . import nvtx
 from . import amp
+
+from torch.runtime import register_runtime
+
+
+current_module = sys.modules[__name__]
+register_runtime('cuda', current_module)
+
Index: pytorch-1.7/torch/nn/parallel/data_parallel.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/nn/parallel/data_parallel.py	(revision c0ab39ee8148b1aa3905f71afa5f4346eeb8ef9f)
+++ pytorch-1.7/torch/nn/parallel/data_parallel.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
@@ -10,6 +10,7 @@
     _get_all_device_indices,
     _get_available_device_type,
     _get_device_index,
+    _get_device_type,
     _get_devices_properties
 )
 
@@ -121,14 +122,19 @@
     def __init__(self, module, device_ids=None, output_device=None, dim=0):
         super(DataParallel, self).__init__()
 
-        device_type = _get_available_device_type()
-        if device_type is None:
-            self.module = module
-            self.device_ids = []
-            return
-
-        if device_ids is None:
+        if device_ids is None:
+            device_type = _get_available_device_type()
+            if device_type is None:
+                self.module = module
+                self.device_ids = []
+                return
             device_ids = _get_all_device_indices()
+        else:
+            distinct_device_types = {_get_device_type(device_id, optional=True) for device_id in device_ids}
+            assert len(distinct_device_types) == 1, (
+                "DataParallel's device_ids must be the same type of devices, but device_ids locate in {}."
+            ).format(distinct_device_types)
+            device_type = list(distinct_device_types)[0]
 
         if output_device is None:
             output_device = device_ids[0]
@@ -192,10 +198,15 @@
     if not isinstance(inputs, tuple):
         inputs = (inputs,)
 
-    device_type = _get_available_device_type()
-
     if device_ids is None:
+        device_type = _get_available_device_type()
         device_ids = _get_all_device_indices()
+    else:
+        distinct_device_types = {_get_device_type(device_id, optional=True) for device_id in device_ids}
+        assert len(distinct_device_types) == 1, (
+            "DataParallel's device_ids must be the same type of devices, but device_ids locate in {}."
+        ).format(distinct_device_types)
+        device_type = list(distinct_device_types)[0]
 
     if output_device is None:
         output_device = device_ids[0]
Index: pytorch-1.7/torch/runtime/__init__.py
===================================================================
--- pytorch-1.7/torch/runtime/__init__.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/runtime/__init__.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
@@ -0,0 +1,53 @@
+from collections import deque
+import torch
+
+_current_runtime = 'cpu'
+
+_run_time_dict = {}
+
+
+def _available_runtimes():
+    print(_run_time_dict)
+
+
+def register_runtime(name, module):
+    """Register a runtime module for backends to expose the APIs not being included in ATen."""
+    global _current_runtime
+
+    if name in _run_time_dict:
+        raise RuntimeError("Runtime {} already registered with {}.".format(name, _run_time_dict[name]))
+    if hasattr(torch, name):
+        raise RuntimeError("torch attribute {} already exists.".format(name))
+    try:
+        # the runtime type should have the same name as the device type
+        device = torch.device(name)
+    except RuntimeError as e:
+        raise RuntimeError("Unsupported runtime type {}.".format(name)) from e
+
+
+    attributes = ['current_device', 'device_count', 'get_device_properties', 'is_available']
+
+    def check_attributes(attr):
+        if not hasattr(module, attr):
+            raise RuntimeError("There is no {} in runtime {}.".format(attr, name))
+
+    for attr in attributes:
+        check_attributes(attr)
+
+    _run_time_dict[name] = module
+    setattr(torch, name, module)
+    _current_runtime = name
+
+
+def current_runtime():
+    global _current_runtime
+    return _run_time_dict[_current_runtime]
+
+
+def get_device_type() -> str:
+    return _current_runtime
+
+
+def get_runtime_attr(get_member):
+    runtime = current_runtime()
+    return get_member(runtime)
Index: pytorch-1.7/aten/src/ATen/core/ivalue.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/core/ivalue.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/aten/src/ATen/core/ivalue.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -97,6 +97,8 @@
       return RRefType::create(toRRef()->type());
     case Tag::Device:
       return DeviceObjType::get();
+    case Tag::Stream:
+      return StreamObjType::get();
     case Tag::Object:
       return toObjectRef().type();
     case Tag::PyObject:
@@ -269,6 +271,8 @@
       return rhs.isGenericDict() && lhs.toGenericDict() == rhs.toGenericDict();
     case Tag::Tuple:
       return rhs.isTuple() && *lhs.toTuple() == *rhs.toTuple();
+    case Tag::Stream:
+      return rhs.isStream() && lhs.toStream() == lhs.toStream();
     case Tag::Device:
       return rhs.isDevice() && lhs.toDevice() == rhs.toDevice();
     case Tag::GenericList:
@@ -634,6 +638,8 @@
       return out << "Uninitialized";
     case IValue::Tag::Device:
       return out << v.toDevice();
+    case IValue::Tag::Stream:
+      return out << v.toStream();
     case IValue::Tag::GenericDict:
       return printDict(out, v.toGenericDict(), formatter);
     case IValue::Tag::PyObject: {
Index: pytorch-1.7/aten/src/ATen/core/ivalue.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/core/ivalue.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/aten/src/ATen/core/ivalue.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -98,6 +98,7 @@
   _(GenericDict)             \
   _(Future)                  \
   _(Device)                  \
+  _(Stream)                  \
   _(Object)                  \
   _(PyObject)                \
   _(Uninitialized)           \
@@ -551,6 +552,15 @@
     return c10::Device(payload.as_device.type, payload.as_device.index);
   }
 
+  //Stream
+  IValue(c10::Stream stream)
+    : tag(Tag::Stream), is_intrusive_ptr(false) {
+    payload.as_int = stream.pack();
+  }
+  c10::Stream toStream() &&;
+  c10::Stream toStream() const &;
+  bool isStream() const { return Tag::Stream == tag; }
+
   // ScalarType
   IValue(ScalarType t)
   : IValue(static_cast<std::underlying_type<ScalarType>::type>(t)) {}
Index: pytorch-1.7/aten/src/ATen/core/ivalue_inl.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/core/ivalue_inl.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/aten/src/ATen/core/ivalue_inl.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -130,6 +130,12 @@
   AT_ASSERT(isTensor(), "Expected Tensor but got ", tagKind());
   return at::Tensor(toIntrusivePtr<at::TensorImpl, at::UndefinedTensorImpl>());
 }
+inline c10::Stream IValue::toStream() && {
+  return c10::Stream::unpack(payload.as_int);
+}
+inline c10::Stream IValue::toStream() const & {
+  return c10::Stream::unpack(payload.as_int);
+}
 inline c10::intrusive_ptr<caffe2::Blob> IValue::toBlob() && {
   AT_ASSERT(isBlob(), "Expected Blob but got ", tagKind());
   return moveToIntrusivePtr<caffe2::Blob>();
@@ -645,6 +651,7 @@
   return this->method_name(); \
 }
 DEFINE_TO(at::Tensor, toTensor)
+DEFINE_TO(c10::Stream, toStream)
 DEFINE_TO(float, toDouble)
 DEFINE_TO(double, toDouble)
 DEFINE_TO(unsigned char, toInt)
Index: pytorch-1.7/aten/src/ATen/core/jit_type.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/core/jit_type.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/aten/src/ATen/core/jit_type.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -47,6 +47,7 @@
   _(OptionalType)           \
   _(VarType)                \
   _(DeviceObjType)          \
+  _(StreamObjType)          \
   _(FunctionType)           \
   _(ClassType)              \
   _(PyObjectType)           \
@@ -1543,6 +1544,28 @@
   DeviceObjType() : Type(TypeKind::DeviceObjType) {}
 };
 
+struct StreamObjType;
+using StreamObjTypePtr = std::shared_ptr<StreamObjType>;
+// This type represents a Generator
+struct CAFFE2_API StreamObjType : public Type {
+  static StreamObjTypePtr create() {
+    return StreamObjTypePtr(
+      new StreamObjType()); // NOLINT(modernize-make-shared)
+  }
+  bool operator==(const Type& rhs) const override {
+    return rhs.kind() == kind();
+  }
+  std::string str() const override {
+    return "Stream";
+  }
+  static const TypeKind Kind = TypeKind::StreamObjType;
+  // global singleton
+  static StreamObjTypePtr get();
+
+private:
+  StreamObjType() : Type(TypeKind::StreamObjType) {}
+};
+
 struct VarType;
 using VarTypePtr = std::shared_ptr<VarType>;
 // This type represents a type variable, used in FunctionSchema
@@ -1719,6 +1742,12 @@
     return TensorType::get();
   }
 };
+template <>
+struct getTypePtr_<c10::Stream> final {
+  static TypePtr call() {
+    return StreamObjType::get();
+  }
+};
 template <>
 struct getTypePtr_<double> final {
   static TypePtr call() {
Index: pytorch-1.7/aten/src/ATen/core/type.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/core/type.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/aten/src/ATen/core/type.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -151,6 +151,10 @@
   static auto value = DeviceObjType::create();
   return value;
 }
+StreamObjTypePtr StreamObjType::get() {
+  static auto value = StreamObjType::create();
+  return value;
+}
 ScalarTypeTypePtr ScalarTypeType::get() {
 static auto value = ScalarTypeType::create();
 return value;
Index: pytorch-1.7/aten/src/ATen/native/cuda/RecordStream.cu
===================================================================
--- pytorch-1.7/aten/src/ATen/native/cuda/RecordStream.cu	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
+++ pytorch-1.7/aten/src/ATen/native/cuda/RecordStream.cu	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -0,0 +1,7 @@
+#include <ATen/ATen.h>
+#include <c10/cuda/CUDACachingAllocator.h>
+namespace at { namespace native {
+void record_stream_cuda(Tensor& self, c10::Stream stream) {
+  c10::cuda::CUDACachingAllocator::recordStream(self.storage().data_ptr(), at::cuda::CUDAStream::unpack(stream.pack()));
+}
+}}  // namespace at::native
Index: pytorch-1.7/aten/src/ATen/native/native_functions.yaml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/aten/src/ATen/native/native_functions.yaml	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/aten/src/ATen/native/native_functions.yaml	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -8107,6 +8107,12 @@
   variants: function, method
   device_guard: False
 
+- func: record_stream(Tensor(a!) self, Stream s) -> ()
+  use_c10_dispatcher: full
+  variants: method
+  dispatch:
+    CUDA: record_stream_cuda
+
 - func: isposinf(Tensor self) -> Tensor
   use_c10_dispatcher: full
   variants: function, method
Index: pytorch-1.7/test/test_overrides.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/test/test_overrides.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/test/test_overrides.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -575,6 +575,8 @@
                     func_args.append(False)
                 elif t.startswith('int') or t in {'Dimname', 'DimnameList'}:
                     func_args.append(0)
+                elif t in {'Stream'}:
+                    func_args.append(torch.Stream())
                 elif t.startswith('float') or t == 'double':
                     func_args.append(1.0)
                 elif t in {'Generator', 'MemoryFormat', 'TensorOptions'}:
Index: pytorch-1.7/tools/autograd/gen_python_functions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/autograd/gen_python_functions.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/tools/autograd/gen_python_functions.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -264,6 +264,7 @@
 UNPACK_METHODS = {
     'const Tensor &': 'tensor',
     'Tensor &': 'tensor',
+    'Stream': 'stream',
     'c10::optional<Tensor>': 'optionalTensor',
     'const c10::optional<Tensor>&': 'optionalTensor',
     'c10::optional<Generator>': 'generator',
Index: pytorch-1.7/tools/autograd/gen_variable_type.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/autograd/gen_variable_type.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/tools/autograd/gen_variable_type.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -146,6 +146,8 @@
     # Functions that return integers should not have output that require gradients
     'argmax', 'argmin', 'argsort', 'searchsorted',
     'bucketize'
+    # Functions return none are not differentiable
+    'record_stream',
 }
 
 # The C -> R functions at the time of adding this are still being audited and tested
Index: pytorch-1.7/tools/build_variables.bzl
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/build_variables.bzl	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/tools/build_variables.bzl	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -476,6 +476,7 @@
     "torch/csrc/python_dimname.cpp",
     "torch/csrc/Size.cpp",
     "torch/csrc/Storage.cpp",
+    "torch/csrc/Stream.cpp",
     "torch/csrc/TypeInfo.cpp",
     "torch/csrc/api/src/python/init.cpp",
     "torch/csrc/autograd/functions/init.cpp",
Index: pytorch-1.7/tools/codegen/api/cpp.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/codegen/api/cpp.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/tools/codegen/api/cpp.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -45,7 +45,7 @@
         elif t.name in [BaseTy.bool, BaseTy.QScheme, BaseTy.Scalar,
                         BaseTy.ScalarType, BaseTy.Generator, BaseTy.Storage,
                         BaseTy.Layout, BaseTy.Device, BaseTy.MemoryFormat,
-                        BaseTy.Dimname, BaseTy.ConstQuantizerPtr]:
+                        BaseTy.Dimname, BaseTy.Stream, BaseTy.ConstQuantizerPtr]:
             # These C++ names line up with their schema names
             return t.name.name
         else:
Index: pytorch-1.7/tools/codegen/model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/codegen/model.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/tools/codegen/model.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -455,6 +455,7 @@
     'MemoryFormat',
     'QScheme',
     'Storage',
+    'Stream',
     'ConstQuantizerPtr',  # TODO: rename
 ))
 
Index: pytorch-1.7/tools/pyi/gen_pyi.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/tools/pyi/gen_pyi.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/tools/pyi/gen_pyi.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -146,7 +146,8 @@
         'Dimname': 'Union[str, ellipsis, None]',
         'DimnameList': 'Sequence[Union[str, ellipsis, None]]',
         'QScheme': '_qscheme',
-        'ArrayRef<double>' : 'Sequence[float]'
+        'ArrayRef<double>' : 'Sequence[float]',
+        'Stream': 'Stream',
     }[typename]
 
     return typename
Index: pytorch-1.7/torch/_C/__init__.pyi.in
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/_C/__init__.pyi.in	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/_C/__init__.pyi.in	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -35,6 +35,13 @@
 
     def __reduce__(self) -> Tuple[Any, ...]: ...  # THPDevice_reduce
 
+# Defined in torch/csrc/Stream.cpp
+class Stream:
+    _cdata: _int  # Stream handle
+    device: device # The device of the stream
+
+    ...
+
 # Defined in torch/csrc/Size.cpp
 class Size(Tuple[_int, ...]):
     # TODO: __reduce__
@@ -650,6 +657,10 @@
     @staticmethod
     def get() -> DeviceObjType: ...
 
+class StreamObjType(JitType):
+    @staticmethod
+    def get() -> StreamObjType: ...
+
 class ListType(JitType):
     def __init__(self, a: JitType) -> None: ...
     def getElementType(self) -> JitType: ...
Index: pytorch-1.7/torch/csrc/Module.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/Module.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/Module.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -23,6 +23,7 @@
 #include <torch/csrc/THP.h>
 #include <torch/csrc/DynamicTypes.h>
 #include <torch/csrc/Device.h>
+#include <torch/csrc/Stream.h>
 #include <torch/csrc/Dtype.h>
 #include <torch/csrc/DataLoader.h>
 #include <torch/csrc/Generator.h>
@@ -722,6 +723,7 @@
   THPMemoryFormat_init(module);
   THPQScheme_init(module);
   THPDevice_init(module);
+  THPStream_init(module);
   ASSERT_TRUE(THPVariable_initModule(module));
   ASSERT_TRUE(THPFunction_initModule(module));
   ASSERT_TRUE(THPEngine_initModule(module));
Index: pytorch-1.7/torch/csrc/Stream.cpp
===================================================================
--- pytorch-1.7/torch/csrc/Stream.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
+++ pytorch-1.7/torch/csrc/Stream.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -0,0 +1,117 @@
+#include <pybind11/pybind11.h>
+#include <torch/csrc/Device.h>
+#include <torch/csrc/THP.h>
+#include <torch/csrc/utils/python_arg_parser.h>
+
+#include <structmember.h>
+
+PyTypeObject *THPStreamClass = nullptr;
+
+static PyObject* THPStream_pynew(
+  PyTypeObject *type, PyObject *args, PyObject *kwargs) {
+  HANDLE_TH_ERRORS
+  uint64_t cdata = 0;
+  static char *kwlist[] = {"_cdata", nullptr};
+  if (!PyArg_ParseTupleAndKeywords(
+    args, kwargs, "|K", kwlist, &cdata)) {
+    return nullptr;
+  }
+
+  THPObjectPtr ptr(type->tp_alloc(type, 0));
+  if (!ptr) {
+    return nullptr;
+  }
+
+  THPStream* self = (THPStream *)ptr.get();
+  self->cdata = cdata;
+  return (PyObject *)ptr.release();
+  END_HANDLE_TH_ERRORS
+}
+
+static void THPStream_dealloc(THPStream *self) {
+  Py_TYPE(self)->tp_free((PyObject*)self);
+}
+
+static PyObject * THPStream_get_device(THPStream *self, void *unused) {
+  HANDLE_TH_ERRORS
+  return THPDevice_New(c10::Stream::unpack(self->cdata).device());
+  END_HANDLE_TH_ERRORS
+}
+
+static PyObject * THPStream_eq(THPStream *self, THPStream *other) {
+  HANDLE_TH_ERRORS
+  return PyBool_FromLong(self->cdata == other->cdata);
+  END_HANDLE_TH_ERRORS
+}
+
+static struct PyMemberDef THPStream_members[] = {
+  {(char*)"_cdata",
+    T_ULONGLONG, offsetof(THPStream, cdata), READONLY, nullptr},
+  {nullptr}
+};
+
+static struct PyGetSetDef THPStream_properties[] = {
+  {"device", (getter)THPStream_get_device, nullptr, nullptr, nullptr},
+  {nullptr}
+};
+
+static PyMethodDef THPStream_methods[] = {
+  {(char*)"__eq__", (PyCFunction)THPStream_eq, METH_O, nullptr},
+  {nullptr}
+};
+
+PyTypeObject THPStreamType = {
+  PyVarObject_HEAD_INIT(nullptr, 0)
+  "torch.Stream",                        /* tp_name */
+  sizeof(THPStream),                     /* tp_basicsize */
+  0,                                     /* tp_itemsize */
+  (destructor)THPStream_dealloc,         /* tp_dealloc */
+  0,                                     /* tp_vectorcall_offset */
+  0,                                     /* tp_getattr */
+  0,                                     /* tp_setattr */
+  0,                                     /* tp_reserved */
+  0,                                     /* tp_repr */
+  0,                                     /* tp_as_number */
+  0,                                     /* tp_as_sequence */
+  0,                                     /* tp_as_mapping */
+  0,                                     /* tp_hash  */
+  0,                                     /* tp_call */
+  0,                                     /* tp_str */
+  0,                                     /* tp_getattro */
+  0,                                     /* tp_setattro */
+  0,                                     /* tp_as_buffer */
+  Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */
+  nullptr,                                  /* tp_doc */
+  0,                                     /* tp_traverse */
+  0,                                     /* tp_clear */
+  0,                                     /* tp_richcompare */
+  0,                                     /* tp_weaklistoffset */
+  0,                                     /* tp_iter */
+  0,                                     /* tp_iternext */
+  THPStream_methods,                     /* tp_methods */
+  THPStream_members,                     /* tp_members */
+  THPStream_properties,                  /* tp_getset */
+  0,                                     /* tp_base */
+  0,                                     /* tp_dict */
+  0,                                     /* tp_descr_get */
+  0,                                     /* tp_descr_set */
+  0,                                     /* tp_dictoffset */
+  0,                                     /* tp_init */
+  0,                                     /* tp_alloc */
+  THPStream_pynew,                       /* tp_new */
+};
+
+
+void THPStream_init(PyObject *module)
+{
+  THPStreamClass = &THPStreamType;
+  Py_TYPE(&THPStreamType) = &PyType_Type;
+  if (PyType_Ready(&THPStreamType) < 0) {
+    throw python_error();
+  }
+  Py_INCREF(&THPStreamType);
+  if (PyModule_AddObject(
+      module, "Stream", (PyObject *)&THPStreamType) < 0) {
+    throw python_error();
+  }
+}
Index: pytorch-1.7/torch/csrc/Stream.h
===================================================================
--- pytorch-1.7/torch/csrc/Stream.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
+++ pytorch-1.7/torch/csrc/Stream.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -0,0 +1,18 @@
+#ifndef THP_STREAM_INC
+#define THP_STREAM_INC
+
+#include <torch/csrc/python_headers.h>
+
+struct THPStream {
+  PyObject_HEAD
+  uint64_t cdata;
+};
+extern PyTypeObject *THPStreamClass;
+
+void THPStream_init(PyObject *module);
+
+inline bool THPStream_Check(PyObject* obj) {
+  return THPStreamClass && PyObject_IsInstance(obj, (PyObject*)THPStreamClass);
+}
+
+#endif // THP_STREAM_INC
Index: pytorch-1.7/torch/csrc/cuda/Stream.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/cuda/Stream.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/cuda/Stream.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -101,13 +101,10 @@
 }
 
 static struct PyMemberDef THCPStream_members[] = {
-  {(char*)"_cdata",
-    T_ULONGLONG, offsetof(THCPStream, cdata), READONLY, nullptr},
   {nullptr}
 };
 
 static struct PyGetSetDef THCPStream_properties[] = {
-  {"device", (getter)THCPStream_get_device, nullptr, nullptr, nullptr},
   {"cuda_stream",
     (getter)THCPStream_get_cuda_stream, nullptr, nullptr, nullptr},
   {"priority", (getter)THCPStream_get_priority, nullptr, nullptr, nullptr},
@@ -154,7 +151,7 @@
   0,                                     /* tp_iternext */
   THCPStream_methods,                    /* tp_methods */
   THCPStream_members,                    /* tp_members */
-  THCPStream_properties,                /* tp_getset */
+  THCPStream_properties,                 /* tp_getset */
   0,                                     /* tp_base */
   0,                                     /* tp_dict */
   0,                                     /* tp_descr_get */
@@ -168,6 +165,8 @@
 
 void THCPStream_init(PyObject *module)
 {
+  Py_INCREF(THPStreamClass);
+  THCPStreamType.tp_base = THPStreamClass;
   THCPStreamClass = (PyObject*)&THCPStreamType;
   if (PyType_Ready(&THCPStreamType) < 0) {
     throw python_error();
Index: pytorch-1.7/torch/csrc/cuda/Stream.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/cuda/Stream.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/cuda/Stream.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -1,13 +1,12 @@
 #ifndef THCP_STREAM_INC
 #define THCP_STREAM_INC
 
+#include <torch/csrc/Stream.h>
 #include <c10/cuda/CUDAStream.h>
 #include <torch/csrc/python_headers.h>
 #include <THC/THC.h>
 
-struct THCPStream {
-  PyObject_HEAD
-  uint64_t cdata;
+struct THCPStream : THPStream{
   at::cuda::CUDAStream cuda_stream;
 };
 extern PyObject *THCPStreamClass;
Index: pytorch-1.7/torch/csrc/jit/frontend/schema_type_parser.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/frontend/schema_type_parser.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/frontend/schema_type_parser.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -24,6 +24,7 @@
 using c10::QSchemeType;
 using c10::QuantizerType;
 using c10::RRefType;
+using c10::StreamObjType;
 using c10::StringType;
 using c10::Symbol;
 using c10::TensorType;
@@ -48,6 +49,7 @@
                         // parser, it should use the custom class mechanism
                         // instead. @jerryzh
       {"Device", DeviceObjType::get()},
+      {"Stream", StreamObjType::get()},
       {"Scalar", NumberType::get()},
       {"str", StringType::get()},
       {"float", FloatType::get()},
Index: pytorch-1.7/torch/csrc/jit/frontend/string_to_type.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/frontend/string_to_type.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/frontend/string_to_type.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -11,6 +11,7 @@
       {"bool", BoolType::get()},
       {"str", StringType::get()},
       {"Device", DeviceObjType::get()},
+      {"Stream", StreamObjType::get()},
       // technically this is not a python type but we need it when
       // parsing serialized methods that use implicit conversions to Scalar
       {"number", NumberType::get()},
Index: pytorch-1.7/torch/csrc/jit/frontend/tracer.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/frontend/tracer.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/frontend/tracer.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -614,6 +614,9 @@
 void addInputs(Node* n, const char* name, at::Device value) {
   detail::genericAddInput(n, value);
 }
+void addInputs(Node* n, const char* name, c10::Stream stream) {
+  detail::genericAddInput(n, static_cast<int64_t>(stream.pack()));
+}
 void addInputs(Node* n, const char* name, at::Layout value) {
   detail::genericAddInput(n, static_cast<int64_t>(value));
 }
Index: pytorch-1.7/torch/csrc/jit/frontend/tracer.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/frontend/tracer.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/frontend/tracer.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -271,6 +271,7 @@
     const char* name,
     const c10::optional<std::string>& value);
 TORCH_API void addInputs(Node* n, const char* name, at::Device value);
+TORCH_API void addInputs(Node* n, const char* name, c10::Stream stream);
 TORCH_API void addInputs(Node* n, const char* name, at::Layout value);
 TORCH_API void addInputs(Node* n, const char* name, at::ScalarType value);
 TORCH_API void addInputs(
Index: pytorch-1.7/torch/csrc/jit/ir/constants.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/ir/constants.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/ir/constants.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -110,6 +110,10 @@
     ss << val.toDevice();
     n->s_(attr::value, ss.str());
     n->output()->setType(DeviceObjType::get());
+  } else if (val.isStream()) {
+    auto stream = val.toStream();
+    n->i_(attr::value, stream.pack());
+    n->output()->setType(StreamObjType::get());
   } else if (val.isNone()) {
     n->output()->setType(NoneType::get());
   } else if (val.isTuple()) {
@@ -179,6 +183,9 @@
   } else if (type == DeviceObjType::get()) {
     auto d = c10::Device(node->s(attr::value));
     return d;
+  } else if (type == StreamObjType::get()) {
+    auto s = c10::Stream::unpack(node->i(attr::value));
+    return s;
   } else if (node->mustBeNone()) {
     return IValue();
   } else if (type->cast<EnumType>()) {
Index: pytorch-1.7/torch/csrc/jit/python/pybind_utils.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/python/pybind_utils.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/python/pybind_utils.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -10,6 +10,7 @@
 #include <torch/csrc/Dtype.h>
 #include <torch/csrc/Layout.h>
 #include <torch/csrc/QScheme.h>
+#include <torch/csrc/Stream.h>
 #include <torch/csrc/WindowsTorchApiMacro.h>
 #include <torch/csrc/jit/api/module.h>
 #include <torch/csrc/jit/frontend/schema_matching.h>
@@ -282,6 +283,8 @@
     return InferredType(IntType::get());
   } else if (THPDevice_Check(input.ptr())) {
     return InferredType(DeviceObjType::get());
+  } else if (THPStream_Check(input.ptr())) {
+    return InferredType(StreamObjType::get());
   } else if (THPDtype_Check(input.ptr())) {
     return InferredType(IntType::get());
   } else if (THPQScheme_Check(input.ptr())) {
@@ -577,6 +580,10 @@
       auto device = reinterpret_cast<THPDevice*>(obj.ptr());
       return device->device;
     }
+    case TypeKind::StreamObjType: {
+      auto stream = reinterpret_cast<THPStream*>(obj.ptr());
+      return static_cast<int64_t>(stream->cdata);
+    }
     case TypeKind::ListType: {
       const auto& elem_type = type->expect<ListType>()->getElementType();
       switch (elem_type->kind()) {
Index: pytorch-1.7/torch/csrc/jit/python/python_ir.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/python/python_ir.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/jit/python/python_ir.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -755,6 +755,9 @@
   py::class_<DeviceObjType, Type, std::shared_ptr<DeviceObjType>>(
       m, "DeviceObjType")
       .def_static("get", &DeviceObjType::get);
+  py::class_<StreamObjType, Type, std::shared_ptr<StreamObjType>>(
+      m, "StreamObjType")
+      .def_static("get", &StreamObjType::get);
   py::class_<PyObjectType, Type, std::shared_ptr<PyObjectType>>(
       m, "PyObjectType")
       .def_static("get", &PyObjectType::get);
Index: pytorch-1.7/torch/csrc/utils/python_arg_parser.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/utils/python_arg_parser.cpp	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/utils/python_arg_parser.cpp	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -35,6 +35,7 @@
   {"MemoryFormat", ParameterType::MEMORY_FORMAT},
   {"QScheme", ParameterType::QSCHEME},
   {"Device", ParameterType::DEVICE},
+  {"Stream", ParameterType::STREAM},
   {"std::string", ParameterType::STRING},
   {"Dimname", ParameterType::DIMNAME},
   {"DimnameList", ParameterType::DIMNAME_LIST},
@@ -431,6 +432,8 @@
     case ParameterType::QSCHEME: return THPQScheme_Check(obj);
     case ParameterType::DEVICE:
       return THPUtils_checkLong(obj) || THPUtils_checkString(obj) || THPDevice_Check(obj);
+    case ParameterType::STREAM:
+      return THPStream_Check(obj);
     case ParameterType::STRING: return THPUtils_checkString(obj);
     default: throw std::runtime_error("unknown parameter type");
   }
@@ -557,6 +560,10 @@
     if (str != "None") {
       throw std::runtime_error("invalid device: " + str);
     }
+  } else if (type_ == ParameterType::STREAM) {
+    if (str != "None") {
+      throw std::runtime_error("invalid stream: " + str);
+    }
   } else if (type_ == ParameterType::STRING) {
     if (str != "None" && str != "") {
       throw std::runtime_error("invalid default string: " + str);
Index: pytorch-1.7/torch/csrc/utils/python_arg_parser.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/utils/python_arg_parser.h	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/csrc/utils/python_arg_parser.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -42,6 +42,7 @@
 
 #include <torch/csrc/python_headers.h>
 
+#include <torch/csrc/Stream.h>
 #include <torch/csrc/Device.h>
 #include <torch/csrc/Dtype.h>
 #include <torch/csrc/DynamicTypes.h>
@@ -78,7 +79,7 @@
 
 enum class ParameterType {
   TENSOR, SCALAR, INT64, DOUBLE, COMPLEX, TENSOR_LIST, INT_LIST, GENERATOR,
-  BOOL, STORAGE, PYOBJECT, SCALARTYPE, LAYOUT, MEMORY_FORMAT, DEVICE, STRING,
+  BOOL, STORAGE, PYOBJECT, SCALARTYPE, LAYOUT, MEMORY_FORMAT, DEVICE, STREAM, STRING,
   DIMNAME, DIMNAME_LIST, QSCHEME, FLOAT_LIST
 };
 
@@ -165,6 +166,7 @@
   inline std::vector<int64_t> intlistWithDefault(int i, std::vector<int64_t> default_intlist);
   inline c10::optional<at::Generator> generator(int i);
   inline at::Storage storage(int i);
+  inline c10::Stream stream(int i);
   inline at::ScalarType scalartype(int i);
   inline at::ScalarType scalartypeWithDefault(int i, at::ScalarType default_scalartype);
   inline c10::optional<at::ScalarType> scalartypeOptional(int i);
@@ -620,6 +622,14 @@
   return createStorage(args[i]);
 }
 
+inline c10::Stream PythonArgs::stream(int i) {
+  if (!args[i]) return c10::Stream(c10::Stream::Default::DEFAULT, c10::Device(DeviceType::CPU, -1));
+  if (!THPStream_Check(args[i])) {
+    throw TypeError("expected Stream object. Got '%s'", Py_TYPE(args[i])->tp_name);
+  }
+  return c10::Stream::unpack(((THPStream*)args[i])->cdata);
+}
+
 inline PyObject* PythonArgs::pyobject(int i) {
   if (!args[i]) return Py_None;
   return args[i];
Index: pytorch-1.7/torch/jit/annotations.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/jit/annotations.py	(revision fc02d38d37fd83de558ab5c844a3b35cf7ca302f)
+++ pytorch-1.7/torch/jit/annotations.py	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
@@ -10,7 +10,7 @@
 
 from torch._C import TensorType, TupleType, FloatType, IntType, \
     ListType, StringType, DictType, BoolType, OptionalType, ClassType, InterfaceType, AnyType, NoneType, \
-    DeviceObjType, FutureType, EnumType
+    DeviceObjType, StreamObjType, FutureType, EnumType
 
 
 from textwrap import dedent
@@ -314,6 +314,8 @@
         return InterfaceType(_qualified_name(ann))
     if ann is torch.device:
         return DeviceObjType.get()
+    if ann is torch.Stream:
+        return StreamObjType.get()
     if ann is torch.dtype:
         return IntType.get()  # dtype not yet bound in as its own type
     if inspect.isclass(ann) and issubclass(ann, enum.Enum):
Index: pytorch-1.7/c10/macros/Macros.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/macros/Macros.h	(revision 68b1d2e65be9e633276df089db9a8e298ec98d6b)
+++ pytorch-1.7/c10/macros/Macros.h	(revision 32ee11733cc0a908ac93e2e10cb9a8424ca5f224)
@@ -270,6 +270,10 @@
 #else // __APPLE__, _MSC_VER
 #if defined(NDEBUG)
 extern "C" {
+#ifdef __SYCL_DEVICE_ONLY__
+extern SYCL_EXTERNAL void __assert_fail(const char *expr, const char *file,
+                                        unsigned int line, const char *func);
+#else
 #if (defined(__CUDA_ARCH__) && !(defined(__clang__) && defined(__CUDA__))) || \
     defined(__HIP_ARCH__) || defined(__HIP__)
 __host__ __device__
@@ -280,6 +284,7 @@
         const char* file,
         unsigned int line,
         const char* function) throw();
+#endif
 }
 #endif // NDEBUG
 #define CUDA_KERNEL_ASSERT(cond)                                         \
Index: pytorch-1.7/torch/csrc/distributed/c10d/reducer.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/distributed/c10d/reducer.cpp	(revision 32ee11733cc0a908ac93e2e10cb9a8424ca5f224)
+++ pytorch-1.7/torch/csrc/distributed/c10d/reducer.cpp	(revision 216fa81ad369be1bb2b874b0788b707ff12a9423)
@@ -357,7 +357,7 @@
     auto wrapped = c10::scalar_to_tensor(double(1.) / divFactor_);
     wrapped.unsafeGetTensorImpl()->set_wrapped_number(true);
     // Divides while copying into the bucket view.
-    at::native::mul_out(bucket_view, grad, wrapped);
+    at::mul_out(bucket_view, grad, wrapped);
   } else {
     bucket_view.copy_(grad);
   }
Index: pytorch-1.7/torch/nn/parallel/distributed.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/nn/parallel/distributed.py	(revision 32ee11733cc0a908ac93e2e10cb9a8424ca5f224)
+++ pytorch-1.7/torch/nn/parallel/distributed.py	(revision 216fa81ad369be1bb2b874b0788b707ff12a9423)
@@ -613,13 +613,10 @@
             # Notify joined ranks whether they should sync in backwards pass or not.
             self._check_global_requires_backward_grad_sync(is_joined_rank=False)
 
-        if self.device_ids:
+        if self.device_ids and len(self.device_ids) != 1:
             inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
-            if len(self.device_ids) == 1:
-                output = self.module(*inputs[0], **kwargs[0])
-            else:
-                outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
-                output = self.gather(outputs, self.output_device)
+            outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
+            output = self.gather(outputs, self.output_device)
         else:
             output = self.module(*inputs, **kwargs)
 
Index: pytorch-1.7/c10/core/Device.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/c10/core/Device.h	(revision 216fa81ad369be1bb2b874b0788b707ff12a9423)
+++ pytorch-1.7/c10/core/Device.h	(revision d4e2fbb0159835408763747f7a325be156a9bbad)
@@ -86,6 +86,11 @@
     return type_ == DeviceType::CPU;
   }
 
+  /// Return true if the device is of XPU type.
+  bool is_xpu() const noexcept {
+    return type_ == DeviceType::XPU;
+  }
+
   /// Same string as returned from operator<<.
   std::string str() const;
 
Index: pytorch-1.7/torch/csrc/jit/codegen/fuser/executor.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/codegen/fuser/executor.cpp	(revision 216fa81ad369be1bb2b874b0788b707ff12a9423)
+++ pytorch-1.7/torch/csrc/jit/codegen/fuser/executor.cpp	(revision d4e2fbb0159835408763747f7a325be156a9bbad)
@@ -362,6 +362,8 @@
     return false;
   if (device.is_cpu() && !canFuseOnCPU())
     return false;
+  if (device.is_xpu())
+    return false;
 
   // Validates sizes and expands inputs as needed
   auto maybe_map_size = canRunKernel(spec, inputs);
Index: pytorch-1.7/CMakeLists.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/CMakeLists.txt	(revision d4e2fbb0159835408763747f7a325be156a9bbad)
+++ pytorch-1.7/CMakeLists.txt	(revision ea9f7cde2db5eb2df6abc0f51f8ae5a564a162c5)
@@ -867,3 +867,5 @@
 
 include(cmake/Summary.cmake)
 caffe2_print_configuration_summary()
+
+add_custom_target(helloworld COMMAND echo dummy stub)
Index: pytorch-1.7/torch/csrc/jit/passes/tensorexpr_fuser.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/passes/tensorexpr_fuser.cpp	(revision ea9f7cde2db5eb2df6abc0f51f8ae5a564a162c5)
+++ pytorch-1.7/torch/csrc/jit/passes/tensorexpr_fuser.cpp	(revision 7f532baaab22ba6721e1a2f1462102e03932e331)
@@ -696,6 +696,8 @@
       return canFuseOnCPU();
     } else if (device->is_cuda()) {
       return canFuseOnGPU();
+    } else if (device->is_xpu()) {
+      return false;
     }
     throw std::runtime_error("Unknown device");
   }
Index: pytorch-1.7/torch/csrc/jit/runtime/argument_spec.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/runtime/argument_spec.h	(revision 7f532baaab22ba6721e1a2f1462102e03932e331)
+++ pytorch-1.7/torch/csrc/jit/runtime/argument_spec.h	(revision 4c6c8d1426a64d46d06512647d8289fa2b68e854)
@@ -15,18 +15,15 @@
 // GraphExecutor creates specializations of Graphs for different
 // dimensionalitities and types of inputs.
 
-inline static at::Device ConvertIntToCPUOrCUDA(int device) {
-  return device < 0 ? at::kCPU : at::Device(DeviceType::CUDA, device);
-}
 struct ArgumentInfo {
   friend struct ArgumentSpec;
-  using plain_data_type = uint32_t;
+  using plain_data_type = uint64_t;
 
   bool defined() const {
     return defined_;
   }
-  int device() const {
-    return device_;
+  at::Device device() const {
+    return at::Device(DeviceType(dev_type_), device_);
   }
   // XXX: It is guaranteed that this will return false when called on non-tensor
   // arguments
@@ -45,7 +42,7 @@
 
     return TensorType::create(
         type(),
-        ConvertIntToCPUOrCUDA(device()),
+        device(),
         c10::optional<size_t>(dim()),
         requires_grad());
   }
@@ -61,6 +58,8 @@
   int device_ : 8; // NOTE: this needs to be signed because we use -1 to
                    // represent CPU
   unsigned type_ : 8;
+  unsigned dev_type_ : 16;
+  unsigned : 16;
 };
 
 static_assert(
@@ -101,7 +100,9 @@
     if ((arg.defined_ = t->defined())) {
       arg.requires_grad_ = with_grad && autograd::Variable(*t).requires_grad();
       arg.dim_ = t->dim();
-      arg.device_ = t->is_cuda() ? t->get_device() : -1;
+      at::Device device = t->device();
+      arg.dev_type_ = static_cast<std::underlying_type<DeviceType>::type>(device.type());
+      arg.device_ = device.index();
       arg.type_ = static_cast<unsigned>(t->scalar_type());
     }
     combineHash(arg);
@@ -214,7 +215,8 @@
   unsigned defined : 1;
   unsigned requires_grad : 1;
   signed device : 14;
-  uint32_t total_dims; // all TensorInfoPODs are in CompleteArgumentSpec's
+  unsigned dev_type : 16;
+  uint16_t total_dims; // all TensorInfoPODs are in CompleteArgumentSpec's
                        // tensor_info() array. total_dims is the total number of
                        // dimensions seen so far in all previous members of
                        // tensor_info(), including this tensor 2*total_dims
@@ -256,6 +258,9 @@
         if (pod.defined) {
           pod.type = static_cast<int>(t.scalar_type());
           pod.device = (!t.is_cuda()) ? -1 : t.get_device();
+          at::Device device = t.device();
+          pod.dev_type = static_cast<std::underlying_type<DeviceType>::type>(device.type());
+          pod.device = device.index();
           pod.requires_grad = with_grad && t.requires_grad();
           total_dims += t.ndimension();
           auto sizes = t.sizes();
@@ -332,8 +337,8 @@
   bool requires_grad() const {
     return pod(i).requires_grad;
   }
-  int device() const {
-    return pod(i).device;
+  at::Device device() const {
+    return at::Device(DeviceType(pod(i).dev_type), pod(i).device);
   }
   int ndimension() const {
     // See [valid range], it is always valid to ask for offset for (i + 1)
@@ -353,7 +358,7 @@
       return TensorType::get();
     return TensorType::create(
         type(),
-        ConvertIntToCPUOrCUDA(device()),
+        device(),
         c10::VaryingShape<int64_t>{sizes()},
         c10::VaryingShape<int64_t>{strides()},
         requires_grad());
Index: pytorch-1.7/test/cpp/jit/test_argument_spec.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/test/cpp/jit/test_argument_spec.cpp	(revision 4c6c8d1426a64d46d06512647d8289fa2b68e854)
+++ pytorch-1.7/test/cpp/jit/test_argument_spec.cpp	(revision 47c78155320cfd1504a78f951dff1f9470375732)
@@ -9,8 +9,8 @@
 
 namespace {
 
-int device(const autograd::Variable& v) {
-  return v.device().is_cuda() ? v.get_device() : -1;
+at::Device device(const autograd::Variable& v) {
+  return v.device();
 }
 
 bool isEqual(at::IntArrayRef lhs, at::IntArrayRef rhs) {
Index: pytorch-1.7/torch/csrc/jit/passes/decompose_ops.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- pytorch-1.7/torch/csrc/jit/passes/decompose_ops.cpp	(revision 4c6c8d1426a64d46d06512647d8289fa2b68e854)
+++ pytorch-1.7/torch/csrc/jit/passes/decompose_ops.cpp	(revision 47c78155320cfd1504a78f951dff1f9470375732)
@@ -41,7 +41,7 @@
   auto device = input->type()->expect<TensorType>()->device();
   // As of now, we do the decomposition for batchnorm/layernorm on GPU device
   // only
-  if (!device || (*device).is_cpu()) {
+  if (!device || (*device).is_cpu() || (*device).is_xpu()) {
     return false;
   }
 
diff --git pytorch-1.7/aten/src/ATen/function_wrapper.py pytorch-1.7/aten/src/ATen/function_wrapper.py
new file mode 100644
