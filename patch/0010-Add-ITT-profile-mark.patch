From f41dd97d83282c5f8ea35ec5b75a6cab97fdc5a9 Mon Sep 17 00:00:00 2001
From: johnlu <chengjun.lu@intel.com>
Date: Thu, 28 Jan 2021 00:09:25 +0800
Subject: [PATCH 10/13] Add ITT profile mark

---
 torch/_C/_autograd.pyi           |  2 ++
 torch/csrc/autograd/init.cpp     |  3 ++-
 torch/csrc/autograd/profiler.cpp | 29 +++++++++++++++++------------
 torch/csrc/autograd/profiler.h   | 22 ++++++++++++++--------
 4 files changed, 35 insertions(+), 21 deletions(-)

diff --git a/torch/_C/_autograd.pyi b/torch/_C/_autograd.pyi
index a154fb1948..7d7d663ab9 100644
--- a/torch/_C/_autograd.pyi
+++ b/torch/_C/_autograd.pyi
@@ -8,6 +8,8 @@ class ProfilerState(Enum):
     CPU = 1
     CUDA = 2
     NVTX = 3
+    XPU = 4
+    ITT = 5
 
 
 class ProfilerConfig:
diff --git a/torch/csrc/autograd/init.cpp b/torch/csrc/autograd/init.cpp
index 5f498b37e5..905d349d0f 100644
--- a/torch/csrc/autograd/init.cpp
+++ b/torch/csrc/autograd/init.cpp
@@ -39,8 +39,9 @@ PyObject* THPAutograd_initExtension(PyObject* _unused, PyObject *unused) {
       .value("Disabled", ProfilerState::Disabled)
       .value("CPU", ProfilerState::CPU)
       .value("CUDA", ProfilerState::CUDA)
+      .value("NVTX", ProfilerState::NVTX)
       .value("XPU", ProfilerState::XPU)
-      .value("NVTX", ProfilerState::NVTX);
+      .value("ITT", ProfilerState::ITT);
 
   py::class_<ProfilerConfig>(m, "ProfilerConfig")
       .def(py::init<ProfilerState, bool, bool, bool>());
diff --git a/torch/csrc/autograd/profiler.cpp b/torch/csrc/autograd/profiler.cpp
index 128b8e65da..894040d275 100644
--- a/torch/csrc/autograd/profiler.cpp
+++ b/torch/csrc/autograd/profiler.cpp
@@ -71,7 +71,6 @@ XPUStubs default_xpu_stubs;
 constexpr CUDAStubs* default_stubs_addr = &default_stubs;
 constexpr XPUStubs* default_xpu_stubs_addr = &default_xpu_stubs;
 // constant initialization, so it is guaranteed to be initialized before
-// Constant initialization, so it is guaranteed to be initialized before
 // static initialization calls which may invoke registerCUDAMethods
 static CUDAStubs* cuda_stubs = default_stubs_addr;
 static XPUStubs* xpu_stubs = default_xpu_stubs_addr;
@@ -206,7 +205,10 @@ struct ProfilerThreadLocalState : public c10::MemoryReportingInfoBase {
     }
     if (config_.state == ProfilerState::NVTX) {
       cuda_stubs->nvtxMarkA(name.c_str());
-    } else {
+    } else if (config_.state == ProfilerState::ITT) {
+      xpu_stubs->ittMark(name.c_str());
+    }
+    else {
       Event evt(
           EventKind::Mark,
           at::StringView(std::move(name)),
@@ -218,15 +220,14 @@ struct ProfilerThreadLocalState : public c10::MemoryReportingInfoBase {
   }
 
   void mark_xpu(std::string name, XPUEventStub& xpu_event) {
-    if (config_.state == ProfilerState::Disabled) {
-      return;
+    if (config_.state == ProfilerState::XPU) {
+      getEventList().record(
+              EventKind::Mark,
+              at::StringView(std::move(name)),
+              at::RecordFunction::currentThreadId(),
+              config_.state,
+              xpu_event);
     }
-    getEventList().record(
-      EventKind::Mark,
-      at::StringView(std::move(name)),
-      at::RecordFunction::currentThreadId(),
-      config_.state,
-      xpu_event);
   }
 
   void setOrAddRemoteProfiledEvents(
@@ -251,6 +252,8 @@ struct ProfilerThreadLocalState : public c10::MemoryReportingInfoBase {
     if (config_.state == ProfilerState::NVTX) {
       cuda_stubs->nvtxRangePushA(getNvtxStr(
           fn.name(), msg, fn.seqNr(), shapes).c_str());
+    } else if (config_.state == ProfilerState::ITT) {
+      xpu_stubs->ittRangePush(fn.name().str());
     } else {
       Event evt(
           EventKind::PushRange,
@@ -284,6 +287,8 @@ struct ProfilerThreadLocalState : public c10::MemoryReportingInfoBase {
     }
     if (config_.state == ProfilerState::NVTX) {
       cuda_stubs->nvtxRangePop();
+    } else if (config_.state == ProfilerState::ITT) {
+      xpu_stubs->ittRangePop();
     } else {
       // In some cases RecordFunction (and popRange) may be
       // called on a different thread than pushRange
@@ -542,6 +547,8 @@ bool profilerEnabled() {
 void enableProfiler(const ProfilerConfig& new_config) {
   TORCH_CHECK(new_config.state != ProfilerState::NVTX || cuda_stubs->enabled(),
     "Can't use NVTX profiler - PyTorch was compiled without CUDA");
+  TORCH_CHECK(new_config.state != ProfilerState::ITT || xpu_stubs->enabled(),
+    "Can't use Intel(R) VTune Profiler's ITT functionality - PyTorch was compiled without ITT");
 
   auto state_ptr = getProfilerTLSState();
   TORCH_CHECK(!state_ptr, "Profiler is already enabled on this thread");
@@ -732,8 +739,6 @@ double Event::xpu_elapsed_us() {
 
 CUDAStubs::~CUDAStubs() = default;
 
-XPUStubs::~XPUStubs() = default;
-
 static jit::CodeTemplate event_template(R"(
 {
   "name": "${name}",
diff --git a/torch/csrc/autograd/profiler.h b/torch/csrc/autograd/profiler.h
index b44dd16f86..8b0c50e74e 100644
--- a/torch/csrc/autograd/profiler.h
+++ b/torch/csrc/autograd/profiler.h
@@ -81,7 +81,16 @@ struct TORCH_API XPUStubs {
   virtual bool enabled() {
     return false;
   }
-  virtual ~XPUStubs();
+  virtual void ittMark(const char* name) {
+    fail();
+  }
+  virtual void ittRangePush(const char* name) {
+    fail();
+  }
+  virtual void ittRangePop() {
+    fail();
+  }
+  virtual ~XPUStubs() {};
 
 private:
   void fail() {
@@ -133,8 +142,9 @@ enum class C10_API_ENUM ProfilerState {
     Disabled,
     CPU, // CPU-only profiling
     CUDA, // CPU + CUDA events
-    XPU, // CPU + XPU events
     NVTX,  // only emit NVTX markers
+    XPU, // CPU + XPU events
+    ITT, // only emit ITT markers
 };
 
 struct TORCH_API ProfilerConfig {
@@ -182,7 +192,6 @@ struct TORCH_API Event final {
         kind_(kind),
         thread_id_(thread_id),
         handle_(handle),
-        xpu_kernel(false),
         shapes_(shapes),
         node_id_(node_id) {
     record(state);
@@ -198,7 +207,6 @@ struct TORCH_API Event final {
     : name_(std::move(name)),
       kind_(kind),
       thread_id_(thread_id),
-      xpu_kernel(true),
       xpu_event(event) {
     record(state);
   }
@@ -223,7 +231,6 @@ struct TORCH_API Event final {
         kind_(kind),
         thread_id_(thread_id),
         handle_(handle),
-        xpu_kernel(false),
         shapes_(shapes),
         cpu_memory_usage_(cpu_memory_usage),
         cuda_memory_usage_(cuda_memory_usage),
@@ -291,7 +298,7 @@ struct TORCH_API Event final {
   }
 
   bool has_xpu() const {
-    return xpu_kernel;
+    return (xpu_event != nullptr);
   }
 
   int device() const {
@@ -391,7 +398,6 @@ struct TORCH_API Event final {
   uint64_t thread_id_;
   uint64_t fwd_thread_id_;
   at::RecordFunctionHandle handle_ {0};
-  bool xpu_kernel;
   std::vector<std::vector<int64_t>> shapes_;
   int64_t cpu_memory_usage_ = 0;
   int64_t cuda_memory_usage_ = 0;
@@ -405,7 +411,7 @@ struct TORCH_API Event final {
 
   std::vector<std::string> stack_;
   uint8_t scope_;
-  XPUEventStub xpu_event;
+  XPUEventStub xpu_event = nullptr;
 };
 
 // a linked-list of fixed sized vectors, to avoid
-- 
2.25.1

