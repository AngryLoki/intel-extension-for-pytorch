<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Releases &mdash; intel_extension_for_pytorch 1.11.0+cpu documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Installation Guide" href="installation.html" />
    <link rel="prev" title="Intel® Extension for PyTorch* optimizations for quantization (Experimental)" href="features/int8.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../versions.html">1.11.0+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Releases</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">1.11.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#highlights">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#what-s-changed">What’s Changed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">1.10.100</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">1.10.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#known-issues">Known Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">What’s Changed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id6">1.9.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what-s-new">What’s New</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id7">1.8.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">What’s New</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id9">1.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id10">What’s New</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-improvement">Performance Improvement</a></li>
<li class="toctree-l3"><a class="reference internal" href="#others">Others</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">Known issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id12">1.1.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">What’s New</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#known-issue">Known issue</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id14">1.0.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alpha">1.0.1-Alpha</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id15">1.0.0-Alpha</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id16">What’s New</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-result">Performance Result</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">Known issue</a></li>
<li class="toctree-l3"><a class="reference internal" href="#note">NOTE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_doc.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Releases</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/releases.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="releases">
<h1>Releases<a class="headerlink" href="#releases" title="Permalink to this headline"></a></h1>
<section id="id1">
<h2>1.11.0<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>We are excited to announce Intel® Extension for PyTorch* 1.11.0-cpu release by tightly following PyTorch 1.11 release. Along with extension 1.11, we focused on continually improving OOB user experience and performance. Highlights include:</p>
<ul class="simple">
<li><p>Support a single binary with runtime dynamic dispatch based on AVX2/AVX512 hardware ISA detection</p></li>
<li><p>Support install binary from <code class="docutils literal notranslate"><span class="pre">pip</span></code> with package name only (without the need of specifying the URL)</p></li>
<li><p>Provide the C++ SDK installation to facilitate ease of C++ app development and deployment</p></li>
<li><p>Add more optimizations, including graph fusions for speeding up Transformer-based models and CNN, etc</p></li>
<li><p>Reduce the binary size for both the PIP wheel and C++ SDK (2X to 5X reduction from the previous version)</p></li>
</ul>
<section id="highlights">
<h3>Highlights<a class="headerlink" href="#highlights" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Combine the AVX2 and AVX512 binary as a single binary and automatically dispatch to different implementations based on hardware ISA detection at runtime. The typical case is to serve the data center that mixtures AVX2-only and AVX512 platforms. It does not need to deploy the different ISA binary now compared to the previous version</p>
<p><em><strong>NOTE</strong></em>:  The extension uses the oneDNN library as the backend. However, the BF16 and INT8 operator sets and features are different between AVX2 and AVX512. Please refer to <a class="reference external" href="https://oneapi-src.github.io/oneDNN/dev_guide_int8_computations.html#processors-with-the-intel-avx2-or-intel-avx-512-support">oneDNN document</a> for more details.</p>
<blockquote>
<div><p>When one input is of type u8, and the other one is of type s8, oneDNN assumes that it is the user’s responsibility to choose the quantization parameters so that no overflow/saturation occurs. For instance, a user can use u7 [0, 127] instead of u8 for the unsigned input, or s7 [-64, 63] instead of the s8 one. It is worth mentioning that this is required only when the Intel AVX2 or Intel AVX512 Instruction Set is used.</p>
</div></blockquote>
</li>
<li><p>The extension wheel packages have been uploaded to <a class="reference external" href="https://pypi.org/project/intel-extension-for-pytorch/">pypi.org</a>. The user could directly install the extension by <code class="docutils literal notranslate"><span class="pre">pip/pip3</span></code> without explicitly specifying the binary location URL.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.10.100-cpu</td>
<td>v1.11.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">intel_extension_for_pytorch</span><span class="o">==</span><span class="mf">1.10.100</span> <span class="o">-</span><span class="n">f</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">software</span><span class="o">.</span><span class="n">intel</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ipex</span><span class="o">-</span><span class="n">whl</span><span class="o">-</span><span class="n">stable</span>
</pre></div>
</div>
</td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">intel_extension_for_pytorch</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Compared to the previous version, this release provides a dedicated installation file for the C++ SDK. The installation file automatically detects the PyTorch C++ SDK location and installs the extension C++ SDK files to the PyTorch C++ SDK. The user does not need to manually add the extension C++ SDK source files and CMake to the PyTorch SDK. In addition to that, the installation file reduces the C++ SDK binary size from ~220MB to ~13.5MB.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.10.100-cpu</td>
<td>v1.11.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">intel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="n">cpu</span><span class="o">-</span><span class="n">libtorch</span><span class="o">-</span><span class="n">shared</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">deps</span><span class="o">-</span><span class="mf">1.10.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">zip</span> <span class="p">(</span><span class="mi">220</span><span class="n">M</span><span class="p">)</span>
<span class="n">intel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="n">cpu</span><span class="o">-</span><span class="n">libtorch</span><span class="o">-</span><span class="n">cxx11</span><span class="o">-</span><span class="n">abi</span><span class="o">-</span><span class="n">shared</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">deps</span><span class="o">-</span><span class="mf">1.10.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">zip</span> <span class="p">(</span><span class="mi">224</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">libintel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="mf">1.11.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">run</span> <span class="p">(</span><span class="mf">13.7</span><span class="n">M</span><span class="p">)</span>
<span class="n">libintel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="n">cxx11</span><span class="o">-</span><span class="n">abi</span><span class="o">-</span><span class="mf">1.11.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">run</span> <span class="p">(</span><span class="mf">13.5</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Add more optimizations, including more custom operators and fusions.</p>
<ul>
<li><p>Fuse the QKV linear operators as a single Linear to accelerate the Transformer*(BERT-*) encoder part  - <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/0f27c269cae0f902973412dc39c9a7aae940e07b">#278</a>.</p></li>
<li><p>Remove Multi-Head-Attention fusion limitations to support the 64bytes unaligned tensor shape. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/dbb10fedb00c6ead0f5b48252146ae9d005a0fad">#531</a></p></li>
<li><p>Fold the binary operator to Convolution and Linear operator to reduce computation. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/564588561fa5d45b8b63e490336d151ff1fc9cbc">#432</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/b4e7dacf08acd849cecf8d143a11dc4581a3857f">#438</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/74aa21262938b923d3ed1e6929e7d2b629b3ff27">#602</a></p></li>
<li><p>Replace the outplace operators with their corresponding in-place version to reduce memory footprint. The extension currently supports the operators including <code class="docutils literal notranslate"><span class="pre">sliu</span></code>, <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, <code class="docutils literal notranslate"><span class="pre">hardsigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">hardswish</span></code>, <code class="docutils literal notranslate"><span class="pre">relu6</span></code>, <code class="docutils literal notranslate"><span class="pre">relu</span></code>, <code class="docutils literal notranslate"><span class="pre">selu</span></code>, <code class="docutils literal notranslate"><span class="pre">softmax</span></code>. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/38647677e8186a235769ea519f4db65925eca33c">#524</a></p></li>
<li><p>Fuse the Concat + BN + ReLU as a single operator. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/275ff503aea780a6b741f04db5323d9529ee1081">#452</a></p></li>
<li><p>Optimize Conv3D for both imperative and JIT by enabling NHWC and pre-packing the weight. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ae33faf62bb63b204b0ee63acb8e29e24f6076f3">#425</a></p></li>
</ul>
</li>
<li><p>Reduce the binary size. C++ SDK is reduced from ~220MB to ~13.5MB while the wheel packaged is reduced from ~100MB to ~40MB.</p></li>
<li><p>Update oneDNN and oneDNN graph to <a class="reference external" href="https://github.com/oneapi-src/oneDNN/releases/tag/v2.5.2">2.5.2</a> and <a class="reference external" href="https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.4.2">0.4.2</a> respectively.</p></li>
</ul>
</section>
<section id="what-s-changed">
<h3>What’s Changed<a class="headerlink" href="#what-s-changed" title="Permalink to this headline"></a></h3>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v1.10.100…v1.11.0</p>
</section>
</section>
<section id="id2">
<h2>1.10.100<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>This release is meant to fix the following issues:</p>
<ul class="simple">
<li><p>Resolve the issue that the PyTorch Tensor Expression(TE) did not work after importing the extension.</p></li>
<li><p>Wraps the BactchNorm(BN) as another operator to break the TE’s BN-related fusions. Because the BatchNorm performance of PyTorch Tensor Expression can not achieve the same performance as PyTorch ATen BN.</p></li>
<li><p>Update the <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/">documentation</a></p>
<ul>
<li><p>Fix the INT8 quantization example issue #205</p></li>
<li><p>Polish the installation guide</p></li>
</ul>
</li>
</ul>
</section>
<section id="id3">
<h2>1.10.0<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>The Intel® Extension for PyTorch* 1.10 is on top of PyTorch 1.10. In this release, we polished the front end APIs. The APIs are more simplible, stable and straightforward now. According to PyTorch community recommendation, we changed the underhood device from <code class="docutils literal notranslate"><span class="pre">XPU</span></code> to <code class="docutils literal notranslate"><span class="pre">CPU</span></code>. With this change, the model and tensor does not need to be converted to the extension device to get performance improvement. It simplifies the model changes.</p>
<p>Besides that, we continuously optimize the Transformer* and CNN models by fusing more operators and applying NHWC. We measured the 1.10 performance on Torchvison and HugginFace. As expected, 1.10 can speed up the two model zones.</p>
<section id="id4">
<h3>Highlights<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Change the package name to <code class="docutils literal notranslate"><span class="pre">intel_extension_for_pytorch</span></code> while the original package name is <code class="docutils literal notranslate"><span class="pre">intel_pytorch_extension</span></code>. This change targets to avoid any potential legal issues.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>The underhood device is changed from the extension-specific device(<code class="docutils literal notranslate"><span class="pre">XPU</span></code>) to the standard CPU device which aligns with PyTorch CPU device design regardless of the dispatch mechanism and operator register mechanism. The interface impactions are that the model does not need to be converted to the extension device explicitly.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>

<span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Explicitly convert the model to the extension device</span>
<span class="n">resnet18_xpu</span> <span class="o">=</span> <span class="n">resnet18</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>

<span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Compared to v1.9.0, v1.10.0 follows PyTorch AMP API(<code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code>) to support auto-mixed-precision. <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> provides convenience for auto data type conversion at runtime. Currently, <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> only supports <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>. It is the default lower precision floating point data type when <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> is enabled. <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> primarily benefits on Intel CPU with BFloat16 instruction set support.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="c1"># Automatically mix precision</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">enable_auto_mixed_precision</span><span class="p">(</span><span class="n">mixed_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>The 1.10 release provides the INT8 calibration as an experimental feature while it only supports post-training static quantization now. Compared to 1.9.0, the fronted APIs for qutization is more straightforward and ease-of-use.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># user dataset for calibration.</span>
<span class="n">xx_c</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># user dataset for validation.</span>
<span class="n">xx_v</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Clibration</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="c1"># Convert the model to the Extension device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="c1"># Create a configuration file to save quantization parameters.</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AmpConf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_c</span><span class="p">:</span>
        <span class="c1"># Run the model under calibration mode to collect quantization parameters</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AutoMixPrecision</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">running_mode</span><span class="o">=</span><span class="s1">&#39;calibration&#39;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">))</span>
<span class="c1"># Save the configuration file</span>
<span class="n">conf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_c</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">conf</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">conf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Inference</p></li>
</ul>
 <table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="c1"># Convert the model to the Extension device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AmpConf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cali_dataset</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AutoMixPrecision</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">running_mode</span><span class="o">=</span><span class="s1">&#39;inference&#39;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">))</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">trace_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_v</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">trace_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>This release introduces the <code class="docutils literal notranslate"><span class="pre">optimize</span></code> API at python front end to optimize the model and optimizer for training. The new API both supports FP32 and BF16, inference and training.</p></li>
<li><p>Runtime Extension (Experimental) provides a runtime CPU pool API to bind threads to cores. It also features async tasks. Please <strong>note</strong>: Intel® Extension for PyTorch* Runtime extension is still in the <strong>POC</strong> stage. The API is subject to change. More detailed descriptions are available in the extension documentation.</p></li>
</ul>
</section>
<section id="known-issues">
<h3>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this headline"></a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">omp_set_num_threads</span></code> function failed to change OpenMP threads number of oneDNN operators if it was set before.</p>
<p><code class="docutils literal notranslate"><span class="pre">omp_set_num_threads</span></code> function is provided in Intel® Extension for PyTorch* to change the number of threads used with OpenMP. However, it failed to change the number of OpenMP threads if it was set before.</p>
<p>pseudo-code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">omp_set_num_threads</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">model_execution</span><span class="p">()</span>
<span class="n">omp_set_num_threads</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">same_model_execution_again</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Reason:</strong> oneDNN primitive descriptor stores the omp number of threads. Current oneDNN integration caches the primitive descriptor in IPEX. So if we use runtime extension with oneDNN based pytorch/ipex operation, the runtime extension fails to change the used omp number of threads.</p>
</li>
<li><p>Low performance with INT8 support for dynamic shapes</p>
<p>The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still working in progress. For the use cases where the input shapes are dynamic, for example, inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* INT8 path may slow down the model inference. In this case, please utilize stock PyTorch INT8 functionality.</p>
</li>
<li><p>Low throughput with DLRM FP32 Train</p>
<p>A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending review. The issue will be fixed when the PR is merged.</p>
</li>
</ul>
</section>
<section id="id5">
<h3>What’s Changed<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v1.9.0…v1.10.0+cpu-rc3</p>
</section>
</section>
<section id="id6">
<h2>1.9.0<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<section id="what-s-new">
<h3>What’s New<a class="headerlink" href="#what-s-new" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Rebased the Intel Extension for Pytorch from PyTorch-1.8.0 to the official PyTorch-1.9.0 release.</p></li>
<li><p>Support binary installation.</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch_ipex==1.9.0</span> <span class="pre">-f</span> <span class="pre">https://software.intel.com/ipex-whl-stable</span></code></p>
</li>
<li><p>Support the C++ library. The third party App can link the Intel-Extension-for-PyTorch C++ library to enable the particular optimizations.</p></li>
</ul>
</section>
</section>
<section id="id7">
<h2>1.8.0<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<section id="id8">
<h3>What’s New<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Rebased the Intel Extension for Pytorch from Pytorch -1.7.0 to the official Pytorch-1.8.0 release. The new XPU device type has been added into Pytorch-1.8.0(49786), don’t need to patch PyTorch to enable Intel Extension for Pytorch anymore</p></li>
<li><p>Upgraded the oneDNN from v1.5-rc to v1.8.1</p></li>
<li><p>Updated the README file to add the sections to introduce supported customized operators, supported fusion patterns, tutorials and joint blogs with stakeholders</p></li>
</ul>
</section>
</section>
<section id="id9">
<h2>1.2.0<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<section id="id10">
<h3>What’s New<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<ul>
<li><p>We rebased the Intel Extension for pytorch from Pytorch -1.5rc3 to the official Pytorch-1.7.0 release. It will have performance improvement with the new Pytorch-1.7 support.</p></li>
<li><p>Device name was changed from DPCPP to XPU.</p>
<p>We changed the device name from DPCPP to XPU to align with the future Intel GPU product for heterogeneous computation.</p>
</li>
<li><p>Enabled the launcher for end users.</p></li>
<li><p>We enabled the launch script which helps users launch the program for training and inference, then automatically setup the strategy for multi-thread, multi-instance, and memory allocator. Please refer to the launch script comments for more details.</p></li>
</ul>
</section>
<section id="performance-improvement">
<h3>Performance Improvement<a class="headerlink" href="#performance-improvement" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>This upgrade provides better INT8 optimization with refined auto mixed-precision API.</p></li>
<li><p>More operators are optimized for the int8 inference and bfp16 training of some key workloads, like MaskRCNN, SSD-ResNet34, DLRM, RNNT.</p></li>
</ul>
</section>
<section id="others">
<h3>Others<a class="headerlink" href="#others" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Bug fixes</p>
<ul class="simple">
<li><p>This upgrade fixes the issue that saving the model trained by Intel extension for PyTorch caused errors.</p></li>
<li><p>This upgrade fixes the issue that Intel extension for PyTorch was slower than pytorch proper for Tacotron2.</p></li>
</ul>
</li>
<li><p>New custom operators</p>
<p>This upgrade adds several custom operators: ROIAlign, RNN, FrozenBatchNorm, nms.</p>
</li>
<li><p>Optimized operators/fusion</p>
<p>This upgrade optimizes several operators: tanh, log_softmax, upsample, embeddingbad and enables int8 linear fusion.</p>
</li>
<li><p>Performance</p>
<p>The release has daily automated testing for the supported models: ResNet50, ResNext101, Huggingface Bert, DLRM, Resnext3d, MaskRNN, SSD-ResNet34. With the extension imported, it can bring up to 2x INT8 over FP32 inference performance improvements on the 3rd Gen Intel Xeon scalable processors (formerly codename Cooper Lake).</p>
</li>
</ul>
</section>
<section id="id11">
<h3>Known issues<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Multi-node training still encounter hang issues after several iterations. The fix will be included in the next official release.</p></li>
</ul>
</section>
</section>
<section id="id12">
<h2>1.1.0<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h2>
<section id="id13">
<h3>What’s New<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added optimization for training with FP32 data type &amp; BF16 data type. All the optimized FP32/BF16 backward operators include:</p>
<ul>
<li><p>Conv2d</p></li>
<li><p>Relu</p></li>
<li><p>Gelu</p></li>
<li><p>Linear</p></li>
<li><p>Pooling</p></li>
<li><p>BatchNorm</p></li>
<li><p>LayerNorm</p></li>
<li><p>Cat</p></li>
<li><p>Softmax</p></li>
<li><p>Sigmoid</p></li>
<li><p>Split</p></li>
<li><p>Embedding_bag</p></li>
<li><p>Interaction</p></li>
<li><p>MLP</p></li>
</ul>
</li>
<li><p>More fusion patterns are supported and validated in the release, see table:</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>Fusion Patterns</th>
<th>Release</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conv + Sum</td>
<td>v1.0</td>
</tr>
<tr>
<td>Conv + BN</td>
<td>v1.0</td>
</tr>
<tr>
<td>Conv + Relu</td>
<td>v1.0</td>
</tr>
<tr>
<td>Linear + Relu</td>
<td>v1.0</td>
</tr>
<tr>
<td>Conv + Eltwise</td>
<td>v1.1</td>
</tr>
<tr>
<td>Linear + Gelu</td>
<td>v1.1</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Add docker support</p></li>
<li><p>[Alpha] Multi-node training with oneCCL support.</p></li>
<li><p>[Alpha] INT8 inference optimization.</p></li>
</ul>
</section>
<section id="performance">
<h3>Performance<a class="headerlink" href="#performance" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The release has daily automated testing for the supported models: ResNet50, ResNext101, <a class="reference external" href="https://github.com/huggingface/transformers">Huggingface Bert</a>, <a class="reference external" href="https://github.com/intel/optimized-models/tree/master/pytorch/dlrm">DLRM</a>, <a class="reference external" href="https://github.com/XiaobingSuper/Resnext3d-for-video-classification">Resnext3d</a>, <a class="reference external" href="https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py">Transformer</a>. With the extension imported, it can bring up to 1.2x~1.7x BF16 over FP32 training performance improvements on the 3rd Gen Intel Xeon scalable processors (formerly codename Cooper Lake).</p></li>
</ul>
</section>
<section id="known-issue">
<h3>Known issue<a class="headerlink" href="#known-issue" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Some workloads may crash after several iterations on the extension with <a class="reference external" href="https://github.com/jemalloc/jemalloc">jemalloc</a> enabled.</p></li>
</ul>
</section>
</section>
<section id="id14">
<h2>1.0.2<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Rebase torch CCL patch to PyTorch 1.5.0-rc3</p></li>
</ul>
</section>
<section id="alpha">
<h2>1.0.1-Alpha<a class="headerlink" href="#alpha" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Static link oneDNN library</p></li>
<li><p>Check AVX512 build option</p></li>
<li><p>Fix the issue that cannot normally invoke <code class="docutils literal notranslate"><span class="pre">enable_auto_optimization</span></code></p></li>
</ul>
</section>
<section id="id15">
<h2>1.0.0-Alpha<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h2>
<section id="id16">
<h3>What’s New<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Auto Operator Optimization</p>
<p>Intel Extension for PyTorch will automatically optimize the operators of PyTorch when importing its python package. It will significantly improve the computation performance if the input tensor and the model is converted to the extension device.</p>
</li>
<li><p>Auto Mixed Precision
Currently, the extension has supported bfloat16. It streamlines the work to enable a bfloat16 model. The feature is controlled by <code class="docutils literal notranslate"><span class="pre">enable_auto_mix_precision</span></code>. If you enable it, the extension will run the operator with bfloat16 automatically to accelerate the operator computation.</p></li>
</ul>
</section>
<section id="performance-result">
<h3>Performance Result<a class="headerlink" href="#performance-result" title="Permalink to this headline"></a></h3>
<p>We collected the performance data of some models on the Intel Cooper Lake platform with 1 socket and 28 cores. Intel Cooper Lake introduced AVX512 BF16 instructions which could improve the bfloat16 computation significantly. The detail is as follows (The data is the speedup ratio and the baseline is upstream PyTorch).</p>
<table border="1" class="docutils">
<thead>
<tr>
<th align="center"></th>
<th align="center">Imperative - Operator Injection</th>
<th align="center">Imperative - Mixed Precision</th>
<th align="center">JIT- Operator Injection</th>
<th align="center">JIT - Mixed Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">RN50</td>
<td align="center">2.68</td>
<td align="center">5.01</td>
<td align="center">5.14</td>
<td align="center">9.66</td>
</tr>
<tr>
<td align="center">ResNet3D</td>
<td align="center">3.00</td>
<td align="center">4.67</td>
<td align="center">5.19</td>
<td align="center">8.39</td>
</tr>
<tr>
<td align="center">BERT-LARGE</td>
<td align="center">0.99</td>
<td align="center">1.40</td>
<td align="center">N/A</td>
<td align="center">N/A</td>
</tr>
</tbody>
</table><p>We also measured the performance of ResNeXt101, Transformer-FB, DLRM, and YOLOv3 with the extension. We observed that the performance could be significantly improved by the extension as expected.</p>
</section>
<section id="id17">
<h3>Known issue<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/issues/10">#10</a> All data types have not been registered for DPCPP</p></li>
<li><p><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/issues/37">#37</a> MaxPool can’t get nan result when input’s value is nan</p></li>
</ul>
</section>
<section id="note">
<h3>NOTE<a class="headerlink" href="#note" title="Permalink to this headline"></a></h3>
<p>The extension supported PyTorch v1.5.0-rc3. Support for other PyTorch versions is working in progress.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="features/int8.html" class="btn btn-neutral float-left" title="Intel® Extension for PyTorch* optimizations for quantization (Experimental)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>