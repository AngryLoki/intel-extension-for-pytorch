// Autogenerated file by gen_code.py
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/scripts/gpu/gen_code.py
// --declarations-path
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/scripts/declarations/Declarations.yaml
// --out
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/csrc/aten/generated/ATen/
// --source-path /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu. Do
// not edit directly!
#include <ATen/AtenIpexTypeQuantizedXPU.h>

#include <ATen/Context.h>
#include <ATen/DeviceGuard.h>
#include <torch/library.h>

namespace at {

namespace AtenIpexTypeQuantizedXPU_impl {

#if 0
Tensor add_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::add(self, other, alpha);
}
Tensor as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::as_strided(_self, size, stride, storage_offset);
}
Tensor & copy_(Tensor & self, const Tensor & src, bool non_blocking) {
  // DeviceGuard omitted
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _src = AtenIpexTypeXPU::to_plain_if_needed(src);
  return AtenIpexTypeQuantizedXPU::copy_(self, _src, non_blocking);
}
Tensor empty_memory_format(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
  const DeviceGuard device_guard(options.device());

  return AtenIpexTypeQuantizedXPU::empty(size, options, memory_format);
}
Tensor _empty_affine_quantized(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<MemoryFormat> memory_format) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, double, int64_t, c10::optional<MemoryFormat>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( IntArrayRef, const TensorOptions &, double, int64_t, c10::optional<MemoryFormat> ), AtenIpexTypeQuantizedXPU::_empty_affine_quantized>()))::func_ptr()(size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
const Tensor & resize_(const Tensor & self, IntArrayRef size, c10::optional<MemoryFormat> memory_format) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::resize_(_self, size, memory_format);
}
Tensor empty_strided(IntArrayRef size, IntArrayRef stride, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (IntArrayRef, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( IntArrayRef, IntArrayRef, const TensorOptions & ), AtenIpexTypeQuantizedXPU::empty_strided>()))::func_ptr()(size, stride, dtype, layout, device, pin_memory);
}
Tensor quantized_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::quantized_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor clone(const Tensor & self, c10::optional<MemoryFormat> memory_format) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::clone(_self, memory_format);
}
Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::addmm(self, mat1, mat2, beta, alpha);
}
Tensor quantize_per_tensor(const Tensor & self, double scale, int64_t zero_point, ScalarType dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::quantize_per_tensor(self, scale, zero_point, dtype);
}
Tensor dequantize_self(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::dequantize(self);
}
double q_scale(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::q_scale(self);
}
int64_t q_zero_point(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::q_zero_point(self);
}
Tensor q_per_channel_scales(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::q_per_channel_scales(self);
}
Tensor q_per_channel_zero_points(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::q_per_channel_zero_points(self);
}
int64_t q_per_channel_axis(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::q_per_channel_axis(self);
}
Tensor int_repr(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::int_repr(_self);
}
QScheme qscheme(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::qscheme(self);
}
Tensor & set_quantizer_(Tensor & self, ConstQuantizerPtr quantizer) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::set_quantizer_(self, quantizer);
}
Tensor view(const Tensor & self, IntArrayRef size) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::view(_self, size);
}
bool equal(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeQuantizedXPU::equal(_self, _other);
}
Tensor leaky_relu(const Tensor & self, Scalar negative_slope) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::leaky_relu(_self, negative_slope);
}
Tensor & leaky_relu_(Tensor & self, Scalar negative_slope) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeQuantizedXPU::leaky_relu_(self, negative_slope);
}
Tensor & adaptive_avg_pool2d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeQuantizedXPU::adaptive_avg_pool2d_out(out, _self, output_size);
}
Tensor adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::adaptive_avg_pool2d(self, output_size);
}
Tensor _adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::_adaptive_avg_pool2d(self, output_size);
}
Tensor & avg_pool2d_out_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::avg_pool2d_out(out, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor avg_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
std::tuple<Tensor,Tensor> max_pool2d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor upsample_nearest2d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeQuantizedXPU::upsample_nearest2d(input, output_size, scale_factors);
}
Tensor upsample_nearest2d(const Tensor & self, IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeQuantizedXPU::upsample_nearest2d(self, output_size, scales_h, scales_w);
}
void record_stream(Tensor & self, Stream s) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
 AtenIpexTypeQuantizedXPU::record_stream(self, s);
}
#endif

} // namespace AtenIpexTypeQuantizedXPU_impl

namespace AtenIpexTypeQuantizedXPU {

#if 0
TORCH_LIBRARY_IMPL(aten, QuantizedXPU, m) {
  m.impl("add.Tensor",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::add_Tensor)
  );
  m.impl("as_strided",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::as_strided)
  );
  m.impl("copy_",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::copy_)
  );
  m.impl("empty.memory_format",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::empty_memory_format)
  );
  m.impl("_empty_affine_quantized",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::_empty_affine_quantized)
  );
  m.impl("resize_",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::resize_)
  );
  m.impl("empty_strided",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::empty_strided)
  );
  m.impl("quantized_max_pool2d",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::quantized_max_pool2d)
  );
  m.impl("clone",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::clone)
  );
  m.impl("addmm",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::addmm)
  );
  m.impl("quantize_per_tensor",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::quantize_per_tensor)
  );
  m.impl("dequantize.self",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::dequantize_self)
  );
  m.impl("q_scale",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::q_scale)
  );
  m.impl("q_zero_point",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::q_zero_point)
  );
  m.impl("q_per_channel_scales",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::q_per_channel_scales)
  );
  m.impl("q_per_channel_zero_points",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::q_per_channel_zero_points)
  );
  m.impl("q_per_channel_axis",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::q_per_channel_axis)
  );
  m.impl("int_repr",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::int_repr)
  );
  m.impl("qscheme",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::qscheme)
  );
  m.impl("set_quantizer_",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::set_quantizer_)
  );
  m.impl("view",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::view)
  );
  m.impl("equal",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::equal)
  );
  m.impl("leaky_relu",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::leaky_relu)
  );
  m.impl("leaky_relu_",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::leaky_relu_)
  );
  m.impl("adaptive_avg_pool2d.out",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::adaptive_avg_pool2d_out_out)
  );
  m.impl("adaptive_avg_pool2d",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::adaptive_avg_pool2d)
  );
  m.impl("_adaptive_avg_pool2d",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::_adaptive_avg_pool2d)
  );
  m.impl("avg_pool2d.out",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::avg_pool2d_out_out)
  );
  m.impl("avg_pool2d",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::avg_pool2d)
  );
  m.impl("max_pool2d_with_indices",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::max_pool2d_with_indices)
  );
  m.impl("upsample_nearest2d.vec",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::upsample_nearest2d_vec)
  );
  m.impl("upsample_nearest2d",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::upsample_nearest2d)
  );
  m.impl("record_stream",
  TORCH_FN(AtenIpexTypeQuantizedXPU_impl::record_stream)
  );
}
#endif

} // namespace AtenIpexTypeQuantizedXPU

} // namespace at
