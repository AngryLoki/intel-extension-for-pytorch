// Autogenerated file by gen_code.py
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/scripts/gpu/gen_code.py
// --declarations-path
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/scripts/declarations/Declarations.yaml
// --out
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/csrc/aten/generated/ATen/
// --source-path /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu. Do
// not edit directly!
#include <ATen/AtenIpexTypeSparseXPU.h>

#include <ATen/Context.h>
#include <ATen/DeviceGuard.h>
#include <torch/library.h>

namespace at {

namespace AtenIpexTypeSparseXPU_impl {

#if 0
Tensor empty_memory_format(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
  const DeviceGuard device_guard(options.device());

  return AtenIpexTypeSparseXPU::empty(size, options, memory_format);
}
Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory) {
  const DeviceGuard device_guard(device_or_default(device));
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  auto _values = AtenIpexTypeXPU::to_plain_if_needed(values);
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (int64_t, int64_t, IntArrayRef, const Tensor &, const Tensor &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( int64_t, int64_t, IntArrayRef, const Tensor &, const Tensor &, const TensorOptions & ), AtenIpexTypeSparseXPU::_sparse_coo_tensor_with_dims_and_tensors>()))::func_ptr()(sparse_dim, dense_dim, size, _indices, _values, dtype, layout, device, pin_memory);
}
Tensor sparse_mask(const Tensor & self, const Tensor & mask) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  return AtenIpexTypeSparseXPU::sparse_mask(_self, _mask);
}
int64_t sparse_dim(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::sparse_dim(_self);
}
int64_t dense_dim(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::dense_dim(_self);
}
int64_t _nnz(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::_nnz(_self);
}
Tensor coalesce(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::coalesce(_self);
}
bool is_coalesced(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::is_coalesced(_self);
}
Tensor _indices(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::_indices(_self);
}
Tensor _values(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeSparseXPU::_values(_self);
}
Tensor & _coalesced_(Tensor & self, bool coalesced) {
  // DeviceGuard omitted
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeSparseXPU::_coalesced_(self, coalesced);
}
Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _src = AtenIpexTypeXPU::to_plain_if_needed(src);
  return AtenIpexTypeSparseXPU::copy_sparse_to_sparse_(self, _src, non_blocking);
}
#endif

} // namespace AtenIpexTypeSparseXPU_impl

namespace AtenIpexTypeSparseXPU {

#if 0
TORCH_LIBRARY_IMPL(aten, SparseXPU, m) {
  m.impl("empty.memory_format",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::empty_memory_format)
  );
  m.impl("_sparse_coo_tensor_with_dims_and_tensors",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::_sparse_coo_tensor_with_dims_and_tensors)
  );
  m.impl("sparse_mask",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::sparse_mask)
  );
  m.impl("sparse_dim",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::sparse_dim)
  );
  m.impl("dense_dim",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::dense_dim)
  );
  m.impl("_nnz",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::_nnz)
  );
  m.impl("coalesce",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::coalesce)
  );
  m.impl("is_coalesced",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::is_coalesced)
  );
  m.impl("_indices",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::_indices)
  );
  m.impl("_values",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::_values)
  );
  m.impl("_coalesced_",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::_coalesced_)
  );
  m.impl("copy_sparse_to_sparse_",
  TORCH_FN(AtenIpexTypeSparseXPU_impl::copy_sparse_to_sparse_)
  );
}
#endif

} // namespace AtenIpexTypeSparseXPU

} // namespace at
