// Autogenerated file by gen_code.py
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/scripts/gpu/gen_code.py
// --declarations-path
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/scripts/declarations/Declarations.yaml
// --out
// /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu/csrc/aten/generated/ATen/
// --source-path /home/gta/work/rebase_pt110/frameworks.ai.pytorch.ipex-gpu. Do
// not edit directly!
#include <ATen/AtenIpexTypeXPU.h>

#include <ATen/Context.h>
#include <ATen/DeviceGuard.h>
#include <torch/library.h>

namespace at {

namespace AtenIpexTypeXPU_impl {

#if 0
std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_fused_dropout(_self, p, generator);
}
Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  return AtenIpexTypeXPU::_masked_scale(_self, _mask, scale);
}
Tensor & abs_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::abs_out(out, _self);
}
Tensor & sgn_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sgn_out(out, _self);
}
Tensor & conj_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::conj_out(out, _self);
}
Tensor & acos_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::acos_out(out, _self);
}
Tensor add_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::add(self, other, alpha);
}
Tensor & add__Tensor(Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::add_(self, other, alpha);
}
Tensor & add_out_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::add_out(out, self, other, alpha);
}
Tensor add_Scalar(const Tensor & self, Scalar other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::add(self, other, alpha);
}
Tensor & add__Scalar(Tensor & self, Scalar other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::add_(self, other, alpha);
}
Tensor addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mat = AtenIpexTypeXPU::to_plain_if_needed(mat);
  auto _vec = AtenIpexTypeXPU::to_plain_if_needed(vec);
  return AtenIpexTypeXPU::addmv(_self, _mat, _vec, beta, alpha);
}
Tensor & addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _mat = AtenIpexTypeXPU::to_plain_if_needed(mat);
  auto _vec = AtenIpexTypeXPU::to_plain_if_needed(vec);
  return AtenIpexTypeXPU::addmv_(self, _mat, _vec, beta, alpha);
}
Tensor addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _vec1 = AtenIpexTypeXPU::to_plain_if_needed(vec1);
  auto _vec2 = AtenIpexTypeXPU::to_plain_if_needed(vec2);
  return AtenIpexTypeXPU::addr(_self, _vec1, _vec2, beta, alpha);
}
Tensor & addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _vec1 = AtenIpexTypeXPU::to_plain_if_needed(vec1);
  auto _vec2 = AtenIpexTypeXPU::to_plain_if_needed(vec2);
  return AtenIpexTypeXPU::addr_(self, _vec1, _vec2, beta, alpha);
}
Tensor all_dim(const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::all(_self, dim, keepdim);
}
Tensor & all_out_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::all_out(out, _self, dim, keepdim);
}
Tensor any_dim(const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::any(_self, dim, keepdim);
}
Tensor & any_out_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::any_out(out, _self, dim, keepdim);
}
Tensor & arange_out_start_out(Tensor & out, Scalar start, Scalar end, Scalar step) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::arange_out(out, start, end, step);
}
Tensor argmax(const Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::argmax(_self, dim, keepdim);
}
Tensor argmin(const Tensor & self, c10::optional<int64_t> dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::argmin(_self, dim, keepdim);
}
Tensor as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::as_strided(_self, size, stride, storage_offset);
}
Tensor & asin_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::asin_out(out, _self);
}
Tensor & atan_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::atan_(self);
}
Tensor & atan_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::atan_out(out, _self);
}
Tensor baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _batch1 = AtenIpexTypeXPU::to_plain_if_needed(batch1);
  auto _batch2 = AtenIpexTypeXPU::to_plain_if_needed(batch2);
  return AtenIpexTypeXPU::baddbmm(_self, _batch1, _batch2, beta, alpha);
}
Tensor & baddbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _batch1 = AtenIpexTypeXPU::to_plain_if_needed(batch1);
  auto _batch2 = AtenIpexTypeXPU::to_plain_if_needed(batch2);
  return AtenIpexTypeXPU::baddbmm_(self, _batch1, _batch2, beta, alpha);
}
Tensor & baddbmm_out_out(Tensor & out, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _batch1 = AtenIpexTypeXPU::to_plain_if_needed(batch1);
  auto _batch2 = AtenIpexTypeXPU::to_plain_if_needed(batch2);
  return AtenIpexTypeXPU::baddbmm_out(out, _self, _batch1, _batch2, beta, alpha);
}
Tensor & bernoulli__Tensor(Tensor & self, const Tensor & p, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _p = AtenIpexTypeXPU::to_plain_if_needed(p);
  return AtenIpexTypeXPU::bernoulli_(self, _p, generator);
}
Tensor & bernoulli__float(Tensor & self, double p, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::bernoulli_(self, p, generator);
}
Tensor binary_cross_entropy(const Tensor & self, const Tensor & target, const c10::optional<Tensor>& weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, int64_t ), AtenIpexTypeXPU::binary_cross_entropy>()))::func_ptr()(_self, _target, _weight, reduction);
}
Tensor & binary_cross_entropy_out_out(Tensor & out, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::binary_cross_entropy_out(out, _self, _target, _weight, reduction);
}
Tensor binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const c10::optional<Tensor>& weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t ), AtenIpexTypeXPU::binary_cross_entropy_backward>()))::func_ptr()(_grad_output, _self, _target, _weight, reduction);
}
Tensor & binary_cross_entropy_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::binary_cross_entropy_backward_out(grad_input, _grad_output, _self, _target, _weight, reduction);
}
Tensor bincount(const Tensor & self, const c10::optional<Tensor>& weights, int64_t minlength) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  c10::optional<Tensor> _weights;
  if (weights.has_value())
      _weights = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weights.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const c10::optional<Tensor>&, int64_t)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, int64_t ), AtenIpexTypeXPU::bincount>()))::func_ptr()(_self, _weights, minlength);
}
Tensor bitwise_not(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::bitwise_not(_self);
}
Tensor & bitwise_not_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::bitwise_not_(self);
}
Tensor & bitwise_not_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::bitwise_not_out(out, _self);
}
Tensor logical_not(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::logical_not(_self);
}
Tensor & logical_not_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::logical_not_(self);
}
Tensor & logical_not_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::logical_not_out(out, _self);
}
Tensor & logical_xor_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::logical_xor_out(out, _self, _other);
}
Tensor & logical_and_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::logical_and_out(out, _self, _other);
}
Tensor & logical_or_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::logical_or_out(out, _self, _other);
}
Tensor bmm(const Tensor & self, const Tensor & mat2) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mat2 = AtenIpexTypeXPU::to_plain_if_needed(mat2);
  return AtenIpexTypeXPU::bmm(_self, _mat2);
}
Tensor & bmm_out_out(Tensor & out, const Tensor & self, const Tensor & mat2) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mat2 = AtenIpexTypeXPU::to_plain_if_needed(mat2);
  return AtenIpexTypeXPU::bmm_out(out, _self, _mat2);
}
Tensor & ceil_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::ceil_out(out, _self);
}
Tensor chain_matmul(TensorList matrices) {
  const OptionalDeviceGuard device_guard(device_of(matrices));
  auto matrices_vec = AtenIpexTypeXPU::to_plain_if_needed(matrices);
  auto _matrices = at::TensorList(matrices_vec);
  return AtenIpexTypeXPU::chain_matmul(_matrices);
}
Tensor clamp(const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::clamp(_self, min, max);
}
Tensor & clamp_(Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::clamp_(self, min, max);
}
Tensor & clamp_out_out(Tensor & out, const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::clamp_out(out, _self, min, max);
}
Tensor & clamp_max_(Tensor & self, Scalar max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::clamp_max_(self, max);
}
Tensor & clamp_max_out_out(Tensor & out, const Tensor & self, Scalar max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::clamp_max_out(out, _self, max);
}
Tensor & clamp_min_(Tensor & self, Scalar min) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::clamp_min_(self, min);
}
Tensor & clamp_min_out_out(Tensor & out, const Tensor & self, Scalar min) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::clamp_min_out(out, _self, min);
}
Tensor convolution_overrideable(const Tensor & input, const Tensor & weight, const c10::optional<Tensor>& bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const c10::optional<Tensor>&, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, IntArrayRef, int64_t ), AtenIpexTypeXPU::convolution_overrideable>()))::func_ptr()(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}
std::tuple<Tensor,Tensor,Tensor> convolution_backward_overrideable(const Tensor & grad_output, const Tensor & input, const Tensor & weight, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, std::array<bool,3> output_mask) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::convolution_backward_overrideable(grad_output, input, weight, stride, padding, dilation, transposed, output_padding, groups, output_mask);
}
Tensor conv_tbc(const Tensor & self, const Tensor & weight, const Tensor & bias, int64_t pad) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  auto _bias = AtenIpexTypeXPU::to_plain_if_needed(bias);
  return AtenIpexTypeXPU::conv_tbc(_self, _weight, _bias, pad);
}
std::tuple<Tensor,Tensor,Tensor> conv_tbc_backward(const Tensor & self, const Tensor & input, const Tensor & weight, const Tensor & bias, int64_t pad) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input = AtenIpexTypeXPU::to_plain_if_needed(input);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  auto _bias = AtenIpexTypeXPU::to_plain_if_needed(bias);
  return AtenIpexTypeXPU::conv_tbc_backward(_self, _input, _weight, _bias, pad);
}
Tensor & copy_(Tensor & self, const Tensor & src, bool non_blocking) {
  // DeviceGuard omitted
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _src = AtenIpexTypeXPU::to_plain_if_needed(src);
  return AtenIpexTypeXPU::copy_(self, _src, non_blocking);
}
Tensor & cos_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::cos_(self);
}
Tensor & cos_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::cos_out(out, _self);
}
Tensor & cosh_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::cosh_out(out, _self);
}
std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) {
  const OptionalDeviceGuard device_guard(device_of(log_probs));
  auto _log_probs = AtenIpexTypeXPU::to_plain_if_needed(log_probs);
  auto _targets = AtenIpexTypeXPU::to_plain_if_needed(targets);
  return AtenIpexTypeXPU::_ctc_loss(_log_probs, _targets, input_lengths, target_lengths, blank, zero_infinity);
}
Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) {
  const OptionalDeviceGuard device_guard(device_of(grad));
  auto _grad = AtenIpexTypeXPU::to_plain_if_needed(grad);
  auto _log_probs = AtenIpexTypeXPU::to_plain_if_needed(log_probs);
  auto _targets = AtenIpexTypeXPU::to_plain_if_needed(targets);
  auto _neg_log_likelihood = AtenIpexTypeXPU::to_plain_if_needed(neg_log_likelihood);
  auto _log_alpha = AtenIpexTypeXPU::to_plain_if_needed(log_alpha);
  return AtenIpexTypeXPU::_ctc_loss_backward(_grad, _log_probs, _targets, input_lengths, target_lengths, _neg_log_likelihood, _log_alpha, blank, zero_infinity);
}
Tensor div_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::div(_self, _other);
}
Tensor & div__Tensor(Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::div_(self, _other);
}
Tensor & div_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::div_out(out, _self, _other);
}
Tensor dot(const Tensor & self, const Tensor & tensor) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _tensor = AtenIpexTypeXPU::to_plain_if_needed(tensor);
  return AtenIpexTypeXPU::dot(_self, _tensor);
}
Tensor embedding_dense_backward(const Tensor & grad_output, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::embedding_dense_backward(_grad_output, _indices, num_weights, padding_idx, scale_grad_by_freq);
}
std::tuple<Tensor,Tensor,Tensor,Tensor> _embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const c10::optional<Tensor>& per_sample_weights, bool include_last_offset) {
  const OptionalDeviceGuard device_guard(device_of(weight));
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  auto _offsets = AtenIpexTypeXPU::to_plain_if_needed(offsets);
  c10::optional<Tensor> _per_sample_weights;
  if (per_sample_weights.has_value())
      _per_sample_weights = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(per_sample_weights.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, bool, int64_t, bool, const c10::optional<Tensor>&, bool)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, bool, int64_t, bool, const Tensor &, bool ), AtenIpexTypeXPU::_embedding_bag>()))::func_ptr()(_weight, _indices, _offsets, scale_grad_by_freq, mode, sparse, _per_sample_weights, include_last_offset);
}
Tensor _embedding_bag_dense_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const c10::optional<Tensor>& per_sample_weights) {
  const OptionalDeviceGuard device_guard(device_of(grad));
  auto _grad = AtenIpexTypeXPU::to_plain_if_needed(grad);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  auto _offsets = AtenIpexTypeXPU::to_plain_if_needed(offsets);
  auto _offset2bag = AtenIpexTypeXPU::to_plain_if_needed(offset2bag);
  auto _bag_size = AtenIpexTypeXPU::to_plain_if_needed(bag_size);
  auto _maximum_indices = AtenIpexTypeXPU::to_plain_if_needed(maximum_indices);
  c10::optional<Tensor> _per_sample_weights;
  if (per_sample_weights.has_value())
      _per_sample_weights = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(per_sample_weights.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, bool, int64_t, const c10::optional<Tensor>&)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, bool, int64_t, const Tensor & ), AtenIpexTypeXPU::_embedding_bag_dense_backward>()))::func_ptr()(_grad, _indices, _offsets, _offset2bag, _bag_size, _maximum_indices, num_weights, scale_grad_by_freq, mode, _per_sample_weights);
}
Tensor empty_memory_format(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
  const DeviceGuard device_guard(options.device());

  return AtenIpexTypeXPU::empty(size, options, memory_format);
}
Tensor _empty_affine_quantized(IntArrayRef size, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, double scale, int64_t zero_point, c10::optional<MemoryFormat> memory_format) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, double, int64_t, c10::optional<MemoryFormat>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( IntArrayRef, const TensorOptions &, double, int64_t, c10::optional<MemoryFormat> ), AtenIpexTypeXPU::_empty_affine_quantized>()))::func_ptr()(size, dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
Tensor _empty_per_channel_affine_quantized(IntArrayRef size, const Tensor & scales, const Tensor & zero_points, int64_t axis, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory, c10::optional<MemoryFormat> memory_format) {
  const DeviceGuard device_guard(device_or_default(device));
  auto _scales = AtenIpexTypeXPU::to_plain_if_needed(scales);
  auto _zero_points = AtenIpexTypeXPU::to_plain_if_needed(zero_points);
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (IntArrayRef, const Tensor &, const Tensor &, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( IntArrayRef, const Tensor &, const Tensor &, int64_t, const TensorOptions &, c10::optional<MemoryFormat> ), AtenIpexTypeXPU::_empty_per_channel_affine_quantized>()))::func_ptr()(size, _scales, _zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
const Tensor & resize_(const Tensor & self, IntArrayRef size, c10::optional<MemoryFormat> memory_format) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::resize_(_self, size, memory_format);
}
Tensor empty_strided(IntArrayRef size, IntArrayRef stride, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (IntArrayRef, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( IntArrayRef, IntArrayRef, const TensorOptions & ), AtenIpexTypeXPU::empty_strided>()))::func_ptr()(size, stride, dtype, layout, device, pin_memory);
}
Tensor & erf_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::erf_(self);
}
Tensor & erf_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::erf_out(out, _self);
}
Tensor & erfc_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::erfc_(self);
}
Tensor & erfc_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::erfc_out(out, _self);
}
Tensor & exp_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::exp_(self);
}
Tensor & exp_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::exp_out(out, _self);
}
Tensor & expm1_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::expm1_out(out, _self);
}
Tensor & eye_out_out(Tensor & out, int64_t n) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::eye_out(out, n);
}
Tensor & eye_out_m_out(Tensor & out, int64_t n, int64_t m) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::eye_out(out, n, m);
}
Tensor & fill__Scalar(Tensor & self, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::fill_(self, value);
}
Tensor & fill__Tensor(Tensor & self, const Tensor & value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _value = AtenIpexTypeXPU::to_plain_if_needed(value);
  return AtenIpexTypeXPU::fill_(self, _value);
}
Tensor & floor_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::floor_out(out, _self);
}
Tensor floor_divide(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::floor_divide(_self, _other);
}
Tensor & floor_divide__Tensor(Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::floor_divide_(self, _other);
}
Tensor & floor_divide_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::floor_divide_out(out, _self, _other);
}
Tensor frac(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::frac(_self);
}
Tensor & frac_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::frac_(self);
}
Tensor & frac_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::frac_out(out, _self);
}
std::tuple<Tensor,Tensor,Tensor> native_group_norm(const Tensor & input, const c10::optional<Tensor>& weight, const c10::optional<Tensor>& bias, int64_t N, int64_t C, int64_t HxW, int64_t group, double eps) {
  const OptionalDeviceGuard device_guard(device_of(input));
  auto _input = AtenIpexTypeXPU::to_plain_if_needed(input);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  c10::optional<Tensor> _bias;
  if (bias.has_value())
      _bias = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(bias.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const c10::optional<Tensor>&, const c10::optional<Tensor>&, int64_t, int64_t, int64_t, int64_t, double)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, int64_t, int64_t, double ), AtenIpexTypeXPU::native_group_norm>()))::func_ptr()(_input, _weight, _bias, N, C, HxW, group, eps);
}
std::tuple<Tensor,Tensor,Tensor> native_group_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & rstd, const c10::optional<Tensor>& weight, int64_t N, int64_t C, int64_t HxW, int64_t group, std::array<bool,3> output_mask) {
  const OptionalDeviceGuard device_guard(device_of(grad_out));
  auto _grad_out = AtenIpexTypeXPU::to_plain_if_needed(grad_out);
  auto _input = AtenIpexTypeXPU::to_plain_if_needed(input);
  auto _mean = AtenIpexTypeXPU::to_plain_if_needed(mean);
  auto _rstd = AtenIpexTypeXPU::to_plain_if_needed(rstd);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t, int64_t, int64_t, int64_t, std::array<bool,3>)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, int64_t, int64_t, std::array<bool,3> ), AtenIpexTypeXPU::native_group_norm_backward>()))::func_ptr()(_grad_out, _input, _mean, _rstd, _weight, N, C, HxW, group, output_mask);
}
Tensor _fft_with_size_norm_modes(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntArrayRef checked_signal_sizes, int64_t normalization, bool onesided, IntArrayRef output_sizes) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_fft_with_size(_self, signal_ndim, complex_input, complex_output, inverse, checked_signal_sizes, normalization, onesided, output_sizes);
}
Tensor index_Tensor(const Tensor & self, TensorList indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto indices_vec = AtenIpexTypeXPU::to_plain_if_needed(indices);
  auto _indices = at::TensorList(indices_vec);
  return AtenIpexTypeXPU::index(_self, _indices);
}
Tensor & index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  auto _source = AtenIpexTypeXPU::to_plain_if_needed(source);
  return AtenIpexTypeXPU::index_copy_(self, dim, _index, _source);
}
Tensor & _index_put_impl_(Tensor & self, TensorList indices, const Tensor & values, bool accumulate, bool unsafe) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto indices_vec = AtenIpexTypeXPU::to_plain_if_needed(indices);
  auto _indices = at::TensorList(indices_vec);
  auto _values = AtenIpexTypeXPU::to_plain_if_needed(values);
  return AtenIpexTypeXPU::_index_put_impl_(self, _indices, _values, accumulate, unsafe);
}
Tensor inverse(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::inverse(_self);
}
Tensor & inverse_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::inverse_out(out, _self);
}
Tensor _inverse_helper(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_inverse_helper(_self);
}
Tensor isnan(const Tensor & self) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::isnan(_self);
}
std::tuple<Tensor &,Tensor &> kthvalue_out_values(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  values = AtenIpexTypeXPU::to_plain_if_needed_(values);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::kthvalue_out(values, indices, _self, k, dim, keepdim);
}
std::tuple<Tensor,Tensor,Tensor> native_layer_norm(const Tensor & input, const c10::optional<Tensor>& weight, const c10::optional<Tensor>& bias, int64_t M, int64_t N, double eps) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const c10::optional<Tensor>&, const c10::optional<Tensor>&, int64_t, int64_t, double)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, double ), AtenIpexTypeXPU::native_layer_norm>()))::func_ptr()(input, weight, bias, M, N, eps);
}
std::tuple<Tensor,Tensor,Tensor> native_layer_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & rstd, const c10::optional<Tensor>& weight, int64_t M, int64_t N, std::array<bool,3> output_mask) {
  const OptionalDeviceGuard device_guard(device_of(grad_out));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t, int64_t, std::array<bool,3>)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, std::array<bool,3> ), AtenIpexTypeXPU::native_layer_norm_backward>()))::func_ptr()(grad_out, input, mean, rstd, weight, M, N, output_mask);
}
Tensor & linspace_out_out(Tensor & out, Scalar start, Scalar end, c10::optional<int64_t> steps) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::linspace_out(out, start, end, steps);
}
Tensor & log_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::log_out(out, _self);
}
Tensor & log10_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::log10_out(out, _self);
}
Tensor & log1p_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::log1p_(self);
}
Tensor & log1p_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::log1p_out(out, _self);
}
Tensor & log2_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::log2_out(out, _self);
}
Tensor & logspace_out_out(Tensor & out, Scalar start, Scalar end, c10::optional<int64_t> steps, double base) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::logspace_out(out, start, end, steps, base);
}
Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_log_softmax(_self, dim, half_to_float);
}
Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_log_softmax_backward_data(_grad_output, _output, dim, _self);
}
std::tuple<Tensor,Tensor> _aminmax(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_aminmax(_self);
}
std::tuple<Tensor,Tensor> _aminmax_dim(const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_aminmax(_self, dim, keepdim);
}
std::tuple<Tensor,Tensor> max_dim(const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::max(_self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> max_out_dim_max(Tensor & max, Tensor & max_values, const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  max = AtenIpexTypeXPU::to_plain_if_needed_(max);
  max_values = AtenIpexTypeXPU::to_plain_if_needed_(max_values);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::max_out(max, max_values, _self, dim, keepdim);
}
Tensor amax(const Tensor & self, IntArrayRef dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::amax(_self, dim, keepdim);
}
Tensor & amax_out_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::amax_out(out, _self, dim, keepdim);
}
Tensor mean(const Tensor & self, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mean(_self, dtype);
}
Tensor mean_dim(const Tensor & self, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mean(_self, dim, keepdim, dtype);
}
Tensor & mean_out_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mean_out(out, _self, dim, keepdim, dtype);
}
std::tuple<Tensor,Tensor> min_dim(const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::min(_self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> min_out_dim_min(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  min = AtenIpexTypeXPU::to_plain_if_needed_(min);
  min_indices = AtenIpexTypeXPU::to_plain_if_needed_(min_indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::min_out(min, min_indices, _self, dim, keepdim);
}
Tensor mm(const Tensor & self, const Tensor & mat2) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::mm(self, mat2);
}
Tensor & mm_out_out(Tensor & out, const Tensor & self, const Tensor & mat2) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::mm_out(out, self, mat2);
}
std::tuple<Tensor,Tensor> mode(const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mode(_self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> mode_out_values(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  values = AtenIpexTypeXPU::to_plain_if_needed_(values);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mode_out(values, indices, _self, dim, keepdim);
}
Tensor mul_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::mul(_self, _other);
}
Tensor & mul__Tensor(Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::mul_(self, _other);
}
Tensor & mul_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::mul_out(out, _self, _other);
}
Tensor mul_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mul(_self, other);
}
Tensor & mul__Scalar(Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::mul_(self, other);
}
Tensor mv(const Tensor & self, const Tensor & vec) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _vec = AtenIpexTypeXPU::to_plain_if_needed(vec);
  return AtenIpexTypeXPU::mv(_self, _vec);
}
Tensor mvlgamma(const Tensor & self, int64_t p) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::mvlgamma(_self, p);
}
Tensor & mvlgamma_(Tensor & self, int64_t p) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::mvlgamma_(self, p);
}
Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::narrow_copy(_self, dim, start, length);
}
std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const c10::optional<Tensor>& weight, const c10::optional<Tensor>& bias, const c10::optional<Tensor>& running_mean, const c10::optional<Tensor>& running_var, bool training, double momentum, double eps) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const c10::optional<Tensor>&, const c10::optional<Tensor>&, const c10::optional<Tensor>&, const c10::optional<Tensor>&, bool, double, double)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, bool, double, double ), AtenIpexTypeXPU::native_batch_norm>()))::func_ptr()(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const c10::optional<Tensor>& weight, const c10::optional<Tensor>& running_mean, const c10::optional<Tensor>& running_var, const c10::optional<Tensor>& save_mean, const c10::optional<Tensor>& save_invstd, bool train, double eps, std::array<bool,3> output_mask) {
  const OptionalDeviceGuard device_guard(device_of(grad_out));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const c10::optional<Tensor>&, const c10::optional<Tensor>&, const c10::optional<Tensor>&, const c10::optional<Tensor>&, const c10::optional<Tensor>&, bool, double, std::array<bool,3>)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, bool, double, std::array<bool,3> ), AtenIpexTypeXPU::native_batch_norm_backward>()))::func_ptr()(grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
Tensor _cdist_forward(const Tensor & x1, const Tensor & x2, double p, c10::optional<int64_t> compute_mode) {
  const OptionalDeviceGuard device_guard(device_of(x1));
  auto _x1 = AtenIpexTypeXPU::to_plain_if_needed(x1);
  auto _x2 = AtenIpexTypeXPU::to_plain_if_needed(x2);
  return AtenIpexTypeXPU::_cdist_forward(_x1, _x2, p, compute_mode);
}
Tensor _cdist_backward(const Tensor & grad, const Tensor & x1, const Tensor & x2, double p, const Tensor & cdist) {
  const OptionalDeviceGuard device_guard(device_of(grad));
  auto _grad = AtenIpexTypeXPU::to_plain_if_needed(grad);
  auto _x1 = AtenIpexTypeXPU::to_plain_if_needed(x1);
  auto _x2 = AtenIpexTypeXPU::to_plain_if_needed(x2);
  auto _cdist = AtenIpexTypeXPU::to_plain_if_needed(cdist);
  return AtenIpexTypeXPU::_cdist_backward(_grad, _x1, _x2, p, _cdist);
}
Tensor _pdist_forward(const Tensor & self, double p) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_pdist_forward(_self, p);
}
Tensor _pdist_backward(const Tensor & grad, const Tensor & self, double p, const Tensor & pdist) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad = AtenIpexTypeXPU::to_plain_if_needed(grad);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _pdist = AtenIpexTypeXPU::to_plain_if_needed(pdist);
  return AtenIpexTypeXPU::_pdist_backward(_grad, _self, p, _pdist);
}
Tensor randperm(int64_t n, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( int64_t, const TensorOptions & ), AtenIpexTypeXPU::randperm>()))::func_ptr()(n, dtype, layout, device, pin_memory);
}
Tensor randperm_generator(int64_t n, c10::optional<Generator> generator, const TensorOptions & options) {
  const DeviceGuard device_guard(options.device());

  return AtenIpexTypeXPU::randperm(n, generator, options);
}
Tensor & randperm_out_out(Tensor & out, int64_t n) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::randperm_out(out, n);
}
Tensor & randperm_out_generator_out(Tensor & out, int64_t n, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::randperm_out(out, n, generator);
}
Tensor & range_out_out(Tensor & out, Scalar start, Scalar end, Scalar step) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  return AtenIpexTypeXPU::range_out(out, start, end, step);
}
Tensor & reciprocal_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::reciprocal_out(out, _self);
}
Tensor & neg_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::neg_out(out, _self);
}
Tensor & round_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::round_out(out, _self);
}
Tensor rrelu(const Tensor & self, Scalar lower, Scalar upper, bool training, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::rrelu(_self, lower, upper, training, generator);
}
Tensor & rrelu_(Tensor & self, Scalar lower, Scalar upper, bool training, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::rrelu_(self, lower, upper, training, generator);
}
Tensor relu(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::relu(self);
}
Tensor & relu_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::relu_(self);
}
Tensor prelu(const Tensor & self, const Tensor & weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::prelu(_self, _weight);
}
std::tuple<Tensor,Tensor> prelu_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::prelu_backward(_grad_output, _self, _weight);
}
Tensor gelu(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::gelu(_self);
}
Tensor gelu_backward(const Tensor & grad, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad = AtenIpexTypeXPU::to_plain_if_needed(grad);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::gelu_backward(_grad, _self);
}
Tensor hardshrink(const Tensor & self, Scalar lambd) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::hardshrink(_self, lambd);
}
Tensor hardshrink_backward(const Tensor & grad_out, const Tensor & self, Scalar lambd) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_out = AtenIpexTypeXPU::to_plain_if_needed(grad_out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::hardshrink_backward(_grad_out, _self, lambd);
}
Tensor & rsqrt_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::rsqrt_out(out, _self);
}
Tensor sigmoid(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sigmoid(_self);
}
Tensor & sigmoid_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::sigmoid_(self);
}
Tensor & sigmoid_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sigmoid_out(out, _self);
}
Tensor & sin_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sin_out(out, _self);
}
Tensor & sinh_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sinh_out(out, _self);
}
Tensor slice_Tensor(const Tensor & self, int64_t dim, int64_t start, int64_t end, int64_t step) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::slice(_self, dim, start, end, step);
}
Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::_softmax(self, dim, half_to_float);
}
Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::_softmax_backward_data(grad_output, output, dim, self);
}
Tensor sum(const Tensor & self, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sum(_self, dtype);
}
Tensor sum_dim_IntList(const Tensor & self, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sum(_self, dim, keepdim, dtype);
}
Tensor & sum_out_IntList_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sum_out(out, _self, dim, keepdim, dtype);
}
Tensor & sqrt_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sqrt_out(out, _self);
}
Tensor std(const Tensor & self, bool unbiased) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::std(_self, unbiased);
}
Tensor std_dim(const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::std(_self, dim, unbiased, keepdim);
}
std::tuple<Tensor,Tensor> std_mean(const Tensor & self, bool unbiased) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::std_mean(_self, unbiased);
}
std::tuple<Tensor,Tensor> std_mean_dim(const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::std_mean(_self, dim, unbiased, keepdim);
}
Tensor & std_out_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::std_out(out, _self, dim, unbiased, keepdim);
}
Tensor prod(const Tensor & self, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::prod(_self, dtype);
}
Tensor prod_dim_int(const Tensor & self, int64_t dim, bool keepdim, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::prod(_self, dim, keepdim, dtype);
}
Tensor & prod_out_int_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim, c10::optional<ScalarType> dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::prod_out(out, _self, dim, keepdim, dtype);
}
Tensor & tan_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::tan_(self);
}
Tensor & tan_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::tan_out(out, _self);
}
Tensor tanh(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::tanh(_self);
}
Tensor & tanh_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::tanh_(self);
}
Tensor & tanh_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::tanh_out(out, _self);
}
Tensor threshold(const Tensor & self, Scalar threshold, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::threshold(_self, threshold, value);
}
Tensor & threshold_(Tensor & self, Scalar threshold, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::threshold_(self, threshold, value);
}
Tensor & threshold_out_out(Tensor & out, const Tensor & self, Scalar threshold, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::threshold_out(out, _self, threshold, value);
}
Tensor threshold_backward(const Tensor & grad_output, const Tensor & self, Scalar threshold) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::threshold_backward(grad_output, self, threshold);
}
Tensor flip(const Tensor & self, IntArrayRef dims) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::flip(_self, dims);
}
Tensor roll(const Tensor & self, IntArrayRef shifts, IntArrayRef dims) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::roll(_self, shifts, dims);
}
Tensor rot90(const Tensor & self, int64_t k, IntArrayRef dims) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::rot90(_self, k, dims);
}
Tensor & trunc_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::trunc_out(out, _self);
}
std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_unique(_self, sorted, return_inverse);
}
std::tuple<Tensor,Tensor,Tensor> unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::unique_dim(_self, dim, sorted, return_inverse, return_counts);
}
std::tuple<Tensor,Tensor,Tensor> unique_consecutive(const Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::unique_consecutive(_self, return_inverse, return_counts, dim);
}
std::tuple<Tensor,Tensor,Tensor> unique_dim_consecutive(const Tensor & self, int64_t dim, bool return_inverse, bool return_counts) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::unique_dim_consecutive(_self, dim, return_inverse, return_counts);
}
std::tuple<Tensor,Tensor,Tensor> _unique2(const Tensor & self, bool sorted, bool return_inverse, bool return_counts) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_unique2(_self, sorted, return_inverse, return_counts);
}
Tensor var(const Tensor & self, bool unbiased) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::var(_self, unbiased);
}
Tensor var_dim(const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::var(_self, dim, unbiased, keepdim);
}
std::tuple<Tensor,Tensor> var_mean(const Tensor & self, bool unbiased) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::var_mean(_self, unbiased);
}
std::tuple<Tensor,Tensor> var_mean_dim(const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::var_mean(_self, dim, unbiased, keepdim);
}
Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _condition = AtenIpexTypeXPU::to_plain_if_needed(condition);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::_s_where(_condition, _self, _other);
}
Tensor norm_ScalarOpt_dtype(const Tensor & self, c10::optional<Scalar> p, ScalarType dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::norm(_self, p, dtype);
}
Tensor norm_Scalar(const Tensor & self, Scalar p) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::norm(_self, p);
}
Tensor norm_ScalarOpt_dim_dtype(const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim, ScalarType dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::norm(_self, p, dim, keepdim, dtype);
}
Tensor norm_ScalarOpt_dim(const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::norm(_self, p, dim, keepdim);
}
Tensor & norm_out_dtype_out(Tensor & out, const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim, ScalarType dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::norm_out(out, _self, p, dim, keepdim, dtype);
}
Tensor & norm_out_out(Tensor & out, const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::norm_out(out, _self, p, dim, keepdim);
}
Tensor clone(const Tensor & self, c10::optional<MemoryFormat> memory_format) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::clone(_self, memory_format);
}
const Tensor & resize_as_(const Tensor & self, const Tensor & the_template, c10::optional<MemoryFormat> memory_format) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _the_template = AtenIpexTypeXPU::to_plain_if_needed(the_template);
  return AtenIpexTypeXPU::resize_as_(_self, _the_template, memory_format);
}
Tensor & zero_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::zero_(self);
}
Tensor & sub_out_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::sub_out(out, _self, _other, alpha);
}
Tensor sub_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::sub(_self, _other, alpha);
}
Tensor & sub__Tensor(Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::sub_(self, _other, alpha);
}
Tensor sub_Scalar(const Tensor & self, Scalar other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sub(_self, other, alpha);
}
Tensor & sub__Scalar(Tensor & self, Scalar other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::sub_(self, other, alpha);
}
Tensor rsub_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::rsub(_self, _other, alpha);
}
Tensor rsub_Scalar(const Tensor & self, Scalar other, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::rsub(_self, other, alpha);
}
Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::addmm(self, mat1, mat2, beta, alpha);
}
Tensor & addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::addmm_(self, mat1, mat2, beta, alpha);
}
Tensor quantize_per_tensor(const Tensor & self, double scale, int64_t zero_point, ScalarType dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::quantize_per_tensor(self, scale, zero_point, dtype);
}
Tensor quantize_per_channel(const Tensor & self, const Tensor & scales, const Tensor & zero_points, int64_t axis, ScalarType dtype) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::quantize_per_channel(self, scales, zero_points, axis, dtype);
}
Tensor dequantize_self(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::dequantize(self);
}
Tensor _make_per_tensor_quantized_tensor(const Tensor & self, double scale, int64_t zero_point) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_make_per_tensor_quantized_tensor(_self, scale, zero_point);
}
Tensor _make_per_channel_quantized_tensor(const Tensor & self, const Tensor & scale, const Tensor & zero_point, int64_t axis) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _scale = AtenIpexTypeXPU::to_plain_if_needed(scale);
  auto _zero_point = AtenIpexTypeXPU::to_plain_if_needed(zero_point);
  return AtenIpexTypeXPU::_make_per_channel_quantized_tensor(_self, _scale, _zero_point, axis);
}
Scalar _local_scalar_dense(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_local_scalar_dense(_self);
}
Tensor & set__source_Storage(Tensor & self, Storage source) {
  // DeviceGuard omitted
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::set_(self, source);
}
Tensor & set__source_Storage_storage_offset(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) {
  // DeviceGuard omitted
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::set_(self, source, storage_offset, size, stride);
}
Tensor & set__source_Tensor(Tensor & self, const Tensor & source) {
  // DeviceGuard omitted
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _source = AtenIpexTypeXPU::to_plain_if_needed(source);
  return AtenIpexTypeXPU::set_(self, _source);
}
Tensor & set_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::set_(self);
}
bool is_set_to(const Tensor & self, const Tensor & tensor) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _tensor = AtenIpexTypeXPU::to_plain_if_needed(tensor);
  return AtenIpexTypeXPU::is_set_to(_self, _tensor);
}
Tensor & masked_fill__Scalar(Tensor & self, const Tensor & mask, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  return AtenIpexTypeXPU::masked_fill_(self, _mask, value);
}
Tensor & masked_fill__Tensor(Tensor & self, const Tensor & mask, const Tensor & value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  auto _value = AtenIpexTypeXPU::to_plain_if_needed(value);
  return AtenIpexTypeXPU::masked_fill_(self, _mask, _value);
}
Tensor & masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  auto _source = AtenIpexTypeXPU::to_plain_if_needed(source);
  return AtenIpexTypeXPU::masked_scatter_(self, _mask, _source);
}
Tensor view(const Tensor & self, IntArrayRef size) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::view(_self, size);
}
Tensor & put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  auto _source = AtenIpexTypeXPU::to_plain_if_needed(source);
  return AtenIpexTypeXPU::put_(self, _index, _source, accumulate);
}
Tensor & index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  auto _source = AtenIpexTypeXPU::to_plain_if_needed(source);
  return AtenIpexTypeXPU::index_add_(self, dim, _index, _source);
}
Tensor & index_fill__int_Scalar(Tensor & self, int64_t dim, const Tensor & index, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::index_fill_(self, dim, _index, value);
}
Tensor & index_fill__int_Tensor(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  auto _value = AtenIpexTypeXPU::to_plain_if_needed(value);
  return AtenIpexTypeXPU::index_fill_(self, dim, _index, _value);
}
Tensor & scatter__src(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  auto _src = AtenIpexTypeXPU::to_plain_if_needed(src);
  return AtenIpexTypeXPU::scatter_(self, dim, _index, _src);
}
Tensor & scatter__value(Tensor & self, int64_t dim, const Tensor & index, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::scatter_(self, dim, _index, value);
}
Tensor & scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  auto _src = AtenIpexTypeXPU::to_plain_if_needed(src);
  return AtenIpexTypeXPU::scatter_add_(self, dim, _index, _src);
}
Tensor & bitwise_and_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::bitwise_and_out(out, _self, _other);
}
Tensor & bitwise_and_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::bitwise_and_out(out, _self, other);
}
Tensor & bitwise_or_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::bitwise_or_out(out, _self, _other);
}
Tensor & bitwise_or_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::bitwise_or_out(out, _self, other);
}
Tensor & bitwise_xor_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::bitwise_xor_out(out, _self, _other);
}
Tensor & bitwise_xor_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::bitwise_xor_out(out, _self, other);
}
Tensor & lgamma_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::lgamma_(self);
}
Tensor & atan2_(Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::atan2_(self, _other);
}
Tensor & tril_(Tensor & self, int64_t diagonal) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::tril_(self, diagonal);
}
Tensor & triu_(Tensor & self, int64_t diagonal) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::triu_(self, diagonal);
}
Tensor & digamma_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::digamma_(self);
}
Tensor & polygamma_(Tensor & self, int64_t n) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::polygamma_(self, n);
}
Tensor & renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::renorm_(self, p, dim, maxnorm);
}
Tensor & pow__Scalar(Tensor & self, Scalar exponent) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::pow_(self, exponent);
}
Tensor & pow__Tensor(Tensor & self, const Tensor & exponent) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _exponent = AtenIpexTypeXPU::to_plain_if_needed(exponent);
  return AtenIpexTypeXPU::pow_(self, _exponent);
}
Tensor & lerp__Scalar(Tensor & self, const Tensor & end, Scalar weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _end = AtenIpexTypeXPU::to_plain_if_needed(end);
  return AtenIpexTypeXPU::lerp_(self, _end, weight);
}
Tensor & lerp__Tensor(Tensor & self, const Tensor & end, const Tensor & weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _end = AtenIpexTypeXPU::to_plain_if_needed(end);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::lerp_(self, _end, _weight);
}
Tensor & fmod__Scalar(Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::fmod_(self, other);
}
Tensor & fmod__Tensor(Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::fmod_(self, _other);
}
Tensor & remainder__Scalar(Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::remainder_(self, other);
}
Tensor & remainder__Tensor(Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::remainder_(self, _other);
}
Tensor & addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _batch1 = AtenIpexTypeXPU::to_plain_if_needed(batch1);
  auto _batch2 = AtenIpexTypeXPU::to_plain_if_needed(batch2);
  return AtenIpexTypeXPU::addbmm_(self, _batch1, _batch2, beta, alpha);
}
Tensor & addbmm_out_out(Tensor & out, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _batch1 = AtenIpexTypeXPU::to_plain_if_needed(batch1);
  auto _batch2 = AtenIpexTypeXPU::to_plain_if_needed(batch2);
  return AtenIpexTypeXPU::addbmm_out(out, _self, _batch1, _batch2, beta, alpha);
}
Tensor addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _batch1 = AtenIpexTypeXPU::to_plain_if_needed(batch1);
  auto _batch2 = AtenIpexTypeXPU::to_plain_if_needed(batch2);
  return AtenIpexTypeXPU::addbmm(_self, _batch1, _batch2, beta, alpha);
}
Tensor & addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _tensor1 = AtenIpexTypeXPU::to_plain_if_needed(tensor1);
  auto _tensor2 = AtenIpexTypeXPU::to_plain_if_needed(tensor2);
  return AtenIpexTypeXPU::addcdiv_(self, _tensor1, _tensor2, value);
}
Tensor & random__from(Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::random_(self, from, to, generator);
}
Tensor & random__to(Tensor & self, int64_t to, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::random_(self, to, generator);
}
Tensor & random_(Tensor & self, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::random_(self, generator);
}
Tensor & uniform_(Tensor & self, double from, double to, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::uniform_(self, from, to, generator);
}
Tensor & cauchy_(Tensor & self, double median, double sigma, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::cauchy_(self, median, sigma, generator);
}
Tensor & log_normal_(Tensor & self, double mean, double std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::log_normal_(self, mean, std, generator);
}
Tensor & exponential_(Tensor & self, double lambd, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::exponential_(self, lambd, generator);
}
Tensor & geometric_(Tensor & self, double p, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::geometric_(self, p, generator);
}
Tensor & diag_out_out(Tensor & out, const Tensor & self, int64_t diagonal) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::diag_out(out, _self, diagonal);
}
Tensor diag(const Tensor & self, int64_t diagonal) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::diag(_self, diagonal);
}
Tensor & cross_out_out(Tensor & out, const Tensor & self, const Tensor & other, c10::optional<int64_t> dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::cross_out(out, _self, _other, dim);
}
Tensor cross(const Tensor & self, const Tensor & other, c10::optional<int64_t> dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::cross(_self, _other, dim);
}
Tensor & triu_out_out(Tensor & out, const Tensor & self, int64_t diagonal) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::triu_out(out, _self, diagonal);
}
Tensor & tril_out_out(Tensor & out, const Tensor & self, int64_t diagonal) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::tril_out(out, _self, diagonal);
}
Tensor tril_indices(int64_t row, int64_t col, int64_t offset, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (int64_t, int64_t, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( int64_t, int64_t, int64_t, const TensorOptions & ), AtenIpexTypeXPU::tril_indices>()))::func_ptr()(row, col, offset, dtype, layout, device, pin_memory);
}
Tensor triu_indices(int64_t row, int64_t col, int64_t offset, c10::optional<ScalarType> dtype, c10::optional<Layout> layout, c10::optional<Device> device, c10::optional<bool> pin_memory) {
  const DeviceGuard device_guard(device_or_default(device));

  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (int64_t, int64_t, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( int64_t, int64_t, int64_t, const TensorOptions & ), AtenIpexTypeXPU::triu_indices>()))::func_ptr()(row, col, offset, dtype, layout, device, pin_memory);
}
Tensor trace(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::trace(_self);
}
Tensor & ne_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::ne_out(out, _self, other);
}
Tensor ne_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::ne(_self, other);
}
Tensor & ne_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::ne_out(out, _self, _other);
}
Tensor ne_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::ne(_self, _other);
}
Tensor & eq_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::eq_out(out, _self, other);
}
Tensor eq_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::eq(_self, other);
}
Tensor & eq_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::eq_out(out, _self, _other);
}
Tensor eq_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::eq(_self, _other);
}
Tensor & ge_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::ge_out(out, _self, other);
}
Tensor ge_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::ge(_self, other);
}
Tensor & ge_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::ge_out(out, _self, _other);
}
Tensor ge_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::ge(_self, _other);
}
Tensor & le_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::le_out(out, _self, other);
}
Tensor le_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::le(_self, other);
}
Tensor & le_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::le_out(out, _self, _other);
}
Tensor le_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::le(_self, _other);
}
Tensor & gt_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::gt_out(out, _self, other);
}
Tensor gt_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::gt(_self, other);
}
Tensor & gt_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::gt_out(out, _self, _other);
}
Tensor gt_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::gt(_self, _other);
}
Tensor & lt_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::lt_out(out, _self, other);
}
Tensor lt_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::lt(_self, other);
}
Tensor & lt_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::lt_out(out, _self, _other);
}
Tensor lt_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::lt(_self, _other);
}
Tensor & take_out_out(Tensor & out, const Tensor & self, const Tensor & index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::take_out(out, _self, _index);
}
Tensor take(const Tensor & self, const Tensor & index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::take(_self, _index);
}
Tensor & index_select_out_out(Tensor & out, const Tensor & self, int64_t dim, const Tensor & index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::index_select_out(out, _self, dim, _index);
}
Tensor index_select(const Tensor & self, int64_t dim, const Tensor & index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::index_select(_self, dim, _index);
}
Tensor & masked_select_out_out(Tensor & out, const Tensor & self, const Tensor & mask) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  return AtenIpexTypeXPU::masked_select_out(out, _self, _mask);
}
Tensor masked_select(const Tensor & self, const Tensor & mask) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _mask = AtenIpexTypeXPU::to_plain_if_needed(mask);
  return AtenIpexTypeXPU::masked_select(_self, _mask);
}
Tensor & nonzero_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::nonzero_out(out, _self);
}
Tensor nonzero(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::nonzero(_self);
}
Tensor & gather_out_out(Tensor & out, const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::gather_out(out, _self, dim, _index, sparse_grad);
}
Tensor gather(const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _index = AtenIpexTypeXPU::to_plain_if_needed(index);
  return AtenIpexTypeXPU::gather(_self, dim, _index, sparse_grad);
}
Tensor & addcmul_out_out(Tensor & out, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _tensor1 = AtenIpexTypeXPU::to_plain_if_needed(tensor1);
  auto _tensor2 = AtenIpexTypeXPU::to_plain_if_needed(tensor2);
  return AtenIpexTypeXPU::addcmul_out(out, _self, _tensor1, _tensor2, value);
}
Tensor addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _tensor1 = AtenIpexTypeXPU::to_plain_if_needed(tensor1);
  auto _tensor2 = AtenIpexTypeXPU::to_plain_if_needed(tensor2);
  return AtenIpexTypeXPU::addcmul(_self, _tensor1, _tensor2, value);
}
Tensor & addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _tensor1 = AtenIpexTypeXPU::to_plain_if_needed(tensor1);
  auto _tensor2 = AtenIpexTypeXPU::to_plain_if_needed(tensor2);
  return AtenIpexTypeXPU::addcmul_(self, _tensor1, _tensor2, value);
}
Tensor & addcdiv_out_out(Tensor & out, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _tensor1 = AtenIpexTypeXPU::to_plain_if_needed(tensor1);
  auto _tensor2 = AtenIpexTypeXPU::to_plain_if_needed(tensor2);
  return AtenIpexTypeXPU::addcdiv_out(out, _self, _tensor1, _tensor2, value);
}
Tensor addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _tensor1 = AtenIpexTypeXPU::to_plain_if_needed(tensor1);
  auto _tensor2 = AtenIpexTypeXPU::to_plain_if_needed(tensor2);
  return AtenIpexTypeXPU::addcdiv(_self, _tensor1, _tensor2, value);
}
std::tuple<Tensor &,Tensor &> triangular_solve_out_X(Tensor & X, Tensor & M, const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) {
  const OptionalDeviceGuard device_guard(device_of(self));
  X = AtenIpexTypeXPU::to_plain_if_needed_(X);
  M = AtenIpexTypeXPU::to_plain_if_needed_(M);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _A = AtenIpexTypeXPU::to_plain_if_needed(A);
  return AtenIpexTypeXPU::triangular_solve_out(X, M, _self, _A, upper, transpose, unitriangular);
}
std::tuple<Tensor,Tensor> triangular_solve(const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _A = AtenIpexTypeXPU::to_plain_if_needed(A);
  return AtenIpexTypeXPU::triangular_solve(_self, _A, upper, transpose, unitriangular);
}
std::tuple<Tensor,Tensor> _triangular_solve_helper(const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _A = AtenIpexTypeXPU::to_plain_if_needed(A);
  return AtenIpexTypeXPU::_triangular_solve_helper(_self, _A, upper, transpose, unitriangular);
}
std::tuple<Tensor,Tensor> _symeig_helper(const Tensor & self, bool eigenvectors, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_symeig_helper(_self, eigenvectors, upper);
}
std::tuple<Tensor &,Tensor &,Tensor &> svd_out_U(Tensor & U, Tensor & S, Tensor & V, const Tensor & self, bool some, bool compute_uv) {
  const OptionalDeviceGuard device_guard(device_of(self));
  U = AtenIpexTypeXPU::to_plain_if_needed_(U);
  S = AtenIpexTypeXPU::to_plain_if_needed_(S);
  V = AtenIpexTypeXPU::to_plain_if_needed_(V);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::svd_out(U, S, V, _self, some, compute_uv);
}
std::tuple<Tensor,Tensor,Tensor> svd(const Tensor & self, bool some, bool compute_uv) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::svd(_self, some, compute_uv);
}
std::tuple<Tensor,Tensor,Tensor> _svd_helper(const Tensor & self, bool some, bool compute_uv) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_svd_helper(_self, some, compute_uv);
}
Tensor & cholesky_out_out(Tensor & out, const Tensor & self, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::cholesky_out(out, _self, upper);
}
Tensor cholesky(const Tensor & self, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::cholesky(_self, upper);
}
Tensor & cholesky_solve_out_out(Tensor & out, const Tensor & self, const Tensor & input2, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input2 = AtenIpexTypeXPU::to_plain_if_needed(input2);
  return AtenIpexTypeXPU::cholesky_solve_out(out, _self, _input2, upper);
}
Tensor cholesky_solve(const Tensor & self, const Tensor & input2, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input2 = AtenIpexTypeXPU::to_plain_if_needed(input2);
  return AtenIpexTypeXPU::cholesky_solve(_self, _input2, upper);
}
std::tuple<Tensor,Tensor> solve(const Tensor & self, const Tensor & A) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _A = AtenIpexTypeXPU::to_plain_if_needed(A);
  return AtenIpexTypeXPU::solve(_self, _A);
}
std::tuple<Tensor &,Tensor &> solve_out_solution(Tensor & solution, Tensor & lu, const Tensor & self, const Tensor & A) {
  const OptionalDeviceGuard device_guard(device_of(self));
  solution = AtenIpexTypeXPU::to_plain_if_needed_(solution);
  lu = AtenIpexTypeXPU::to_plain_if_needed_(lu);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _A = AtenIpexTypeXPU::to_plain_if_needed(A);
  return AtenIpexTypeXPU::solve_out(solution, lu, _self, _A);
}
std::tuple<Tensor,Tensor> _solve_helper(const Tensor & self, const Tensor & A) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _A = AtenIpexTypeXPU::to_plain_if_needed(A);
  return AtenIpexTypeXPU::_solve_helper(_self, _A);
}
Tensor & cholesky_inverse_out_out(Tensor & out, const Tensor & self, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::cholesky_inverse_out(out, _self, upper);
}
Tensor cholesky_inverse(const Tensor & self, bool upper) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::cholesky_inverse(_self, upper);
}
std::tuple<Tensor &,Tensor &> qr_out_Q(Tensor & Q, Tensor & R, const Tensor & self, bool some) {
  const OptionalDeviceGuard device_guard(device_of(self));
  Q = AtenIpexTypeXPU::to_plain_if_needed_(Q);
  R = AtenIpexTypeXPU::to_plain_if_needed_(R);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::qr_out(Q, R, _self, some);
}
std::tuple<Tensor,Tensor> qr(const Tensor & self, bool some) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::qr(_self, some);
}
std::tuple<Tensor,Tensor> _qr_helper(const Tensor & self, bool some) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_qr_helper(_self, some);
}
std::tuple<Tensor &,Tensor &> geqrf_out_a(Tensor & a, Tensor & tau, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  a = AtenIpexTypeXPU::to_plain_if_needed_(a);
  tau = AtenIpexTypeXPU::to_plain_if_needed_(tau);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::geqrf_out(a, tau, _self);
}
std::tuple<Tensor,Tensor> geqrf(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::geqrf(_self);
}
Tensor & orgqr_out_out(Tensor & out, const Tensor & self, const Tensor & input2) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input2 = AtenIpexTypeXPU::to_plain_if_needed(input2);
  return AtenIpexTypeXPU::orgqr_out(out, _self, _input2);
}
Tensor orgqr(const Tensor & self, const Tensor & input2) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input2 = AtenIpexTypeXPU::to_plain_if_needed(input2);
  return AtenIpexTypeXPU::orgqr(_self, _input2);
}
Tensor & ormqr_out_out(Tensor & out, const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input2 = AtenIpexTypeXPU::to_plain_if_needed(input2);
  auto _input3 = AtenIpexTypeXPU::to_plain_if_needed(input3);
  return AtenIpexTypeXPU::ormqr_out(out, _self, _input2, _input3, left, transpose);
}
Tensor ormqr(const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _input2 = AtenIpexTypeXPU::to_plain_if_needed(input2);
  auto _input3 = AtenIpexTypeXPU::to_plain_if_needed(input3);
  return AtenIpexTypeXPU::ormqr(_self, _input2, _input3, left, transpose);
}
std::tuple<Tensor,Tensor,Tensor> _lu_with_info(const Tensor & self, bool pivot, bool check_errors) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_lu_with_info(_self, pivot, check_errors);
}
Tensor & lu_solve_out_out(Tensor & out, const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _LU_data = AtenIpexTypeXPU::to_plain_if_needed(LU_data);
  auto _LU_pivots = AtenIpexTypeXPU::to_plain_if_needed(LU_pivots);
  return AtenIpexTypeXPU::lu_solve_out(out, _self, _LU_data, _LU_pivots);
}
Tensor lu_solve(const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _LU_data = AtenIpexTypeXPU::to_plain_if_needed(LU_data);
  auto _LU_pivots = AtenIpexTypeXPU::to_plain_if_needed(LU_pivots);
  return AtenIpexTypeXPU::lu_solve(_self, _LU_data, _LU_pivots);
}
Tensor _lu_solve_helper(const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _LU_data = AtenIpexTypeXPU::to_plain_if_needed(LU_data);
  auto _LU_pivots = AtenIpexTypeXPU::to_plain_if_needed(LU_pivots);
  return AtenIpexTypeXPU::_lu_solve_helper(_self, _LU_data, _LU_pivots);
}
Tensor & multinomial_out_out(Tensor & out, const Tensor & self, int64_t num_samples, bool replacement, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::multinomial_out(out, _self, num_samples, replacement, generator);
}
Tensor multinomial(const Tensor & self, int64_t num_samples, bool replacement, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::multinomial(_self, num_samples, replacement, generator);
}
Tensor lgamma(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::lgamma(_self);
}
Tensor & digamma_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::digamma_out(out, _self);
}
Tensor digamma(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::digamma(_self);
}
Tensor & polygamma_out_out(Tensor & out, int64_t n, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::polygamma_out(out, n, _self);
}
Tensor polygamma(int64_t n, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::polygamma(n, _self);
}
Tensor erfinv(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::erfinv(_self);
}
Tensor & erfinv_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::erfinv_(self);
}
Tensor & erfinv_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::erfinv_out(out, _self);
}
Tensor sign(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sign(_self);
}
Tensor & sign_(Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::sign_(self);
}
Tensor & sign_out_out(Tensor & out, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sign_out(out, _self);
}
Tensor & atan2_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::atan2_out(out, _self, _other);
}
Tensor atan2(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::atan2(_self, _other);
}
Tensor & lerp_out_Scalar_out(Tensor & out, const Tensor & self, const Tensor & end, Scalar weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _end = AtenIpexTypeXPU::to_plain_if_needed(end);
  return AtenIpexTypeXPU::lerp_out(out, _self, _end, weight);
}
Tensor & lerp_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & end, const Tensor & weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _end = AtenIpexTypeXPU::to_plain_if_needed(end);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::lerp_out(out, _self, _end, _weight);
}
Tensor lerp_Scalar(const Tensor & self, const Tensor & end, Scalar weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _end = AtenIpexTypeXPU::to_plain_if_needed(end);
  return AtenIpexTypeXPU::lerp(_self, _end, weight);
}
Tensor lerp_Tensor(const Tensor & self, const Tensor & end, const Tensor & weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _end = AtenIpexTypeXPU::to_plain_if_needed(end);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::lerp(_self, _end, _weight);
}
Tensor & histc_out_out(Tensor & out, const Tensor & self, int64_t bins, Scalar min, Scalar max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::histc_out(out, _self, bins, min, max);
}
Tensor histc(const Tensor & self, int64_t bins, Scalar min, Scalar max) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::histc(_self, bins, min, max);
}
Tensor & fmod_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::fmod_out(out, _self, other);
}
Tensor fmod_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::fmod(_self, other);
}
Tensor & fmod_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::fmod_out(out, _self, _other);
}
Tensor fmod_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::fmod(_self, _other);
}
Tensor & remainder_out_Scalar_out(Tensor & out, const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::remainder_out(out, _self, other);
}
Tensor remainder_Scalar(const Tensor & self, Scalar other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::remainder(_self, other);
}
Tensor & remainder_out_Tensor_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::remainder_out(out, _self, _other);
}
Tensor remainder_Tensor(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::remainder(_self, _other);
}
Tensor min(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::min(_self);
}
Tensor max(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::max(_self);
}
Tensor maximum(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::maximum(_self, _other);
}
Tensor & maximum_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::maximum_out(out, _self, _other);
}
Tensor & max_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::max_out(out, _self, _other);
}
Tensor minimum(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::minimum(_self, _other);
}
Tensor & minimum_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::minimum_out(out, _self, _other);
}
Tensor & min_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::min_out(out, _self, _other);
}
Tensor median(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::median(_self);
}
std::tuple<Tensor &,Tensor &> sort_out_values(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {
  const OptionalDeviceGuard device_guard(device_of(self));
  values = AtenIpexTypeXPU::to_plain_if_needed_(values);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sort_out(values, indices, _self, dim, descending);
}
std::tuple<Tensor,Tensor> sort(const Tensor & self, int64_t dim, bool descending) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::sort(_self, dim, descending);
}
std::tuple<Tensor &,Tensor &> topk_out_values(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
  const OptionalDeviceGuard device_guard(device_of(self));
  values = AtenIpexTypeXPU::to_plain_if_needed_(values);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::topk_out(values, indices, _self, k, dim, largest, sorted);
}
std::tuple<Tensor,Tensor> topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::topk(_self, k, dim, largest, sorted);
}
Tensor all(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::all(_self);
}
Tensor any(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::any(_self);
}
Tensor & renorm_out_out(Tensor & out, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::renorm_out(out, _self, p, dim, maxnorm);
}
Tensor renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::renorm(_self, p, dim, maxnorm);
}
Tensor unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) {
  // DeviceGuard omitted
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::unfold(_self, dimension, size, step);
}
bool equal(const Tensor & self, const Tensor & other) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _other = AtenIpexTypeXPU::to_plain_if_needed(other);
  return AtenIpexTypeXPU::equal(_self, _other);
}
Tensor & pow_out_Tensor_Tensor_out(Tensor & out, const Tensor & self, const Tensor & exponent) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _exponent = AtenIpexTypeXPU::to_plain_if_needed(exponent);
  return AtenIpexTypeXPU::pow_out(out, _self, _exponent);
}
Tensor pow_Tensor_Tensor(const Tensor & self, const Tensor & exponent) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _exponent = AtenIpexTypeXPU::to_plain_if_needed(exponent);
  return AtenIpexTypeXPU::pow(_self, _exponent);
}
Tensor & pow_out_Scalar_out(Tensor & out, Scalar self, const Tensor & exponent) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _exponent = AtenIpexTypeXPU::to_plain_if_needed(exponent);
  return AtenIpexTypeXPU::pow_out(out, self, _exponent);
}
Tensor pow_Scalar(Scalar self, const Tensor & exponent) {
  const OptionalDeviceGuard device_guard(device_of(exponent));
  auto _exponent = AtenIpexTypeXPU::to_plain_if_needed(exponent);
  return AtenIpexTypeXPU::pow(self, _exponent);
}
Tensor & pow_out_Tensor_Scalar_out(Tensor & out, const Tensor & self, Scalar exponent) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::pow_out(out, _self, exponent);
}
Tensor pow_Tensor_Scalar(const Tensor & self, Scalar exponent) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::pow(_self, exponent);
}
Tensor & normal_(Tensor & self, double mean, double std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::normal_(self, mean, std, generator);
}
Tensor & normal_out_Tensor_float_out(Tensor & out, const Tensor & mean, double std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _mean = AtenIpexTypeXPU::to_plain_if_needed(mean);
  return AtenIpexTypeXPU::normal_out(out, _mean, std, generator);
}
Tensor normal_Tensor_float(const Tensor & mean, double std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(mean));
  auto _mean = AtenIpexTypeXPU::to_plain_if_needed(mean);
  return AtenIpexTypeXPU::normal(_mean, std, generator);
}
Tensor & normal_out_float_Tensor_out(Tensor & out, double mean, const Tensor & std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _std = AtenIpexTypeXPU::to_plain_if_needed(std);
  return AtenIpexTypeXPU::normal_out(out, mean, _std, generator);
}
Tensor normal_float_Tensor(double mean, const Tensor & std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(std));
  auto _std = AtenIpexTypeXPU::to_plain_if_needed(std);
  return AtenIpexTypeXPU::normal(mean, _std, generator);
}
Tensor & normal_out_Tensor_Tensor_out(Tensor & out, const Tensor & mean, const Tensor & std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _mean = AtenIpexTypeXPU::to_plain_if_needed(mean);
  auto _std = AtenIpexTypeXPU::to_plain_if_needed(std);
  return AtenIpexTypeXPU::normal_out(out, _mean, _std, generator);
}
Tensor normal_Tensor_Tensor(const Tensor & mean, const Tensor & std, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(mean));
  auto _mean = AtenIpexTypeXPU::to_plain_if_needed(mean);
  auto _std = AtenIpexTypeXPU::to_plain_if_needed(std);
  return AtenIpexTypeXPU::normal(_mean, _std, generator);
}
Tensor _cumsum(const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_cumsum(_self, dim);
}
Tensor & _cumsum_out_out(Tensor & out, const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_cumsum_out(out, _self, dim);
}
Tensor _cumprod(const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_cumprod(_self, dim);
}
Tensor & _cumprod_out_out(Tensor & out, const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_cumprod_out(out, _self, dim);
}
Tensor _var(const Tensor & self, bool unbiased) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_var(_self, unbiased);
}
Tensor _std(const Tensor & self, bool unbiased) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::_std(_self, unbiased);
}
Tensor _cat(TensorList tensors, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(tensors));
  auto tensors_vec = AtenIpexTypeXPU::to_plain_if_needed(tensors);
  auto _tensors = at::TensorList(tensors_vec);
  return AtenIpexTypeXPU::_cat(_tensors, dim);
}
Tensor & _cat_out_out(Tensor & out, TensorList tensors, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(out));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto tensors_vec = AtenIpexTypeXPU::to_plain_if_needed(tensors);
  auto _tensors = at::TensorList(tensors_vec);
  return AtenIpexTypeXPU::_cat_out(out, _tensors, dim);
}
Tensor & mse_loss_out_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::mse_loss_out(out, _self, _target, reduction);
}
Tensor mse_loss(const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::mse_loss(_self, _target, reduction);
}
Tensor & mse_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::mse_loss_backward_out(grad_input, _grad_output, _self, _target, reduction);
}
Tensor mse_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::mse_loss_backward(_grad_output, _self, _target, reduction);
}
Tensor & l1_loss_out_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::l1_loss_out(out, _self, _target, reduction);
}
Tensor l1_loss(const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::l1_loss(_self, _target, reduction);
}
Tensor & l1_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::l1_loss_backward_out(grad_input, _grad_output, _self, _target, reduction);
}
Tensor l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::l1_loss_backward(_grad_output, _self, _target, reduction);
}
Tensor & multi_margin_loss_out_out(Tensor & out, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::multi_margin_loss_out(out, _self, _target, p, margin, _weight, reduction);
}
Tensor multi_margin_loss(const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const c10::optional<Tensor>& weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, Scalar, Scalar, const c10::optional<Tensor>&, int64_t)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, Scalar, Scalar, const Tensor &, int64_t ), AtenIpexTypeXPU::multi_margin_loss>()))::func_ptr()(_self, _target, p, margin, _weight, reduction);
}
Tensor & multi_margin_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::multi_margin_loss_backward_out(grad_input, _grad_output, _self, _target, p, margin, _weight, reduction);
}
Tensor multi_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const c10::optional<Tensor>& weight, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar, const c10::optional<Tensor>&, int64_t)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, Scalar, Scalar, const Tensor &, int64_t ), AtenIpexTypeXPU::multi_margin_loss_backward>()))::func_ptr()(_grad_output, _self, _target, p, margin, _weight, reduction);
}
std::tuple<Tensor &,Tensor &> multilabel_margin_loss_forward_out_output(Tensor & output, Tensor & is_target, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  output = AtenIpexTypeXPU::to_plain_if_needed_(output);
  is_target = AtenIpexTypeXPU::to_plain_if_needed_(is_target);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::multilabel_margin_loss_forward_out(output, is_target, _self, _target, reduction);
}
std::tuple<Tensor,Tensor> multilabel_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::multilabel_margin_loss_forward(_self, _target, reduction);
}
Tensor & multilabel_margin_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _is_target = AtenIpexTypeXPU::to_plain_if_needed(is_target);
  return AtenIpexTypeXPU::multilabel_margin_loss_backward_out(grad_input, _grad_output, _self, _target, reduction, _is_target);
}
Tensor multilabel_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _is_target = AtenIpexTypeXPU::to_plain_if_needed(is_target);
  return AtenIpexTypeXPU::multilabel_margin_loss_backward(_grad_output, _self, _target, reduction, _is_target);
}
std::tuple<Tensor &,Tensor &> nll_loss_forward_out_output(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  output = AtenIpexTypeXPU::to_plain_if_needed_(output);
  total_weight = AtenIpexTypeXPU::to_plain_if_needed_(total_weight);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::nll_loss_forward_out(output, total_weight, _self, _target, _weight, reduction, ignore_index);
}
std::tuple<Tensor,Tensor> nll_loss_forward(const Tensor & self, const Tensor & target, const c10::optional<Tensor>& weight, int64_t reduction, int64_t ignore_index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor> (const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t, int64_t)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t ), AtenIpexTypeXPU::nll_loss_forward>()))::func_ptr()(_self, _target, _weight, reduction, ignore_index);
}
Tensor & nll_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  auto _total_weight = AtenIpexTypeXPU::to_plain_if_needed(total_weight);
  return AtenIpexTypeXPU::nll_loss_backward_out(grad_input, _grad_output, _self, _target, _weight, reduction, ignore_index, _total_weight);
}
Tensor nll_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const c10::optional<Tensor>& weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  auto _total_weight = AtenIpexTypeXPU::to_plain_if_needed(total_weight);
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t, int64_t, const Tensor &)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, const Tensor & ), AtenIpexTypeXPU::nll_loss_backward>()))::func_ptr()(_grad_output, _self, _target, _weight, reduction, ignore_index, _total_weight);
}
std::tuple<Tensor &,Tensor &> nll_loss2d_forward_out_output(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  output = AtenIpexTypeXPU::to_plain_if_needed_(output);
  total_weight = AtenIpexTypeXPU::to_plain_if_needed_(total_weight);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  return AtenIpexTypeXPU::nll_loss2d_forward_out(output, total_weight, _self, _target, _weight, reduction, ignore_index);
}
std::tuple<Tensor,Tensor> nll_loss2d_forward(const Tensor & self, const Tensor & target, const c10::optional<Tensor>& weight, int64_t reduction, int64_t ignore_index) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<std::tuple<Tensor,Tensor> (const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t, int64_t)>(
  ::c10::CompileTimeFunctionPointer<std::tuple<Tensor,Tensor> ( const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t ), AtenIpexTypeXPU::nll_loss2d_forward>()))::func_ptr()(_self, _target, _weight, reduction, ignore_index);
}
Tensor & nll_loss2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  auto _weight = AtenIpexTypeXPU::to_plain_if_needed(weight);
  auto _total_weight = AtenIpexTypeXPU::to_plain_if_needed(total_weight);
  return AtenIpexTypeXPU::nll_loss2d_backward_out(grad_input, _grad_output, _self, _target, _weight, reduction, ignore_index, _total_weight);
}
Tensor nll_loss2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const c10::optional<Tensor>& weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  c10::optional<Tensor> _weight;
  if (weight.has_value())
      _weight = c10::optional<Tensor>(AtenIpexTypeXPU::to_plain_if_needed(weight.value()));
  auto _total_weight = AtenIpexTypeXPU::to_plain_if_needed(total_weight);
  return decltype(c10::impl::hacky_wrapper_for_legacy_signatures<Tensor (const Tensor &, const Tensor &, const Tensor &, const c10::optional<Tensor>&, int64_t, int64_t, const Tensor &)>(
  ::c10::CompileTimeFunctionPointer<Tensor ( const Tensor &, const Tensor &, const Tensor &, const Tensor &, int64_t, int64_t, const Tensor & ), AtenIpexTypeXPU::nll_loss2d_backward>()))::func_ptr()(_grad_output, _self, _target, _weight, reduction, ignore_index, _total_weight);
}
Tensor & smooth_l1_loss_out_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction, double beta) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::smooth_l1_loss_out(out, _self, _target, reduction, beta);
}
Tensor smooth_l1_loss(const Tensor & self, const Tensor & target, int64_t reduction, double beta) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::smooth_l1_loss(_self, _target, reduction, beta);
}
Tensor & smooth_l1_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, double beta) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::smooth_l1_loss_backward_out(grad_input, _grad_output, _self, _target, reduction, beta);
}
Tensor smooth_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, double beta) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::smooth_l1_loss_backward(_grad_output, _self, _target, reduction, beta);
}
Tensor & soft_margin_loss_out_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::soft_margin_loss_out(out, _self, _target, reduction);
}
Tensor soft_margin_loss(const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::soft_margin_loss(_self, _target, reduction);
}
Tensor & soft_margin_loss_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::soft_margin_loss_backward_out(grad_input, _grad_output, _self, _target, reduction);
}
Tensor soft_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _target = AtenIpexTypeXPU::to_plain_if_needed(target);
  return AtenIpexTypeXPU::soft_margin_loss_backward(_grad_output, _self, _target, reduction);
}
Tensor & elu_out_out(Tensor & out, const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::elu_out(out, _self, alpha, scale, input_scale);
}
Tensor elu(const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::elu(_self, alpha, scale, input_scale);
}
Tensor & elu_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::elu_backward_out(grad_input, _grad_output, alpha, scale, input_scale, _output);
}
Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::elu_backward(_grad_output, alpha, scale, input_scale, _output);
}
Tensor & elu_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::elu_(self, alpha, scale, input_scale);
}
Tensor & glu_out_out(Tensor & out, const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::glu_out(out, _self, dim);
}
Tensor glu(const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::glu(_self, dim);
}
Tensor & glu_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::glu_backward_out(grad_input, _grad_output, _self, dim);
}
Tensor glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::glu_backward(_grad_output, _self, dim);
}
Tensor & hardtanh_out_out(Tensor & out, const Tensor & self, Scalar min_val, Scalar max_val) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::hardtanh_out(out, _self, min_val, max_val);
}
Tensor hardtanh(const Tensor & self, Scalar min_val, Scalar max_val) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::hardtanh(_self, min_val, max_val);
}
Tensor & hardtanh_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::hardtanh_backward_out(grad_input, _grad_output, _self, min_val, max_val);
}
Tensor hardtanh_backward(const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::hardtanh_backward(_grad_output, _self, min_val, max_val);
}
Tensor & hardtanh_(Tensor & self, Scalar min_val, Scalar max_val) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::hardtanh_(self, min_val, max_val);
}
Tensor & leaky_relu_out_out(Tensor & out, const Tensor & self, Scalar negative_slope) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::leaky_relu_out(out, _self, negative_slope);
}
Tensor leaky_relu(const Tensor & self, Scalar negative_slope) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::leaky_relu(_self, negative_slope);
}
Tensor leaky_relu_backward(const Tensor & grad_output, const Tensor & self, Scalar negative_slope, bool self_is_result) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::leaky_relu_backward(_grad_output, _self, negative_slope, self_is_result);
}
Tensor & leaky_relu_(Tensor & self, Scalar negative_slope) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  return AtenIpexTypeXPU::leaky_relu_(self, negative_slope);
}
std::tuple<Tensor &,Tensor &> log_sigmoid_forward_out_output(Tensor & output, Tensor & buffer, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  output = AtenIpexTypeXPU::to_plain_if_needed_(output);
  buffer = AtenIpexTypeXPU::to_plain_if_needed_(buffer);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::log_sigmoid_forward_out(output, buffer, _self);
}
std::tuple<Tensor,Tensor> log_sigmoid_forward(const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::log_sigmoid_forward(_self);
}
Tensor & log_sigmoid_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _buffer = AtenIpexTypeXPU::to_plain_if_needed(buffer);
  return AtenIpexTypeXPU::log_sigmoid_backward_out(grad_input, _grad_output, _self, _buffer);
}
Tensor log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _buffer = AtenIpexTypeXPU::to_plain_if_needed(buffer);
  return AtenIpexTypeXPU::log_sigmoid_backward(_grad_output, _self, _buffer);
}
Tensor & rrelu_with_noise_out_out(Tensor & out, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _noise = AtenIpexTypeXPU::to_plain_if_needed(noise);
  return AtenIpexTypeXPU::rrelu_with_noise_out(out, _self, _noise, lower, upper, training, generator);
}
Tensor rrelu_with_noise(const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _noise = AtenIpexTypeXPU::to_plain_if_needed(noise);
  return AtenIpexTypeXPU::rrelu_with_noise(_self, _noise, lower, upper, training, generator);
}
Tensor rrelu_with_noise_backward(const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, bool self_is_result) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _noise = AtenIpexTypeXPU::to_plain_if_needed(noise);
  return AtenIpexTypeXPU::rrelu_with_noise_backward(_grad_output, _self, _noise, lower, upper, training, self_is_result);
}
Tensor & rrelu_with_noise_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, c10::optional<Generator> generator) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
  auto _noise = AtenIpexTypeXPU::to_plain_if_needed(noise);
  return AtenIpexTypeXPU::rrelu_with_noise_(self, _noise, lower, upper, training, generator);
}
Tensor & softplus_out_out(Tensor & out, const Tensor & self, Scalar beta, Scalar threshold) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::softplus_out(out, _self, beta, threshold);
}
Tensor softplus(const Tensor & self, Scalar beta, Scalar threshold) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::softplus(_self, beta, threshold);
}
Tensor & softplus_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::softplus_backward_out(grad_input, _grad_output, _self, beta, threshold, _output);
}
Tensor softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::softplus_backward(_grad_output, _self, beta, threshold, _output);
}
Tensor & softshrink_out_out(Tensor & out, const Tensor & self, Scalar lambd) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::softshrink_out(out, _self, lambd);
}
Tensor softshrink(const Tensor & self, Scalar lambd) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::softshrink(_self, lambd);
}
Tensor & softshrink_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar lambd) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::softshrink_backward_out(grad_input, _grad_output, _self, lambd);
}
Tensor softshrink_backward(const Tensor & grad_output, const Tensor & self, Scalar lambd) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::softshrink_backward(_grad_output, _self, lambd);
}
Tensor & adaptive_avg_pool2d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_avg_pool2d_out(out, _self, output_size);
}
Tensor _adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::_adaptive_avg_pool2d(self, output_size);
}
Tensor _adaptive_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::_adaptive_avg_pool2d_backward(grad_output, self);
}
Tensor & adaptive_avg_pool3d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_avg_pool3d_out(out, _self, output_size);
}
Tensor adaptive_avg_pool3d(const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_avg_pool3d(_self, output_size);
}
Tensor & adaptive_avg_pool3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_avg_pool3d_backward_out(grad_input, _grad_output, _self);
}
Tensor adaptive_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_avg_pool3d_backward(_grad_output, _self);
}
std::tuple<Tensor &,Tensor &> adaptive_max_pool2d_out_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::adaptive_max_pool2d_out(out, indices, self, output_size);
}
std::tuple<Tensor,Tensor> adaptive_max_pool2d(const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::adaptive_max_pool2d(self, output_size);
}
Tensor & adaptive_max_pool2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::adaptive_max_pool2d_backward_out(grad_input, grad_output, self, indices);
}
Tensor adaptive_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::adaptive_max_pool2d_backward(grad_output, self, indices);
}
std::tuple<Tensor &,Tensor &> adaptive_max_pool3d_out_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_max_pool3d_out(out, indices, _self, output_size);
}
std::tuple<Tensor,Tensor> adaptive_max_pool3d(const Tensor & self, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::adaptive_max_pool3d(_self, output_size);
}
Tensor & adaptive_max_pool3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::adaptive_max_pool3d_backward_out(grad_input, _grad_output, _self, _indices);
}
Tensor adaptive_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::adaptive_max_pool3d_backward(_grad_output, _self, _indices);
}
Tensor & avg_pool2d_out_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::avg_pool2d_out(out, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor avg_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor & avg_pool2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::avg_pool2d_backward_out(grad_input, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor avg_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor & avg_pool3d_out_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::avg_pool3d_out(out, _self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor avg_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::avg_pool3d(_self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor & avg_pool3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::avg_pool3d_backward_out(grad_input, _grad_output, _self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor avg_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::avg_pool3d_backward(_grad_output, _self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
std::tuple<Tensor &,Tensor &> fractional_max_pool2d_out_output(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) {
  const OptionalDeviceGuard device_guard(device_of(self));
  output = AtenIpexTypeXPU::to_plain_if_needed_(output);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _random_samples = AtenIpexTypeXPU::to_plain_if_needed(random_samples);
  return AtenIpexTypeXPU::fractional_max_pool2d_out(output, indices, _self, kernel_size, output_size, _random_samples);
}
std::tuple<Tensor,Tensor> fractional_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _random_samples = AtenIpexTypeXPU::to_plain_if_needed(random_samples);
  return AtenIpexTypeXPU::fractional_max_pool2d(_self, kernel_size, output_size, _random_samples);
}
Tensor & fractional_max_pool2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::fractional_max_pool2d_backward_out(grad_input, _grad_output, _self, kernel_size, output_size, _indices);
}
Tensor fractional_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::fractional_max_pool2d_backward(_grad_output, _self, kernel_size, output_size, _indices);
}
std::tuple<Tensor &,Tensor &> fractional_max_pool3d_out_output(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) {
  const OptionalDeviceGuard device_guard(device_of(self));
  output = AtenIpexTypeXPU::to_plain_if_needed_(output);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _random_samples = AtenIpexTypeXPU::to_plain_if_needed(random_samples);
  return AtenIpexTypeXPU::fractional_max_pool3d_out(output, indices, _self, kernel_size, output_size, _random_samples);
}
std::tuple<Tensor,Tensor> fractional_max_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _random_samples = AtenIpexTypeXPU::to_plain_if_needed(random_samples);
  return AtenIpexTypeXPU::fractional_max_pool3d(_self, kernel_size, output_size, _random_samples);
}
Tensor & fractional_max_pool3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::fractional_max_pool3d_backward_out(grad_input, _grad_output, _self, kernel_size, output_size, _indices);
}
Tensor fractional_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::fractional_max_pool3d_backward(_grad_output, _self, kernel_size, output_size, _indices);
}
std::tuple<Tensor &,Tensor &> max_pool2d_with_indices_out_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::max_pool2d_with_indices_out(out, indices, self, kernel_size, stride, padding, dilation, ceil_mode);
}
std::tuple<Tensor,Tensor> max_pool2d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor & max_pool2d_with_indices_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::max_pool2d_with_indices_backward_out(grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
Tensor max_pool2d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
std::tuple<Tensor &,Tensor &> max_pool3d_with_indices_out_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  indices = AtenIpexTypeXPU::to_plain_if_needed_(indices);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::max_pool3d_with_indices_out(out, indices, _self, kernel_size, stride, padding, dilation, ceil_mode);
}
std::tuple<Tensor,Tensor> max_pool3d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::max_pool3d_with_indices(_self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor & max_pool3d_with_indices_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_pool3d_with_indices_backward_out(grad_input, _grad_output, _self, kernel_size, stride, padding, dilation, ceil_mode, _indices);
}
Tensor max_pool3d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_pool3d_with_indices_backward(_grad_output, _self, kernel_size, stride, padding, dilation, ceil_mode, _indices);
}
Tensor & max_unpool2d_out_out(Tensor & out, const Tensor & self, const Tensor & indices, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool2d_out(out, _self, _indices, output_size);
}
Tensor max_unpool2d(const Tensor & self, const Tensor & indices, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool2d(_self, _indices, output_size);
}
Tensor & max_unpool2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool2d_backward_out(grad_input, _grad_output, _self, _indices, output_size);
}
Tensor max_unpool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool2d_backward(_grad_output, _self, _indices, output_size);
}
Tensor & max_unpool3d_out_out(Tensor & out, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool3d_out(out, _self, _indices, output_size, stride, padding);
}
Tensor max_unpool3d(const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool3d(_self, _indices, output_size, stride, padding);
}
Tensor & max_unpool3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool3d_backward_out(grad_input, _grad_output, _self, _indices, output_size, stride, padding);
}
Tensor max_unpool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _indices = AtenIpexTypeXPU::to_plain_if_needed(indices);
  return AtenIpexTypeXPU::max_unpool3d_backward(_grad_output, _self, _indices, output_size, stride, padding);
}
Tensor & reflection_pad1d_out_out(Tensor & out, const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::reflection_pad1d_out(out, _self, padding);
}
Tensor reflection_pad1d(const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::reflection_pad1d(_self, padding);
}
Tensor & reflection_pad1d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::reflection_pad1d_backward_out(grad_input, _grad_output, _self, padding);
}
Tensor reflection_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::reflection_pad1d_backward(_grad_output, _self, padding);
}
Tensor & replication_pad2d_out_out(Tensor & out, const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::replication_pad2d_out(out, _self, padding);
}
Tensor replication_pad2d(const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::replication_pad2d(_self, padding);
}
Tensor & replication_pad2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::replication_pad2d_backward_out(grad_input, _grad_output, _self, padding);
}
Tensor replication_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::replication_pad2d_backward(_grad_output, _self, padding);
}
Tensor upsample_linear1d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeXPU::upsample_linear1d(input, output_size, align_corners, scale_factors);
}
Tensor upsample_linear1d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_linear1d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}
Tensor upsample_bilinear2d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeXPU::upsample_bilinear2d(input, output_size, align_corners, scale_factors);
}
Tensor upsample_bilinear2d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}
Tensor upsample_trilinear3d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeXPU::upsample_trilinear3d(input, output_size, align_corners, scale_factors);
}
Tensor upsample_trilinear3d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_trilinear3d_backward(grad_output, output_size, input_size, align_corners, scale_factors);
}
Tensor upsample_bicubic2d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));
  auto _input = AtenIpexTypeXPU::to_plain_if_needed(input);
  return AtenIpexTypeXPU::upsample_bicubic2d(_input, output_size, align_corners, scale_factors);
}
Tensor upsample_bicubic2d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, bool align_corners, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::upsample_bicubic2d_backward(_grad_output, output_size, input_size, align_corners, scale_factors);
}
Tensor upsample_nearest1d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeXPU::upsample_nearest1d(input, output_size, scale_factors);
}
Tensor upsample_nearest1d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_nearest1d_backward(grad_output, output_size, input_size, scale_factors);
}
Tensor upsample_nearest2d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeXPU::upsample_nearest2d(input, output_size, scale_factors);
}
Tensor upsample_nearest2d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_nearest2d_backward(grad_output, output_size, input_size, scale_factors);
}
Tensor upsample_nearest3d_vec(const Tensor & input, c10::optional<IntArrayRef> output_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(input));

  return AtenIpexTypeXPU::upsample_nearest3d(input, output_size, scale_factors);
}
Tensor upsample_nearest3d_backward_vec(const Tensor & grad_output, c10::optional<IntArrayRef> output_size, IntArrayRef input_size, c10::optional<ArrayRef<double>> scale_factors) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_nearest3d_backward(grad_output, output_size, input_size, scale_factors);
}
Tensor & upsample_linear1d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_linear1d_out(out, self, output_size, align_corners, scales);
}
Tensor upsample_linear1d(const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_linear1d(self, output_size, align_corners, scales);
}
Tensor & upsample_linear1d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));

  return AtenIpexTypeXPU::upsample_linear1d_backward_out(grad_input, grad_output, output_size, input_size, align_corners, scales);
}
Tensor upsample_linear1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_linear1d_backward(grad_output, output_size, input_size, align_corners, scales);
}
Tensor & upsample_bilinear2d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_bilinear2d_out(out, self, output_size, align_corners, scales_h, scales_w);
}
Tensor upsample_bilinear2d(const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
}
Tensor & upsample_bilinear2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));

  return AtenIpexTypeXPU::upsample_bilinear2d_backward_out(grad_input, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}
Tensor upsample_bilinear2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}
Tensor & upsample_bicubic2d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::upsample_bicubic2d_out(out, _self, output_size, align_corners, scales_h, scales_w);
}
Tensor upsample_bicubic2d(const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::upsample_bicubic2d(_self, output_size, align_corners, scales_h, scales_w);
}
Tensor & upsample_bicubic2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::upsample_bicubic2d_backward_out(grad_input, _grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}
Tensor upsample_bicubic2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::upsample_bicubic2d_backward(_grad_output, output_size, input_size, align_corners, scales_h, scales_w);
}
Tensor & upsample_trilinear3d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_trilinear3d_out(out, self, output_size, align_corners, scales_d, scales_h, scales_w);
}
Tensor upsample_trilinear3d(const Tensor & self, IntArrayRef output_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_trilinear3d(self, output_size, align_corners, scales_d, scales_h, scales_w);
}
Tensor & upsample_trilinear3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));

  return AtenIpexTypeXPU::upsample_trilinear3d_backward_out(grad_input, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}
Tensor upsample_trilinear3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_trilinear3d_backward(grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
}
Tensor & upsample_nearest1d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_nearest1d_out(out, self, output_size, scales);
}
Tensor upsample_nearest1d(const Tensor & self, IntArrayRef output_size, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_nearest1d(self, output_size, scales);
}
Tensor & upsample_nearest1d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));

  return AtenIpexTypeXPU::upsample_nearest1d_backward_out(grad_input, grad_output, output_size, input_size, scales);
}
Tensor upsample_nearest1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_nearest1d_backward(grad_output, output_size, input_size, scales);
}
Tensor & upsample_nearest2d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_nearest2d_out(out, self, output_size, scales_h, scales_w);
}
Tensor upsample_nearest2d(const Tensor & self, IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_nearest2d(self, output_size, scales_h, scales_w);
}
Tensor & upsample_nearest2d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));

  return AtenIpexTypeXPU::upsample_nearest2d_backward_out(grad_input, grad_output, output_size, input_size, scales_h, scales_w);
}
Tensor upsample_nearest2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_nearest2d_backward(grad_output, output_size, input_size, scales_h, scales_w);
}
Tensor & upsample_nearest3d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_nearest3d_out(out, self, output_size, scales_d, scales_h, scales_w);
}
Tensor upsample_nearest3d(const Tensor & self, IntArrayRef output_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(self));

  return AtenIpexTypeXPU::upsample_nearest3d(self, output_size, scales_d, scales_h, scales_w);
}
Tensor & upsample_nearest3d_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));

  return AtenIpexTypeXPU::upsample_nearest3d_backward_out(grad_input, grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}
Tensor upsample_nearest3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, c10::optional<double> scales_d, c10::optional<double> scales_h, c10::optional<double> scales_w) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));

  return AtenIpexTypeXPU::upsample_nearest3d_backward(grad_output, output_size, input_size, scales_d, scales_h, scales_w);
}
Tensor & sigmoid_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::sigmoid_backward_out(grad_input, _grad_output, _output);
}
Tensor sigmoid_backward(const Tensor & grad_output, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::sigmoid_backward(_grad_output, _output);
}
Tensor & tanh_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::tanh_backward_out(grad_input, _grad_output, _output);
}
Tensor tanh_backward(const Tensor & grad_output, const Tensor & output) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  auto _output = AtenIpexTypeXPU::to_plain_if_needed(output);
  return AtenIpexTypeXPU::tanh_backward(_grad_output, _output);
}
Tensor & col2im_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::col2im_out(out, _self, output_size, kernel_size, dilation, padding, stride);
}
Tensor col2im(const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::col2im(_self, output_size, kernel_size, dilation, padding, stride);
}
Tensor & col2im_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::col2im_backward_out(grad_input, _grad_output, kernel_size, dilation, padding, stride);
}
Tensor col2im_backward(const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::col2im_backward(_grad_output, kernel_size, dilation, padding, stride);
}
Tensor & im2col_out_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::im2col_out(out, _self, kernel_size, dilation, padding, stride);
}
Tensor im2col(const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  return AtenIpexTypeXPU::im2col(_self, kernel_size, dilation, padding, stride);
}
Tensor & im2col_backward_out_grad_input(Tensor & grad_input, const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(grad_input));
  grad_input = AtenIpexTypeXPU::to_plain_if_needed_(grad_input);
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::im2col_backward_out(grad_input, _grad_output, input_size, kernel_size, dilation, padding, stride);
}
Tensor im2col_backward(const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) {
  const OptionalDeviceGuard device_guard(device_of(grad_output));
  auto _grad_output = AtenIpexTypeXPU::to_plain_if_needed(grad_output);
  return AtenIpexTypeXPU::im2col_backward(_grad_output, input_size, kernel_size, dilation, padding, stride);
}
void record_stream(Tensor & self, Stream s) {
  const OptionalDeviceGuard device_guard(device_of(self));
  self = AtenIpexTypeXPU::to_plain_if_needed_(self);
 AtenIpexTypeXPU::record_stream(self, s);
}
Tensor ger(const Tensor & self, const Tensor & vec2) {
  const OptionalDeviceGuard device_guard(device_of(self));
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _vec2 = AtenIpexTypeXPU::to_plain_if_needed(vec2);
  return AtenIpexTypeXPU::ger(_self, _vec2);
}
Tensor & ger_out_out(Tensor & out, const Tensor & self, const Tensor & vec2) {
  const OptionalDeviceGuard device_guard(device_of(self));
  out = AtenIpexTypeXPU::to_plain_if_needed_(out);
  auto _self = AtenIpexTypeXPU::to_plain_if_needed(self);
  auto _vec2 = AtenIpexTypeXPU::to_plain_if_needed(vec2);
  return AtenIpexTypeXPU::ger_out(out, _self, _vec2);
}
#endif

} // namespace AtenIpexTypeXPU_impl

namespace AtenIpexTypeXPU {

#if 0
TORCH_LIBRARY_IMPL(aten, XPU, m) {
  m.impl("_fused_dropout",
  TORCH_FN(AtenIpexTypeXPU_impl::_fused_dropout)
  );
  m.impl("_masked_scale",
  TORCH_FN(AtenIpexTypeXPU_impl::_masked_scale)
  );
  m.impl("abs.out",
  TORCH_FN(AtenIpexTypeXPU_impl::abs_out_out)
  );
  m.impl("sgn.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sgn_out_out)
  );
  m.impl("conj.out",
  TORCH_FN(AtenIpexTypeXPU_impl::conj_out_out)
  );
  m.impl("acos.out",
  TORCH_FN(AtenIpexTypeXPU_impl::acos_out_out)
  );
  m.impl("add.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::add_Tensor)
  );
  m.impl("add_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::add__Tensor)
  );
  m.impl("add.out",
  TORCH_FN(AtenIpexTypeXPU_impl::add_out_out)
  );
  m.impl("add.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::add_Scalar)
  );
  m.impl("add_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::add__Scalar)
  );
  m.impl("addmv",
  TORCH_FN(AtenIpexTypeXPU_impl::addmv)
  );
  m.impl("addmv_",
  TORCH_FN(AtenIpexTypeXPU_impl::addmv_)
  );
  m.impl("addr",
  TORCH_FN(AtenIpexTypeXPU_impl::addr)
  );
  m.impl("addr_",
  TORCH_FN(AtenIpexTypeXPU_impl::addr_)
  );
  m.impl("all.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::all_dim)
  );
  m.impl("all.out",
  TORCH_FN(AtenIpexTypeXPU_impl::all_out_out)
  );
  m.impl("any.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::any_dim)
  );
  m.impl("any.out",
  TORCH_FN(AtenIpexTypeXPU_impl::any_out_out)
  );
  m.impl("arange.start_out",
  TORCH_FN(AtenIpexTypeXPU_impl::arange_out_start_out)
  );
  m.impl("argmax",
  TORCH_FN(AtenIpexTypeXPU_impl::argmax)
  );
  m.impl("argmin",
  TORCH_FN(AtenIpexTypeXPU_impl::argmin)
  );
  m.impl("as_strided",
  TORCH_FN(AtenIpexTypeXPU_impl::as_strided)
  );
  m.impl("asin.out",
  TORCH_FN(AtenIpexTypeXPU_impl::asin_out_out)
  );
  m.impl("atan_",
  TORCH_FN(AtenIpexTypeXPU_impl::atan_)
  );
  m.impl("atan.out",
  TORCH_FN(AtenIpexTypeXPU_impl::atan_out_out)
  );
  m.impl("baddbmm",
  TORCH_FN(AtenIpexTypeXPU_impl::baddbmm)
  );
  m.impl("baddbmm_",
  TORCH_FN(AtenIpexTypeXPU_impl::baddbmm_)
  );
  m.impl("baddbmm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::baddbmm_out_out)
  );
  m.impl("bernoulli_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::bernoulli__Tensor)
  );
  m.impl("bernoulli_.float",
  TORCH_FN(AtenIpexTypeXPU_impl::bernoulli__float)
  );
  m.impl("binary_cross_entropy",
  TORCH_FN(AtenIpexTypeXPU_impl::binary_cross_entropy)
  );
  m.impl("binary_cross_entropy.out",
  TORCH_FN(AtenIpexTypeXPU_impl::binary_cross_entropy_out_out)
  );
  m.impl("binary_cross_entropy_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::binary_cross_entropy_backward)
  );
  m.impl("binary_cross_entropy_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::binary_cross_entropy_backward_out_grad_input)
  );
  m.impl("bincount",
  TORCH_FN(AtenIpexTypeXPU_impl::bincount)
  );
  m.impl("bitwise_not",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_not)
  );
  m.impl("bitwise_not_",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_not_)
  );
  m.impl("bitwise_not.out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_not_out_out)
  );
  m.impl("logical_not",
  TORCH_FN(AtenIpexTypeXPU_impl::logical_not)
  );
  m.impl("logical_not_",
  TORCH_FN(AtenIpexTypeXPU_impl::logical_not_)
  );
  m.impl("logical_not.out",
  TORCH_FN(AtenIpexTypeXPU_impl::logical_not_out_out)
  );
  m.impl("logical_xor.out",
  TORCH_FN(AtenIpexTypeXPU_impl::logical_xor_out_out)
  );
  m.impl("logical_and.out",
  TORCH_FN(AtenIpexTypeXPU_impl::logical_and_out_out)
  );
  m.impl("logical_or.out",
  TORCH_FN(AtenIpexTypeXPU_impl::logical_or_out_out)
  );
  m.impl("bmm",
  TORCH_FN(AtenIpexTypeXPU_impl::bmm)
  );
  m.impl("bmm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::bmm_out_out)
  );
  m.impl("ceil.out",
  TORCH_FN(AtenIpexTypeXPU_impl::ceil_out_out)
  );
  m.impl("chain_matmul",
  TORCH_FN(AtenIpexTypeXPU_impl::chain_matmul)
  );
  m.impl("clamp",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp)
  );
  m.impl("clamp_",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp_)
  );
  m.impl("clamp.out",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp_out_out)
  );
  m.impl("clamp_max_",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp_max_)
  );
  m.impl("clamp_max.out",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp_max_out_out)
  );
  m.impl("clamp_min_",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp_min_)
  );
  m.impl("clamp_min.out",
  TORCH_FN(AtenIpexTypeXPU_impl::clamp_min_out_out)
  );
  m.impl("convolution_overrideable",
  TORCH_FN(AtenIpexTypeXPU_impl::convolution_overrideable)
  );
  m.impl("convolution_backward_overrideable",
  TORCH_FN(AtenIpexTypeXPU_impl::convolution_backward_overrideable)
  );
  m.impl("conv_tbc",
  TORCH_FN(AtenIpexTypeXPU_impl::conv_tbc)
  );
  m.impl("conv_tbc_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::conv_tbc_backward)
  );
  m.impl("copy_",
  TORCH_FN(AtenIpexTypeXPU_impl::copy_)
  );
  m.impl("cos_",
  TORCH_FN(AtenIpexTypeXPU_impl::cos_)
  );
  m.impl("cos.out",
  TORCH_FN(AtenIpexTypeXPU_impl::cos_out_out)
  );
  m.impl("cosh.out",
  TORCH_FN(AtenIpexTypeXPU_impl::cosh_out_out)
  );
  m.impl("_ctc_loss",
  TORCH_FN(AtenIpexTypeXPU_impl::_ctc_loss)
  );
  m.impl("_ctc_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::_ctc_loss_backward)
  );
  m.impl("div.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::div_Tensor)
  );
  m.impl("div_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::div__Tensor)
  );
  m.impl("div.out",
  TORCH_FN(AtenIpexTypeXPU_impl::div_out_out)
  );
  m.impl("dot",
  TORCH_FN(AtenIpexTypeXPU_impl::dot)
  );
  m.impl("embedding_dense_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::embedding_dense_backward)
  );
  m.impl("_embedding_bag",
  TORCH_FN(AtenIpexTypeXPU_impl::_embedding_bag)
  );
  m.impl("_embedding_bag_dense_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::_embedding_bag_dense_backward)
  );
  m.impl("empty.memory_format",
  TORCH_FN(AtenIpexTypeXPU_impl::empty_memory_format)
  );
  m.impl("_empty_affine_quantized",
  TORCH_FN(AtenIpexTypeXPU_impl::_empty_affine_quantized)
  );
  m.impl("_empty_per_channel_affine_quantized",
  TORCH_FN(AtenIpexTypeXPU_impl::_empty_per_channel_affine_quantized)
  );
  m.impl("resize_",
  TORCH_FN(AtenIpexTypeXPU_impl::resize_)
  );
  m.impl("empty_strided",
  TORCH_FN(AtenIpexTypeXPU_impl::empty_strided)
  );
  m.impl("erf_",
  TORCH_FN(AtenIpexTypeXPU_impl::erf_)
  );
  m.impl("erf.out",
  TORCH_FN(AtenIpexTypeXPU_impl::erf_out_out)
  );
  m.impl("erfc_",
  TORCH_FN(AtenIpexTypeXPU_impl::erfc_)
  );
  m.impl("erfc.out",
  TORCH_FN(AtenIpexTypeXPU_impl::erfc_out_out)
  );
  m.impl("exp_",
  TORCH_FN(AtenIpexTypeXPU_impl::exp_)
  );
  m.impl("exp.out",
  TORCH_FN(AtenIpexTypeXPU_impl::exp_out_out)
  );
  m.impl("expm1.out",
  TORCH_FN(AtenIpexTypeXPU_impl::expm1_out_out)
  );
  m.impl("eye.out",
  TORCH_FN(AtenIpexTypeXPU_impl::eye_out_out)
  );
  m.impl("eye.m_out",
  TORCH_FN(AtenIpexTypeXPU_impl::eye_out_m_out)
  );
  m.impl("fill_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::fill__Scalar)
  );
  m.impl("fill_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::fill__Tensor)
  );
  m.impl("floor.out",
  TORCH_FN(AtenIpexTypeXPU_impl::floor_out_out)
  );
  m.impl("floor_divide",
  TORCH_FN(AtenIpexTypeXPU_impl::floor_divide)
  );
  m.impl("floor_divide_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::floor_divide__Tensor)
  );
  m.impl("floor_divide.out",
  TORCH_FN(AtenIpexTypeXPU_impl::floor_divide_out_out)
  );
  m.impl("frac",
  TORCH_FN(AtenIpexTypeXPU_impl::frac)
  );
  m.impl("frac_",
  TORCH_FN(AtenIpexTypeXPU_impl::frac_)
  );
  m.impl("frac.out",
  TORCH_FN(AtenIpexTypeXPU_impl::frac_out_out)
  );
  m.impl("native_group_norm",
  TORCH_FN(AtenIpexTypeXPU_impl::native_group_norm)
  );
  m.impl("native_group_norm_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::native_group_norm_backward)
  );
  m.impl("_fft_with_size.norm_modes",
  TORCH_FN(AtenIpexTypeXPU_impl::_fft_with_size_norm_modes)
  );
  m.impl("index.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::index_Tensor)
  );
  m.impl("index_copy_",
  TORCH_FN(AtenIpexTypeXPU_impl::index_copy_)
  );
  m.impl("_index_put_impl_",
  TORCH_FN(AtenIpexTypeXPU_impl::_index_put_impl_)
  );
  m.impl("inverse",
  TORCH_FN(AtenIpexTypeXPU_impl::inverse)
  );
  m.impl("inverse.out",
  TORCH_FN(AtenIpexTypeXPU_impl::inverse_out_out)
  );
  m.impl("_inverse_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_inverse_helper)
  );
  m.impl("isnan",
  TORCH_FN(AtenIpexTypeXPU_impl::isnan)
  );
  m.impl("kthvalue.values",
  TORCH_FN(AtenIpexTypeXPU_impl::kthvalue_out_values)
  );
  m.impl("native_layer_norm",
  TORCH_FN(AtenIpexTypeXPU_impl::native_layer_norm)
  );
  m.impl("native_layer_norm_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::native_layer_norm_backward)
  );
  m.impl("linspace.out",
  TORCH_FN(AtenIpexTypeXPU_impl::linspace_out_out)
  );
  m.impl("log.out",
  TORCH_FN(AtenIpexTypeXPU_impl::log_out_out)
  );
  m.impl("log10.out",
  TORCH_FN(AtenIpexTypeXPU_impl::log10_out_out)
  );
  m.impl("log1p_",
  TORCH_FN(AtenIpexTypeXPU_impl::log1p_)
  );
  m.impl("log1p.out",
  TORCH_FN(AtenIpexTypeXPU_impl::log1p_out_out)
  );
  m.impl("log2.out",
  TORCH_FN(AtenIpexTypeXPU_impl::log2_out_out)
  );
  m.impl("logspace.out",
  TORCH_FN(AtenIpexTypeXPU_impl::logspace_out_out)
  );
  m.impl("_log_softmax",
  TORCH_FN(AtenIpexTypeXPU_impl::_log_softmax)
  );
  m.impl("_log_softmax_backward_data",
  TORCH_FN(AtenIpexTypeXPU_impl::_log_softmax_backward_data)
  );
  m.impl("_aminmax",
  TORCH_FN(AtenIpexTypeXPU_impl::_aminmax)
  );
  m.impl("_aminmax.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::_aminmax_dim)
  );
  m.impl("max.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::max_dim)
  );
  m.impl("max.dim_max",
  TORCH_FN(AtenIpexTypeXPU_impl::max_out_dim_max)
  );
  m.impl("amax",
  TORCH_FN(AtenIpexTypeXPU_impl::amax)
  );
  m.impl("amax.out",
  TORCH_FN(AtenIpexTypeXPU_impl::amax_out_out)
  );
  m.impl("mean",
  TORCH_FN(AtenIpexTypeXPU_impl::mean)
  );
  m.impl("mean.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::mean_dim)
  );
  m.impl("mean.out",
  TORCH_FN(AtenIpexTypeXPU_impl::mean_out_out)
  );
  m.impl("min.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::min_dim)
  );
  m.impl("min.dim_min",
  TORCH_FN(AtenIpexTypeXPU_impl::min_out_dim_min)
  );
  m.impl("mm",
  TORCH_FN(AtenIpexTypeXPU_impl::mm)
  );
  m.impl("mm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::mm_out_out)
  );
  m.impl("mode",
  TORCH_FN(AtenIpexTypeXPU_impl::mode)
  );
  m.impl("mode.values",
  TORCH_FN(AtenIpexTypeXPU_impl::mode_out_values)
  );
  m.impl("mul.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::mul_Tensor)
  );
  m.impl("mul_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::mul__Tensor)
  );
  m.impl("mul.out",
  TORCH_FN(AtenIpexTypeXPU_impl::mul_out_out)
  );
  m.impl("mul.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::mul_Scalar)
  );
  m.impl("mul_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::mul__Scalar)
  );
  m.impl("mv",
  TORCH_FN(AtenIpexTypeXPU_impl::mv)
  );
  m.impl("mvlgamma",
  TORCH_FN(AtenIpexTypeXPU_impl::mvlgamma)
  );
  m.impl("mvlgamma_",
  TORCH_FN(AtenIpexTypeXPU_impl::mvlgamma_)
  );
  m.impl("narrow_copy",
  TORCH_FN(AtenIpexTypeXPU_impl::narrow_copy)
  );
  m.impl("native_batch_norm",
  TORCH_FN(AtenIpexTypeXPU_impl::native_batch_norm)
  );
  m.impl("native_batch_norm_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::native_batch_norm_backward)
  );
  m.impl("_cdist_forward",
  TORCH_FN(AtenIpexTypeXPU_impl::_cdist_forward)
  );
  m.impl("_cdist_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::_cdist_backward)
  );
  m.impl("_pdist_forward",
  TORCH_FN(AtenIpexTypeXPU_impl::_pdist_forward)
  );
  m.impl("_pdist_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::_pdist_backward)
  );
  m.impl("randperm",
  TORCH_FN(AtenIpexTypeXPU_impl::randperm)
  );
  m.impl("randperm.generator",
  TORCH_FN(AtenIpexTypeXPU_impl::randperm_generator)
  );
  m.impl("randperm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::randperm_out_out)
  );
  m.impl("randperm.generator_out",
  TORCH_FN(AtenIpexTypeXPU_impl::randperm_out_generator_out)
  );
  m.impl("range.out",
  TORCH_FN(AtenIpexTypeXPU_impl::range_out_out)
  );
  m.impl("reciprocal.out",
  TORCH_FN(AtenIpexTypeXPU_impl::reciprocal_out_out)
  );
  m.impl("neg.out",
  TORCH_FN(AtenIpexTypeXPU_impl::neg_out_out)
  );
  m.impl("round.out",
  TORCH_FN(AtenIpexTypeXPU_impl::round_out_out)
  );
  m.impl("rrelu",
  TORCH_FN(AtenIpexTypeXPU_impl::rrelu)
  );
  m.impl("rrelu_",
  TORCH_FN(AtenIpexTypeXPU_impl::rrelu_)
  );
  m.impl("relu",
  TORCH_FN(AtenIpexTypeXPU_impl::relu)
  );
  m.impl("relu_",
  TORCH_FN(AtenIpexTypeXPU_impl::relu_)
  );
  m.impl("prelu",
  TORCH_FN(AtenIpexTypeXPU_impl::prelu)
  );
  m.impl("prelu_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::prelu_backward)
  );
  m.impl("gelu",
  TORCH_FN(AtenIpexTypeXPU_impl::gelu)
  );
  m.impl("gelu_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::gelu_backward)
  );
  m.impl("hardshrink",
  TORCH_FN(AtenIpexTypeXPU_impl::hardshrink)
  );
  m.impl("hardshrink_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::hardshrink_backward)
  );
  m.impl("rsqrt.out",
  TORCH_FN(AtenIpexTypeXPU_impl::rsqrt_out_out)
  );
  m.impl("sigmoid",
  TORCH_FN(AtenIpexTypeXPU_impl::sigmoid)
  );
  m.impl("sigmoid_",
  TORCH_FN(AtenIpexTypeXPU_impl::sigmoid_)
  );
  m.impl("sigmoid.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sigmoid_out_out)
  );
  m.impl("sin.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sin_out_out)
  );
  m.impl("sinh.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sinh_out_out)
  );
  m.impl("slice.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::slice_Tensor)
  );
  m.impl("_softmax",
  TORCH_FN(AtenIpexTypeXPU_impl::_softmax)
  );
  m.impl("_softmax_backward_data",
  TORCH_FN(AtenIpexTypeXPU_impl::_softmax_backward_data)
  );
  m.impl("sum",
  TORCH_FN(AtenIpexTypeXPU_impl::sum)
  );
  m.impl("sum.dim_IntList",
  TORCH_FN(AtenIpexTypeXPU_impl::sum_dim_IntList)
  );
  m.impl("sum.IntList_out",
  TORCH_FN(AtenIpexTypeXPU_impl::sum_out_IntList_out)
  );
  m.impl("sqrt.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sqrt_out_out)
  );
  m.impl("std",
  TORCH_FN(AtenIpexTypeXPU_impl::std)
  );
  m.impl("std.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::std_dim)
  );
  m.impl("std_mean",
  TORCH_FN(AtenIpexTypeXPU_impl::std_mean)
  );
  m.impl("std_mean.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::std_mean_dim)
  );
  m.impl("std.out",
  TORCH_FN(AtenIpexTypeXPU_impl::std_out_out)
  );
  m.impl("prod",
  TORCH_FN(AtenIpexTypeXPU_impl::prod)
  );
  m.impl("prod.dim_int",
  TORCH_FN(AtenIpexTypeXPU_impl::prod_dim_int)
  );
  m.impl("prod.int_out",
  TORCH_FN(AtenIpexTypeXPU_impl::prod_out_int_out)
  );
  m.impl("tan_",
  TORCH_FN(AtenIpexTypeXPU_impl::tan_)
  );
  m.impl("tan.out",
  TORCH_FN(AtenIpexTypeXPU_impl::tan_out_out)
  );
  m.impl("tanh",
  TORCH_FN(AtenIpexTypeXPU_impl::tanh)
  );
  m.impl("tanh_",
  TORCH_FN(AtenIpexTypeXPU_impl::tanh_)
  );
  m.impl("tanh.out",
  TORCH_FN(AtenIpexTypeXPU_impl::tanh_out_out)
  );
  m.impl("threshold",
  TORCH_FN(AtenIpexTypeXPU_impl::threshold)
  );
  m.impl("threshold_",
  TORCH_FN(AtenIpexTypeXPU_impl::threshold_)
  );
  m.impl("threshold.out",
  TORCH_FN(AtenIpexTypeXPU_impl::threshold_out_out)
  );
  m.impl("threshold_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::threshold_backward)
  );
  m.impl("flip",
  TORCH_FN(AtenIpexTypeXPU_impl::flip)
  );
  m.impl("roll",
  TORCH_FN(AtenIpexTypeXPU_impl::roll)
  );
  m.impl("rot90",
  TORCH_FN(AtenIpexTypeXPU_impl::rot90)
  );
  m.impl("trunc.out",
  TORCH_FN(AtenIpexTypeXPU_impl::trunc_out_out)
  );
  m.impl("_unique",
  TORCH_FN(AtenIpexTypeXPU_impl::_unique)
  );
  m.impl("unique_dim",
  TORCH_FN(AtenIpexTypeXPU_impl::unique_dim)
  );
  m.impl("unique_consecutive",
  TORCH_FN(AtenIpexTypeXPU_impl::unique_consecutive)
  );
  m.impl("unique_dim_consecutive",
  TORCH_FN(AtenIpexTypeXPU_impl::unique_dim_consecutive)
  );
  m.impl("_unique2",
  TORCH_FN(AtenIpexTypeXPU_impl::_unique2)
  );
  m.impl("var",
  TORCH_FN(AtenIpexTypeXPU_impl::var)
  );
  m.impl("var.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::var_dim)
  );
  m.impl("var_mean",
  TORCH_FN(AtenIpexTypeXPU_impl::var_mean)
  );
  m.impl("var_mean.dim",
  TORCH_FN(AtenIpexTypeXPU_impl::var_mean_dim)
  );
  m.impl("_s_where",
  TORCH_FN(AtenIpexTypeXPU_impl::_s_where)
  );
  m.impl("norm.ScalarOpt_dtype",
  TORCH_FN(AtenIpexTypeXPU_impl::norm_ScalarOpt_dtype)
  );
  m.impl("norm.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::norm_Scalar)
  );
  m.impl("norm.ScalarOpt_dim_dtype",
  TORCH_FN(AtenIpexTypeXPU_impl::norm_ScalarOpt_dim_dtype)
  );
  m.impl("norm.ScalarOpt_dim",
  TORCH_FN(AtenIpexTypeXPU_impl::norm_ScalarOpt_dim)
  );
  m.impl("norm.dtype_out",
  TORCH_FN(AtenIpexTypeXPU_impl::norm_out_dtype_out)
  );
  m.impl("norm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::norm_out_out)
  );
  m.impl("clone",
  TORCH_FN(AtenIpexTypeXPU_impl::clone)
  );
  m.impl("resize_as_",
  TORCH_FN(AtenIpexTypeXPU_impl::resize_as_)
  );
  m.impl("zero_",
  TORCH_FN(AtenIpexTypeXPU_impl::zero_)
  );
  m.impl("sub.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sub_out_out)
  );
  m.impl("sub.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::sub_Tensor)
  );
  m.impl("sub_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::sub__Tensor)
  );
  m.impl("sub.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::sub_Scalar)
  );
  m.impl("sub_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::sub__Scalar)
  );
  m.impl("rsub.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::rsub_Tensor)
  );
  m.impl("rsub.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::rsub_Scalar)
  );
  m.impl("addmm",
  TORCH_FN(AtenIpexTypeXPU_impl::addmm)
  );
  m.impl("addmm_",
  TORCH_FN(AtenIpexTypeXPU_impl::addmm_)
  );
  m.impl("quantize_per_tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::quantize_per_tensor)
  );
  m.impl("quantize_per_channel",
  TORCH_FN(AtenIpexTypeXPU_impl::quantize_per_channel)
  );
  m.impl("dequantize.self",
  TORCH_FN(AtenIpexTypeXPU_impl::dequantize_self)
  );
  m.impl("_make_per_tensor_quantized_tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::_make_per_tensor_quantized_tensor)
  );
  m.impl("_make_per_channel_quantized_tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::_make_per_channel_quantized_tensor)
  );
  m.impl("_local_scalar_dense",
  TORCH_FN(AtenIpexTypeXPU_impl::_local_scalar_dense)
  );
  m.impl("set_.source_Storage",
  TORCH_FN(AtenIpexTypeXPU_impl::set__source_Storage)
  );
  m.impl("set_.source_Storage_storage_offset",
  TORCH_FN(AtenIpexTypeXPU_impl::set__source_Storage_storage_offset)
  );
  m.impl("set_.source_Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::set__source_Tensor)
  );
  m.impl("set_",
  TORCH_FN(AtenIpexTypeXPU_impl::set_)
  );
  m.impl("is_set_to",
  TORCH_FN(AtenIpexTypeXPU_impl::is_set_to)
  );
  m.impl("masked_fill_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::masked_fill__Scalar)
  );
  m.impl("masked_fill_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::masked_fill__Tensor)
  );
  m.impl("masked_scatter_",
  TORCH_FN(AtenIpexTypeXPU_impl::masked_scatter_)
  );
  m.impl("view",
  TORCH_FN(AtenIpexTypeXPU_impl::view)
  );
  m.impl("put_",
  TORCH_FN(AtenIpexTypeXPU_impl::put_)
  );
  m.impl("index_add_",
  TORCH_FN(AtenIpexTypeXPU_impl::index_add_)
  );
  m.impl("index_fill_.int_Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::index_fill__int_Scalar)
  );
  m.impl("index_fill_.int_Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::index_fill__int_Tensor)
  );
  m.impl("scatter_.src",
  TORCH_FN(AtenIpexTypeXPU_impl::scatter__src)
  );
  m.impl("scatter_.value",
  TORCH_FN(AtenIpexTypeXPU_impl::scatter__value)
  );
  m.impl("scatter_add_",
  TORCH_FN(AtenIpexTypeXPU_impl::scatter_add_)
  );
  m.impl("bitwise_and.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_and_out_Tensor_out)
  );
  m.impl("bitwise_and.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_and_out_Scalar_out)
  );
  m.impl("bitwise_or.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_or_out_Tensor_out)
  );
  m.impl("bitwise_or.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_or_out_Scalar_out)
  );
  m.impl("bitwise_xor.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_xor_out_Tensor_out)
  );
  m.impl("bitwise_xor.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::bitwise_xor_out_Scalar_out)
  );
  m.impl("lgamma_",
  TORCH_FN(AtenIpexTypeXPU_impl::lgamma_)
  );
  m.impl("atan2_",
  TORCH_FN(AtenIpexTypeXPU_impl::atan2_)
  );
  m.impl("tril_",
  TORCH_FN(AtenIpexTypeXPU_impl::tril_)
  );
  m.impl("triu_",
  TORCH_FN(AtenIpexTypeXPU_impl::triu_)
  );
  m.impl("digamma_",
  TORCH_FN(AtenIpexTypeXPU_impl::digamma_)
  );
  m.impl("polygamma_",
  TORCH_FN(AtenIpexTypeXPU_impl::polygamma_)
  );
  m.impl("renorm_",
  TORCH_FN(AtenIpexTypeXPU_impl::renorm_)
  );
  m.impl("pow_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::pow__Scalar)
  );
  m.impl("pow_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::pow__Tensor)
  );
  m.impl("lerp_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::lerp__Scalar)
  );
  m.impl("lerp_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::lerp__Tensor)
  );
  m.impl("fmod_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::fmod__Scalar)
  );
  m.impl("fmod_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::fmod__Tensor)
  );
  m.impl("remainder_.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::remainder__Scalar)
  );
  m.impl("remainder_.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::remainder__Tensor)
  );
  m.impl("addbmm_",
  TORCH_FN(AtenIpexTypeXPU_impl::addbmm_)
  );
  m.impl("addbmm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::addbmm_out_out)
  );
  m.impl("addbmm",
  TORCH_FN(AtenIpexTypeXPU_impl::addbmm)
  );
  m.impl("addcdiv_",
  TORCH_FN(AtenIpexTypeXPU_impl::addcdiv_)
  );
  m.impl("random_.from",
  TORCH_FN(AtenIpexTypeXPU_impl::random__from)
  );
  m.impl("random_.to",
  TORCH_FN(AtenIpexTypeXPU_impl::random__to)
  );
  m.impl("random_",
  TORCH_FN(AtenIpexTypeXPU_impl::random_)
  );
  m.impl("uniform_",
  TORCH_FN(AtenIpexTypeXPU_impl::uniform_)
  );
  m.impl("cauchy_",
  TORCH_FN(AtenIpexTypeXPU_impl::cauchy_)
  );
  m.impl("log_normal_",
  TORCH_FN(AtenIpexTypeXPU_impl::log_normal_)
  );
  m.impl("exponential_",
  TORCH_FN(AtenIpexTypeXPU_impl::exponential_)
  );
  m.impl("geometric_",
  TORCH_FN(AtenIpexTypeXPU_impl::geometric_)
  );
  m.impl("diag.out",
  TORCH_FN(AtenIpexTypeXPU_impl::diag_out_out)
  );
  m.impl("diag",
  TORCH_FN(AtenIpexTypeXPU_impl::diag)
  );
  m.impl("cross.out",
  TORCH_FN(AtenIpexTypeXPU_impl::cross_out_out)
  );
  m.impl("cross",
  TORCH_FN(AtenIpexTypeXPU_impl::cross)
  );
  m.impl("triu.out",
  TORCH_FN(AtenIpexTypeXPU_impl::triu_out_out)
  );
  m.impl("tril.out",
  TORCH_FN(AtenIpexTypeXPU_impl::tril_out_out)
  );
  m.impl("tril_indices",
  TORCH_FN(AtenIpexTypeXPU_impl::tril_indices)
  );
  m.impl("triu_indices",
  TORCH_FN(AtenIpexTypeXPU_impl::triu_indices)
  );
  m.impl("trace",
  TORCH_FN(AtenIpexTypeXPU_impl::trace)
  );
  m.impl("ne.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::ne_out_Scalar_out)
  );
  m.impl("ne.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::ne_Scalar)
  );
  m.impl("ne.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::ne_out_Tensor_out)
  );
  m.impl("ne.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::ne_Tensor)
  );
  m.impl("eq.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::eq_out_Scalar_out)
  );
  m.impl("eq.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::eq_Scalar)
  );
  m.impl("eq.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::eq_out_Tensor_out)
  );
  m.impl("eq.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::eq_Tensor)
  );
  m.impl("ge.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::ge_out_Scalar_out)
  );
  m.impl("ge.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::ge_Scalar)
  );
  m.impl("ge.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::ge_out_Tensor_out)
  );
  m.impl("ge.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::ge_Tensor)
  );
  m.impl("le.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::le_out_Scalar_out)
  );
  m.impl("le.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::le_Scalar)
  );
  m.impl("le.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::le_out_Tensor_out)
  );
  m.impl("le.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::le_Tensor)
  );
  m.impl("gt.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::gt_out_Scalar_out)
  );
  m.impl("gt.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::gt_Scalar)
  );
  m.impl("gt.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::gt_out_Tensor_out)
  );
  m.impl("gt.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::gt_Tensor)
  );
  m.impl("lt.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::lt_out_Scalar_out)
  );
  m.impl("lt.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::lt_Scalar)
  );
  m.impl("lt.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::lt_out_Tensor_out)
  );
  m.impl("lt.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::lt_Tensor)
  );
  m.impl("take.out",
  TORCH_FN(AtenIpexTypeXPU_impl::take_out_out)
  );
  m.impl("take",
  TORCH_FN(AtenIpexTypeXPU_impl::take)
  );
  m.impl("index_select.out",
  TORCH_FN(AtenIpexTypeXPU_impl::index_select_out_out)
  );
  m.impl("index_select",
  TORCH_FN(AtenIpexTypeXPU_impl::index_select)
  );
  m.impl("masked_select.out",
  TORCH_FN(AtenIpexTypeXPU_impl::masked_select_out_out)
  );
  m.impl("masked_select",
  TORCH_FN(AtenIpexTypeXPU_impl::masked_select)
  );
  m.impl("nonzero.out",
  TORCH_FN(AtenIpexTypeXPU_impl::nonzero_out_out)
  );
  m.impl("nonzero",
  TORCH_FN(AtenIpexTypeXPU_impl::nonzero)
  );
  m.impl("gather.out",
  TORCH_FN(AtenIpexTypeXPU_impl::gather_out_out)
  );
  m.impl("gather",
  TORCH_FN(AtenIpexTypeXPU_impl::gather)
  );
  m.impl("addcmul.out",
  TORCH_FN(AtenIpexTypeXPU_impl::addcmul_out_out)
  );
  m.impl("addcmul",
  TORCH_FN(AtenIpexTypeXPU_impl::addcmul)
  );
  m.impl("addcmul_",
  TORCH_FN(AtenIpexTypeXPU_impl::addcmul_)
  );
  m.impl("addcdiv.out",
  TORCH_FN(AtenIpexTypeXPU_impl::addcdiv_out_out)
  );
  m.impl("addcdiv",
  TORCH_FN(AtenIpexTypeXPU_impl::addcdiv)
  );
  m.impl("triangular_solve.X",
  TORCH_FN(AtenIpexTypeXPU_impl::triangular_solve_out_X)
  );
  m.impl("triangular_solve",
  TORCH_FN(AtenIpexTypeXPU_impl::triangular_solve)
  );
  m.impl("_triangular_solve_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_triangular_solve_helper)
  );
  m.impl("_symeig_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_symeig_helper)
  );
  m.impl("svd.U",
  TORCH_FN(AtenIpexTypeXPU_impl::svd_out_U)
  );
  m.impl("svd",
  TORCH_FN(AtenIpexTypeXPU_impl::svd)
  );
  m.impl("_svd_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_svd_helper)
  );
  m.impl("cholesky.out",
  TORCH_FN(AtenIpexTypeXPU_impl::cholesky_out_out)
  );
  m.impl("cholesky",
  TORCH_FN(AtenIpexTypeXPU_impl::cholesky)
  );
  m.impl("cholesky_solve.out",
  TORCH_FN(AtenIpexTypeXPU_impl::cholesky_solve_out_out)
  );
  m.impl("cholesky_solve",
  TORCH_FN(AtenIpexTypeXPU_impl::cholesky_solve)
  );
  m.impl("solve",
  TORCH_FN(AtenIpexTypeXPU_impl::solve)
  );
  m.impl("solve.solution",
  TORCH_FN(AtenIpexTypeXPU_impl::solve_out_solution)
  );
  m.impl("_solve_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_solve_helper)
  );
  m.impl("cholesky_inverse.out",
  TORCH_FN(AtenIpexTypeXPU_impl::cholesky_inverse_out_out)
  );
  m.impl("cholesky_inverse",
  TORCH_FN(AtenIpexTypeXPU_impl::cholesky_inverse)
  );
  m.impl("qr.Q",
  TORCH_FN(AtenIpexTypeXPU_impl::qr_out_Q)
  );
  m.impl("qr",
  TORCH_FN(AtenIpexTypeXPU_impl::qr)
  );
  m.impl("_qr_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_qr_helper)
  );
  m.impl("geqrf.a",
  TORCH_FN(AtenIpexTypeXPU_impl::geqrf_out_a)
  );
  m.impl("geqrf",
  TORCH_FN(AtenIpexTypeXPU_impl::geqrf)
  );
  m.impl("orgqr.out",
  TORCH_FN(AtenIpexTypeXPU_impl::orgqr_out_out)
  );
  m.impl("orgqr",
  TORCH_FN(AtenIpexTypeXPU_impl::orgqr)
  );
  m.impl("ormqr.out",
  TORCH_FN(AtenIpexTypeXPU_impl::ormqr_out_out)
  );
  m.impl("ormqr",
  TORCH_FN(AtenIpexTypeXPU_impl::ormqr)
  );
  m.impl("_lu_with_info",
  TORCH_FN(AtenIpexTypeXPU_impl::_lu_with_info)
  );
  m.impl("lu_solve.out",
  TORCH_FN(AtenIpexTypeXPU_impl::lu_solve_out_out)
  );
  m.impl("lu_solve",
  TORCH_FN(AtenIpexTypeXPU_impl::lu_solve)
  );
  m.impl("_lu_solve_helper",
  TORCH_FN(AtenIpexTypeXPU_impl::_lu_solve_helper)
  );
  m.impl("multinomial.out",
  TORCH_FN(AtenIpexTypeXPU_impl::multinomial_out_out)
  );
  m.impl("multinomial",
  TORCH_FN(AtenIpexTypeXPU_impl::multinomial)
  );
  m.impl("lgamma",
  TORCH_FN(AtenIpexTypeXPU_impl::lgamma)
  );
  m.impl("digamma.out",
  TORCH_FN(AtenIpexTypeXPU_impl::digamma_out_out)
  );
  m.impl("digamma",
  TORCH_FN(AtenIpexTypeXPU_impl::digamma)
  );
  m.impl("polygamma.out",
  TORCH_FN(AtenIpexTypeXPU_impl::polygamma_out_out)
  );
  m.impl("polygamma",
  TORCH_FN(AtenIpexTypeXPU_impl::polygamma)
  );
  m.impl("erfinv",
  TORCH_FN(AtenIpexTypeXPU_impl::erfinv)
  );
  m.impl("erfinv_",
  TORCH_FN(AtenIpexTypeXPU_impl::erfinv_)
  );
  m.impl("erfinv.out",
  TORCH_FN(AtenIpexTypeXPU_impl::erfinv_out_out)
  );
  m.impl("sign",
  TORCH_FN(AtenIpexTypeXPU_impl::sign)
  );
  m.impl("sign_",
  TORCH_FN(AtenIpexTypeXPU_impl::sign_)
  );
  m.impl("sign.out",
  TORCH_FN(AtenIpexTypeXPU_impl::sign_out_out)
  );
  m.impl("atan2.out",
  TORCH_FN(AtenIpexTypeXPU_impl::atan2_out_out)
  );
  m.impl("atan2",
  TORCH_FN(AtenIpexTypeXPU_impl::atan2)
  );
  m.impl("lerp.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::lerp_out_Scalar_out)
  );
  m.impl("lerp.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::lerp_out_Tensor_out)
  );
  m.impl("lerp.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::lerp_Scalar)
  );
  m.impl("lerp.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::lerp_Tensor)
  );
  m.impl("histc.out",
  TORCH_FN(AtenIpexTypeXPU_impl::histc_out_out)
  );
  m.impl("histc",
  TORCH_FN(AtenIpexTypeXPU_impl::histc)
  );
  m.impl("fmod.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::fmod_out_Scalar_out)
  );
  m.impl("fmod.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::fmod_Scalar)
  );
  m.impl("fmod.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::fmod_out_Tensor_out)
  );
  m.impl("fmod.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::fmod_Tensor)
  );
  m.impl("remainder.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::remainder_out_Scalar_out)
  );
  m.impl("remainder.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::remainder_Scalar)
  );
  m.impl("remainder.Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::remainder_out_Tensor_out)
  );
  m.impl("remainder.Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::remainder_Tensor)
  );
  m.impl("min",
  TORCH_FN(AtenIpexTypeXPU_impl::min)
  );
  m.impl("max",
  TORCH_FN(AtenIpexTypeXPU_impl::max)
  );
  m.impl("maximum",
  TORCH_FN(AtenIpexTypeXPU_impl::maximum)
  );
  m.impl("maximum.out",
  TORCH_FN(AtenIpexTypeXPU_impl::maximum_out_out)
  );
  m.impl("max.out",
  TORCH_FN(AtenIpexTypeXPU_impl::max_out_out)
  );
  m.impl("minimum",
  TORCH_FN(AtenIpexTypeXPU_impl::minimum)
  );
  m.impl("minimum.out",
  TORCH_FN(AtenIpexTypeXPU_impl::minimum_out_out)
  );
  m.impl("min.out",
  TORCH_FN(AtenIpexTypeXPU_impl::min_out_out)
  );
  m.impl("median",
  TORCH_FN(AtenIpexTypeXPU_impl::median)
  );
  m.impl("sort.values",
  TORCH_FN(AtenIpexTypeXPU_impl::sort_out_values)
  );
  m.impl("sort",
  TORCH_FN(AtenIpexTypeXPU_impl::sort)
  );
  m.impl("topk.values",
  TORCH_FN(AtenIpexTypeXPU_impl::topk_out_values)
  );
  m.impl("topk",
  TORCH_FN(AtenIpexTypeXPU_impl::topk)
  );
  m.impl("all",
  TORCH_FN(AtenIpexTypeXPU_impl::all)
  );
  m.impl("any",
  TORCH_FN(AtenIpexTypeXPU_impl::any)
  );
  m.impl("renorm.out",
  TORCH_FN(AtenIpexTypeXPU_impl::renorm_out_out)
  );
  m.impl("renorm",
  TORCH_FN(AtenIpexTypeXPU_impl::renorm)
  );
  m.impl("unfold",
  TORCH_FN(AtenIpexTypeXPU_impl::unfold)
  );
  m.impl("equal",
  TORCH_FN(AtenIpexTypeXPU_impl::equal)
  );
  m.impl("pow.Tensor_Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::pow_out_Tensor_Tensor_out)
  );
  m.impl("pow.Tensor_Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::pow_Tensor_Tensor)
  );
  m.impl("pow.Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::pow_out_Scalar_out)
  );
  m.impl("pow.Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::pow_Scalar)
  );
  m.impl("pow.Tensor_Scalar_out",
  TORCH_FN(AtenIpexTypeXPU_impl::pow_out_Tensor_Scalar_out)
  );
  m.impl("pow.Tensor_Scalar",
  TORCH_FN(AtenIpexTypeXPU_impl::pow_Tensor_Scalar)
  );
  m.impl("normal_",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_)
  );
  m.impl("normal.Tensor_float_out",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_out_Tensor_float_out)
  );
  m.impl("normal.Tensor_float",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_Tensor_float)
  );
  m.impl("normal.float_Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_out_float_Tensor_out)
  );
  m.impl("normal.float_Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_float_Tensor)
  );
  m.impl("normal.Tensor_Tensor_out",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_out_Tensor_Tensor_out)
  );
  m.impl("normal.Tensor_Tensor",
  TORCH_FN(AtenIpexTypeXPU_impl::normal_Tensor_Tensor)
  );
  m.impl("_cumsum",
  TORCH_FN(AtenIpexTypeXPU_impl::_cumsum)
  );
  m.impl("_cumsum.out",
  TORCH_FN(AtenIpexTypeXPU_impl::_cumsum_out_out)
  );
  m.impl("_cumprod",
  TORCH_FN(AtenIpexTypeXPU_impl::_cumprod)
  );
  m.impl("_cumprod.out",
  TORCH_FN(AtenIpexTypeXPU_impl::_cumprod_out_out)
  );
  m.impl("_var",
  TORCH_FN(AtenIpexTypeXPU_impl::_var)
  );
  m.impl("_std",
  TORCH_FN(AtenIpexTypeXPU_impl::_std)
  );
  m.impl("_cat",
  TORCH_FN(AtenIpexTypeXPU_impl::_cat)
  );
  m.impl("_cat.out",
  TORCH_FN(AtenIpexTypeXPU_impl::_cat_out_out)
  );
  m.impl("mse_loss.out",
  TORCH_FN(AtenIpexTypeXPU_impl::mse_loss_out_out)
  );
  m.impl("mse_loss",
  TORCH_FN(AtenIpexTypeXPU_impl::mse_loss)
  );
  m.impl("mse_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::mse_loss_backward_out_grad_input)
  );
  m.impl("mse_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::mse_loss_backward)
  );
  m.impl("l1_loss.out",
  TORCH_FN(AtenIpexTypeXPU_impl::l1_loss_out_out)
  );
  m.impl("l1_loss",
  TORCH_FN(AtenIpexTypeXPU_impl::l1_loss)
  );
  m.impl("l1_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::l1_loss_backward_out_grad_input)
  );
  m.impl("l1_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::l1_loss_backward)
  );
  m.impl("multi_margin_loss.out",
  TORCH_FN(AtenIpexTypeXPU_impl::multi_margin_loss_out_out)
  );
  m.impl("multi_margin_loss",
  TORCH_FN(AtenIpexTypeXPU_impl::multi_margin_loss)
  );
  m.impl("multi_margin_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::multi_margin_loss_backward_out_grad_input)
  );
  m.impl("multi_margin_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::multi_margin_loss_backward)
  );
  m.impl("multilabel_margin_loss_forward.output",
  TORCH_FN(AtenIpexTypeXPU_impl::multilabel_margin_loss_forward_out_output)
  );
  m.impl("multilabel_margin_loss_forward",
  TORCH_FN(AtenIpexTypeXPU_impl::multilabel_margin_loss_forward)
  );
  m.impl("multilabel_margin_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::multilabel_margin_loss_backward_out_grad_input)
  );
  m.impl("multilabel_margin_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::multilabel_margin_loss_backward)
  );
  m.impl("nll_loss_forward.output",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss_forward_out_output)
  );
  m.impl("nll_loss_forward",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss_forward)
  );
  m.impl("nll_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss_backward_out_grad_input)
  );
  m.impl("nll_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss_backward)
  );
  m.impl("nll_loss2d_forward.output",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss2d_forward_out_output)
  );
  m.impl("nll_loss2d_forward",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss2d_forward)
  );
  m.impl("nll_loss2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss2d_backward_out_grad_input)
  );
  m.impl("nll_loss2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::nll_loss2d_backward)
  );
  m.impl("smooth_l1_loss.out",
  TORCH_FN(AtenIpexTypeXPU_impl::smooth_l1_loss_out_out)
  );
  m.impl("smooth_l1_loss",
  TORCH_FN(AtenIpexTypeXPU_impl::smooth_l1_loss)
  );
  m.impl("smooth_l1_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::smooth_l1_loss_backward_out_grad_input)
  );
  m.impl("smooth_l1_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::smooth_l1_loss_backward)
  );
  m.impl("soft_margin_loss.out",
  TORCH_FN(AtenIpexTypeXPU_impl::soft_margin_loss_out_out)
  );
  m.impl("soft_margin_loss",
  TORCH_FN(AtenIpexTypeXPU_impl::soft_margin_loss)
  );
  m.impl("soft_margin_loss_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::soft_margin_loss_backward_out_grad_input)
  );
  m.impl("soft_margin_loss_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::soft_margin_loss_backward)
  );
  m.impl("elu.out",
  TORCH_FN(AtenIpexTypeXPU_impl::elu_out_out)
  );
  m.impl("elu",
  TORCH_FN(AtenIpexTypeXPU_impl::elu)
  );
  m.impl("elu_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::elu_backward_out_grad_input)
  );
  m.impl("elu_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::elu_backward)
  );
  m.impl("elu_",
  TORCH_FN(AtenIpexTypeXPU_impl::elu_)
  );
  m.impl("glu.out",
  TORCH_FN(AtenIpexTypeXPU_impl::glu_out_out)
  );
  m.impl("glu",
  TORCH_FN(AtenIpexTypeXPU_impl::glu)
  );
  m.impl("glu_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::glu_backward_out_grad_input)
  );
  m.impl("glu_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::glu_backward)
  );
  m.impl("hardtanh.out",
  TORCH_FN(AtenIpexTypeXPU_impl::hardtanh_out_out)
  );
  m.impl("hardtanh",
  TORCH_FN(AtenIpexTypeXPU_impl::hardtanh)
  );
  m.impl("hardtanh_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::hardtanh_backward_out_grad_input)
  );
  m.impl("hardtanh_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::hardtanh_backward)
  );
  m.impl("hardtanh_",
  TORCH_FN(AtenIpexTypeXPU_impl::hardtanh_)
  );
  m.impl("leaky_relu.out",
  TORCH_FN(AtenIpexTypeXPU_impl::leaky_relu_out_out)
  );
  m.impl("leaky_relu",
  TORCH_FN(AtenIpexTypeXPU_impl::leaky_relu)
  );
  m.impl("leaky_relu_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::leaky_relu_backward)
  );
  m.impl("leaky_relu_",
  TORCH_FN(AtenIpexTypeXPU_impl::leaky_relu_)
  );
  m.impl("log_sigmoid_forward.output",
  TORCH_FN(AtenIpexTypeXPU_impl::log_sigmoid_forward_out_output)
  );
  m.impl("log_sigmoid_forward",
  TORCH_FN(AtenIpexTypeXPU_impl::log_sigmoid_forward)
  );
  m.impl("log_sigmoid_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::log_sigmoid_backward_out_grad_input)
  );
  m.impl("log_sigmoid_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::log_sigmoid_backward)
  );
  m.impl("rrelu_with_noise.out",
  TORCH_FN(AtenIpexTypeXPU_impl::rrelu_with_noise_out_out)
  );
  m.impl("rrelu_with_noise",
  TORCH_FN(AtenIpexTypeXPU_impl::rrelu_with_noise)
  );
  m.impl("rrelu_with_noise_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::rrelu_with_noise_backward)
  );
  m.impl("rrelu_with_noise_",
  TORCH_FN(AtenIpexTypeXPU_impl::rrelu_with_noise_)
  );
  m.impl("softplus.out",
  TORCH_FN(AtenIpexTypeXPU_impl::softplus_out_out)
  );
  m.impl("softplus",
  TORCH_FN(AtenIpexTypeXPU_impl::softplus)
  );
  m.impl("softplus_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::softplus_backward_out_grad_input)
  );
  m.impl("softplus_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::softplus_backward)
  );
  m.impl("softshrink.out",
  TORCH_FN(AtenIpexTypeXPU_impl::softshrink_out_out)
  );
  m.impl("softshrink",
  TORCH_FN(AtenIpexTypeXPU_impl::softshrink)
  );
  m.impl("softshrink_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::softshrink_backward_out_grad_input)
  );
  m.impl("softshrink_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::softshrink_backward)
  );
  m.impl("adaptive_avg_pool2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_avg_pool2d_out_out)
  );
  m.impl("_adaptive_avg_pool2d",
  TORCH_FN(AtenIpexTypeXPU_impl::_adaptive_avg_pool2d)
  );
  m.impl("_adaptive_avg_pool2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::_adaptive_avg_pool2d_backward)
  );
  m.impl("adaptive_avg_pool3d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_avg_pool3d_out_out)
  );
  m.impl("adaptive_avg_pool3d",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_avg_pool3d)
  );
  m.impl("adaptive_avg_pool3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_avg_pool3d_backward_out_grad_input)
  );
  m.impl("adaptive_avg_pool3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_avg_pool3d_backward)
  );
  m.impl("adaptive_max_pool2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool2d_out_out)
  );
  m.impl("adaptive_max_pool2d",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool2d)
  );
  m.impl("adaptive_max_pool2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool2d_backward_out_grad_input)
  );
  m.impl("adaptive_max_pool2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool2d_backward)
  );
  m.impl("adaptive_max_pool3d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool3d_out_out)
  );
  m.impl("adaptive_max_pool3d",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool3d)
  );
  m.impl("adaptive_max_pool3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool3d_backward_out_grad_input)
  );
  m.impl("adaptive_max_pool3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::adaptive_max_pool3d_backward)
  );
  m.impl("avg_pool2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool2d_out_out)
  );
  m.impl("avg_pool2d",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool2d)
  );
  m.impl("avg_pool2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool2d_backward_out_grad_input)
  );
  m.impl("avg_pool2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool2d_backward)
  );
  m.impl("avg_pool3d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool3d_out_out)
  );
  m.impl("avg_pool3d",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool3d)
  );
  m.impl("avg_pool3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool3d_backward_out_grad_input)
  );
  m.impl("avg_pool3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::avg_pool3d_backward)
  );
  m.impl("fractional_max_pool2d.output",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool2d_out_output)
  );
  m.impl("fractional_max_pool2d",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool2d)
  );
  m.impl("fractional_max_pool2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool2d_backward_out_grad_input)
  );
  m.impl("fractional_max_pool2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool2d_backward)
  );
  m.impl("fractional_max_pool3d.output",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool3d_out_output)
  );
  m.impl("fractional_max_pool3d",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool3d)
  );
  m.impl("fractional_max_pool3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool3d_backward_out_grad_input)
  );
  m.impl("fractional_max_pool3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::fractional_max_pool3d_backward)
  );
  m.impl("max_pool2d_with_indices.out",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool2d_with_indices_out_out)
  );
  m.impl("max_pool2d_with_indices",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool2d_with_indices)
  );
  m.impl("max_pool2d_with_indices_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool2d_with_indices_backward_out_grad_input)
  );
  m.impl("max_pool2d_with_indices_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool2d_with_indices_backward)
  );
  m.impl("max_pool3d_with_indices.out",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool3d_with_indices_out_out)
  );
  m.impl("max_pool3d_with_indices",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool3d_with_indices)
  );
  m.impl("max_pool3d_with_indices_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool3d_with_indices_backward_out_grad_input)
  );
  m.impl("max_pool3d_with_indices_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::max_pool3d_with_indices_backward)
  );
  m.impl("max_unpool2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool2d_out_out)
  );
  m.impl("max_unpool2d",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool2d)
  );
  m.impl("max_unpool2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool2d_backward_out_grad_input)
  );
  m.impl("max_unpool2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool2d_backward)
  );
  m.impl("max_unpool3d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool3d_out_out)
  );
  m.impl("max_unpool3d",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool3d)
  );
  m.impl("max_unpool3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool3d_backward_out_grad_input)
  );
  m.impl("max_unpool3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::max_unpool3d_backward)
  );
  m.impl("reflection_pad1d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::reflection_pad1d_out_out)
  );
  m.impl("reflection_pad1d",
  TORCH_FN(AtenIpexTypeXPU_impl::reflection_pad1d)
  );
  m.impl("reflection_pad1d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::reflection_pad1d_backward_out_grad_input)
  );
  m.impl("reflection_pad1d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::reflection_pad1d_backward)
  );
  m.impl("replication_pad2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::replication_pad2d_out_out)
  );
  m.impl("replication_pad2d",
  TORCH_FN(AtenIpexTypeXPU_impl::replication_pad2d)
  );
  m.impl("replication_pad2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::replication_pad2d_backward_out_grad_input)
  );
  m.impl("replication_pad2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::replication_pad2d_backward)
  );
  m.impl("upsample_linear1d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_linear1d_vec)
  );
  m.impl("upsample_linear1d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_linear1d_backward_vec)
  );
  m.impl("upsample_bilinear2d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bilinear2d_vec)
  );
  m.impl("upsample_bilinear2d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bilinear2d_backward_vec)
  );
  m.impl("upsample_trilinear3d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_trilinear3d_vec)
  );
  m.impl("upsample_trilinear3d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_trilinear3d_backward_vec)
  );
  m.impl("upsample_bicubic2d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bicubic2d_vec)
  );
  m.impl("upsample_bicubic2d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bicubic2d_backward_vec)
  );
  m.impl("upsample_nearest1d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest1d_vec)
  );
  m.impl("upsample_nearest1d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest1d_backward_vec)
  );
  m.impl("upsample_nearest2d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest2d_vec)
  );
  m.impl("upsample_nearest2d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest2d_backward_vec)
  );
  m.impl("upsample_nearest3d.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest3d_vec)
  );
  m.impl("upsample_nearest3d_backward.vec",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest3d_backward_vec)
  );
  m.impl("upsample_linear1d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_linear1d_out_out)
  );
  m.impl("upsample_linear1d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_linear1d)
  );
  m.impl("upsample_linear1d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_linear1d_backward_out_grad_input)
  );
  m.impl("upsample_linear1d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_linear1d_backward)
  );
  m.impl("upsample_bilinear2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bilinear2d_out_out)
  );
  m.impl("upsample_bilinear2d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bilinear2d)
  );
  m.impl("upsample_bilinear2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bilinear2d_backward_out_grad_input)
  );
  m.impl("upsample_bilinear2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bilinear2d_backward)
  );
  m.impl("upsample_bicubic2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bicubic2d_out_out)
  );
  m.impl("upsample_bicubic2d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bicubic2d)
  );
  m.impl("upsample_bicubic2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bicubic2d_backward_out_grad_input)
  );
  m.impl("upsample_bicubic2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_bicubic2d_backward)
  );
  m.impl("upsample_trilinear3d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_trilinear3d_out_out)
  );
  m.impl("upsample_trilinear3d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_trilinear3d)
  );
  m.impl("upsample_trilinear3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_trilinear3d_backward_out_grad_input)
  );
  m.impl("upsample_trilinear3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_trilinear3d_backward)
  );
  m.impl("upsample_nearest1d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest1d_out_out)
  );
  m.impl("upsample_nearest1d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest1d)
  );
  m.impl("upsample_nearest1d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest1d_backward_out_grad_input)
  );
  m.impl("upsample_nearest1d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest1d_backward)
  );
  m.impl("upsample_nearest2d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest2d_out_out)
  );
  m.impl("upsample_nearest2d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest2d)
  );
  m.impl("upsample_nearest2d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest2d_backward_out_grad_input)
  );
  m.impl("upsample_nearest2d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest2d_backward)
  );
  m.impl("upsample_nearest3d.out",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest3d_out_out)
  );
  m.impl("upsample_nearest3d",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest3d)
  );
  m.impl("upsample_nearest3d_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest3d_backward_out_grad_input)
  );
  m.impl("upsample_nearest3d_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::upsample_nearest3d_backward)
  );
  m.impl("sigmoid_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::sigmoid_backward_out_grad_input)
  );
  m.impl("sigmoid_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::sigmoid_backward)
  );
  m.impl("tanh_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::tanh_backward_out_grad_input)
  );
  m.impl("tanh_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::tanh_backward)
  );
  m.impl("col2im.out",
  TORCH_FN(AtenIpexTypeXPU_impl::col2im_out_out)
  );
  m.impl("col2im",
  TORCH_FN(AtenIpexTypeXPU_impl::col2im)
  );
  m.impl("col2im_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::col2im_backward_out_grad_input)
  );
  m.impl("col2im_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::col2im_backward)
  );
  m.impl("im2col.out",
  TORCH_FN(AtenIpexTypeXPU_impl::im2col_out_out)
  );
  m.impl("im2col",
  TORCH_FN(AtenIpexTypeXPU_impl::im2col)
  );
  m.impl("im2col_backward.grad_input",
  TORCH_FN(AtenIpexTypeXPU_impl::im2col_backward_out_grad_input)
  );
  m.impl("im2col_backward",
  TORCH_FN(AtenIpexTypeXPU_impl::im2col_backward)
  );
  m.impl("record_stream",
  TORCH_FN(AtenIpexTypeXPU_impl::record_stream)
  );
  m.impl("ger",
  TORCH_FN(AtenIpexTypeXPU_impl::ger)
  );
  m.impl("ger.out",
  TORCH_FN(AtenIpexTypeXPU_impl::ger_out_out)
  );
}
#endif

} // namespace AtenIpexTypeXPU

} // namespace at
