<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance Tuning Guide &mdash; intel_extension_for_pytorch 1.11.200+cpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Launch Script Usage Guide" href="launch_script.html" />
    <link rel="prev" title="Performance Tuning Guide" href="../performance_tuning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../versions.html">1.11.200+cpu ▼</a>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Performance Tuning Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hardware-configuration">Hardware Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#intel-cpu-structure">Intel CPU Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#non-uniform-memory-access-numa">Non-Uniform Memory Access (NUMA)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#software-configuration">Software Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#channels-last">Channels Last</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numactl">Numactl</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmp">OpenMP</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#omp-num-threads">OMP_NUM_THREADS</a></li>
<li class="toctree-l5"><a class="reference internal" href="#gnu-openmp">GNU OpenMP</a></li>
<li class="toctree-l5"><a class="reference internal" href="#intel-openmp">Intel OpenMP</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#memory-allocator">Memory Allocator</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#jemalloc">Jemalloc</a></li>
<li class="toctree-l5"><a class="reference internal" href="#tcmalloc">TCMalloc</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#denormal-number">Denormal Number</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
<li class="toctree-l2"><a class="reference internal" href="known_issues.html">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../performance_tuning.html">Performance Tuning Guide</a> &raquo;</li>
      <li>Performance Tuning Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/tuning_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="performance-tuning-guide">
<h1>Performance Tuning Guide<a class="headerlink" href="#performance-tuning-guide" title="Permalink to this headline"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Intel Extension for PyTorch (IPEX) is a Python package to extend official PyTorch. It is designed to make the Out-of-Box user experience of PyTorch CPU better while achieving good performance. To fully utilize the power of Intel® architecture and thus yield high performance, PyTorch, as well as IPEX, are powered by <a class="reference external" href="https://github.com/oneapi-src/oneDNN">oneAPI Deep Neural Network Library (oneDNN)</a>, an open-source cross-platform performance library of basic building blocks for deep learning applications. It is developed and optimized for Intel Architecture Processors, Intel Processor Graphics and Xe architecture-based Graphics.</p>
<p>Although by default primitives of PyTorch and IPEX are highly optimized, there are still something that users can do to optimize for performance further more. Most optimized configurations can be automatically set by the launcher script. This article introduces common methods that Intel developers recommend to take.</p>
<ul class="simple">
<li><p>Hardware Configuration</p>
<ul>
<li><p>Intel CPU Structure</p></li>
<li><p>Non-Uniform Memory Access (NUMA)</p></li>
</ul>
</li>
<li><p>Software Configuration</p>
<ul>
<li><p>Numactl</p></li>
<li><p>OpenMP</p>
<ul>
<li><p>OMP_NUM_THREADS</p></li>
<li><p>GNU OpenMP</p>
<ul>
<li><p>GOMP_CPU_AFFINITY</p></li>
<li><p>OMP_PROC_BIND</p></li>
<li><p>OMP_SCHEDULE</p></li>
</ul>
</li>
<li><p>Intel OpenMP</p>
<ul>
<li><p>KMP_AFFINITY</p></li>
<li><p>KMP_BLOCKTIME</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Memory Allocator</p>
<ul>
<li><p>Jemalloc</p></li>
<li><p>TCMalloc</p></li>
</ul>
</li>
<li><p>Denormal Number</p></li>
</ul>
</li>
</ul>
</section>
<section id="hardware-configuration">
<h2>Hardware Configuration<a class="headerlink" href="#hardware-configuration" title="Permalink to this headline"></a></h2>
<p>This section briefly instroduces structure of Intel CPUs, as well as concept of Non-Uniform Memory Access (NUMA), as background knowledges.</p>
<section id="intel-cpu-structure">
<h3>Intel CPU Structure<a class="headerlink" href="#intel-cpu-structure" title="Permalink to this headline"></a></h3>
<p>There are a bunch of SKUs or families of Intel CPUs. In this article, Intel® Xeon® processor Scalable family is used as an example to show briefly what is Intel CPU, and how it works. Understanding these background knowledge is helpful to understand the optimization methodologies that Intel engineers recommend to use.</p>
<p><img alt="Intel® Xeon® processor Scalable family" src="https://www.trentonsystems.com/hs-fs/hubfs/Intel-Xeon-Scalable-1.jpg?width=2520&amp;name=Intel-Xeon-Scalable-1.jpg" /></p>
<p>Figure 1.1 Intel® Xeon® processor Scalable family</p>
<p>Figure 1.1 shows a series of Intel Xeon processor Scalable family CPU chips. On the Purley platform each chip provides up to 28 cores. Each core has a non-inclusive last-level cache and an 1MB L2 cache. The CPU features fast 2666 MHz DDR4 memory, six memory channels per CPU, Intel Ultra Path Interconnect (UPI) high speed point-to-point processor interconnect, and more. Figure 1.2 shows microarchitecture of the Intel® Xeon® processor Scalable family chips. Each CPU chip consists of a number of cores, along with core-specific cache. 6 channels of DDR4 memory are connected to the chip directly. Meanwhile, chips communicates through the Intel UPI interconnect, which features a transfer speed of up to 10.4 GT/s.</p>
<p><img alt="Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture" src="https://software.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig03-737410.png" /></p>
<p>Figure 1.2 Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture</p>
<p>Usually, a CPU chip is called a socket. A typical two-socket configuration is illustrated as Figure 1.3. Two CPU chips, or say two sockets, are equipped on one motherboard. Each socket is connected to up to 6 channels of memory, which is called its local memory, from socket perspective. Sockets are connected to each other via Intel UPI. It is possible for each socket to access memories attached on other sockets, usually called remote memory access. Local memory access is always faster than remote memory access. Meanwhile, cores on one socket share a space of high speed cache memory, which is much faster than communication via Intel UPI. Figure 1.4 shows an ASUS Z11PA-D8 Intel® Xeon® server motherboard, equipping with two sockets for Intel® Xeon® processor Scalable family CPUs.</p>
<p><img alt="Typical two-socket configuration" src="https://software.intel.com/content/dam/develop/external/us/en/images/xeon-processor-scalable-family-tech-overview-fig06-737410.png" /></p>
<p>Figure 1.3 Typical two-socket configuration</p>
<p><img alt="ASUS Z11PA-D8 Intel® Xeon® server motherboard" src="https://dlcdnimgs.asus.com/websites/global/products/MCCApMgGOdr9WJxN/MB-Z11PAD8-overview-01-s.jpg" /></p>
<p>Figure 1.4 An ASUS Z11PA-D8 Intel® Xeon® server motherboard. It contains two sockets for Intel® Xeon® processor Scalable family CPUs.</p>
</section>
<section id="non-uniform-memory-access-numa">
<h3>Non-Uniform Memory Access (NUMA)<a class="headerlink" href="#non-uniform-memory-access-numa" title="Permalink to this headline"></a></h3>
<p>It is a good thing that more and more CPU cores are provided to users in one socket, because this brings more computation resources. However, this also brings memory access competitions. Program can stall because memory is busy to visit. To address this problem, Non-Uniform Memory Access (NUMA) was introduced. Comparing to Uniform Memory Access (UMA), in which scenario all memories are connected to all cores equally, NUMA tells memories into multiple groups. Certain number of memories are directly attached to one socket’s integrated memory controller to become local memory of this socket. As described in the previous section, local memory access is much faster than remote memory access.</p>
<p>Usrs can get CPU information with <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> command on Linux to learn how many cores, sockets there on the machine. Also, NUMA information like how CPU cores are distributed can also be retrieved. The following is an example of <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> execution on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. 2 sockets were detected. Each socket has 28 physical cores onboard. Since Hyper-Threading is enabled, each core can run 2 threads. I.e. each socket has another 28 logical cores. Thus, there are 112 CPU cores on service. When indexing CPU cores, usually physical cores are indexed prior to logical core. In this case, the first 28 cores (0-27) are physical cores on the first NUMA socket (node), the second 28 cores (28-55) are physical cores on the second NUMA socket (node). Logical cores are indexed afterward. 56-83 are 28 logical cores on the first NUMA socket (node), 84-111 are the second 28 logical cores on the second NUMA socket (node). Typically, running IPEX should avoid using logical cores to get a good performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ lscpu
...
CPU(s):              112
On-line CPU(s) list: 0-111
Thread(s) per core:  2
Core(s) per socket:  28
Socket(s):           2
NUMA node(s):        2
...
Model name:          Intel(R) Xeon(R) Platinum 8180M CPU @ 2.50GHz
...
NUMA node0 CPU(s):   0-27,56-83
NUMA node1 CPU(s):   28-55,84-111
...
</pre></div>
</div>
</section>
</section>
<section id="software-configuration">
<h2>Software Configuration<a class="headerlink" href="#software-configuration" title="Permalink to this headline"></a></h2>
<p>This section introduces software configurations that helps to boost performance.</p>
<section id="channels-last">
<h3>Channels Last<a class="headerlink" href="#channels-last" title="Permalink to this headline"></a></h3>
<p>Please take advantage of <strong>Channels Last</strong> memory format for image processing tasks. Comparing to PyTorch default NCHW (<code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>) memory format, NHWC (<code class="docutils literal notranslate"><span class="pre">torch.channels_last</span></code>) is more friendly to Intel platforms, and thus generally yields better performance. More detailed introduction can be found at <a class="reference external" href="../features/nhwc.html">Channels Last page</a>. You can get sample codes with Resnet50 at <a class="reference external" href="../examples.html">Example page</a>.</p>
</section>
<section id="numactl">
<h3>Numactl<a class="headerlink" href="#numactl" title="Permalink to this headline"></a></h3>
<p>Since NUMA largely influences memory access performance, this functionality should also be implemented in software side.</p>
<p>During development of Linux kernels, more and more sophisticated implementations/optimizations/strategies had been brought out. Version 2.5 of the Linux kernel already contained basic NUMA support, which was further improved in subsequent kernel releases. Version 3.8 of the Linux kernel brought a new NUMA foundation that allowed development of more efficient NUMA policies in later kernel releases. Version 3.13 of the Linux kernel brought numerous policies that aim at putting a process near its memory, together with the handling of cases such as having memory pages shared between processes, or the use of transparent huge pages; new sysctl settings allow NUMA balancing to be enabled or disabled, as well as the configuration of various NUMA memory balancing parameters.[1] Behaviour of Linux kernels are thus different according to kernel version. Newer Linux kernels may contain further optimizations of NUMA strategies, and thus have better performances. For some workloads, NUMA strategy influences performance great.</p>
<p>Linux provides a tool, <code class="docutils literal notranslate"><span class="pre">numactl</span></code>, to allow users to control NUMA policy for processes or shared memory. It runs processes with a specific NUMA scheduling or memory placement policy. As described in previous section, cores share high-speed cache in one socket, thus it is a good idea to avoid cross socket computations. From memory access perspective, bounding memory access to local ones is much faster than accessing remote memories.</p>
<p>The following is an example of numactl usage to run a workload on the Nth socket, and limit memory access to its local memories on the Nth socket. More detailed description of numactl command can be found <a class="reference external" href="https://linux.die.net/man/8/numactl">here</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">--cpunodebind</span> <span class="pre">N</span> <span class="pre">--membind</span> <span class="pre">N</span> <span class="pre">python</span> <span class="pre">&lt;script&gt;</span></code></p>
<p>Assume core 0-3 are on socket 0, the following command binds script execution on core 0-3, and binds memory access to socket 0 local memories.</p>
<p><code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">--membind</span> <span class="pre">0</span> <span class="pre">-C</span> <span class="pre">0-3</span> <span class="pre">python</span> <span class="pre">&lt;script&gt;</span></code></p>
<p>[1] <a class="reference external" href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">Wikipedia - Non-uniform memory access</a></p>
</section>
<section id="openmp">
<h3>OpenMP<a class="headerlink" href="#openmp" title="Permalink to this headline"></a></h3>
<p>OpenMP is an implementation of multithreading, a method of parallelizing whereby a primary thread (a series of instructions executed consecutively) forks a specified number of sub-threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors.[1] Figure 2.1 illustrates fork-join model of OpenMP execution.</p>
<p><img alt="A number of parallel block execution threads are forked from primary thread" src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Fork_join.svg/1920px-Fork_join.svg.png" /></p>
<p>Figure 2.1 A number of parallel block execution threads are forked from primary thread</p>
<p>Users can control OpenMP behaviours through some environment variables to fit for their workloads. Also, beside GNU OpenMP library (<a class="reference external" href="https://gcc.gnu.org/onlinedocs/libgomp/">libgomp</a>), Intel provides another OpenMP implementation <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support.html">libiomp</a> for users to choose from. Environment variables which controls behaviour of OpenMP threads may differ from libgomp and libiomp. They will be introduced separately in sections below.</p>
<p>GNU OpenMP (libgomp) is the default multi-threading library for both PyTorch and IPEX.</p>
<p>[1] <a class="reference external" href="https://en.wikipedia.org/wiki/OpenMP">Wikipedia - OpenMP</a></p>
<section id="omp-num-threads">
<h4>OMP_NUM_THREADS<a class="headerlink" href="#omp-num-threads" title="Permalink to this headline"></a></h4>
<p>Environment variable OMP_NUM_THREADS sets the number of threads to use for parallel regions. By default, it is set to be number of available physical cores. It can be used alongwith numactl settings, as the following example. If cores 0-3 are on socket 0, this example command runs &lt;script&gt; on cores 0-3, with 4 OpenMP threads.</p>
<p>This environment variable works on both libgomp and libiomp.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">4</span>
<span class="n">numactl</span> <span class="o">-</span><span class="n">C</span> <span class="mi">0</span><span class="o">-</span><span class="mi">3</span> <span class="o">--</span><span class="n">membind</span> <span class="mi">0</span> <span class="n">python</span> <span class="o">&lt;</span><span class="n">script</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="gnu-openmp">
<h4>GNU OpenMP<a class="headerlink" href="#gnu-openmp" title="Permalink to this headline"></a></h4>
<p>Beside OMP_NUM_THREADS, A couple of GNU OpenMP specific environment variables are commonly used to improve performance.</p>
<ul class="simple">
<li><p>GOMP_CPU_AFFINITY: Binds threads to specific CPUs. The variable should contain a space-separated or comma-separated list of CPUs.</p></li>
<li><p>OMP_PROC_BIND: Specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions.</p></li>
<li><p>OMP_SCHEDULE: Determine how OpenMP threads are scheduled.</p></li>
</ul>
<p>Following is a recommended combination of these environment variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">GOMP_CPU_AFFINITY</span><span class="o">=</span><span class="s2">&quot;0-3&quot;</span>
<span class="n">export</span> <span class="n">OMP_PROC_BIND</span><span class="o">=</span><span class="n">CLOSE</span>
<span class="n">export</span> <span class="n">OMP_SCHEDULE</span><span class="o">=</span><span class="n">STATIC</span>
</pre></div>
</div>
</section>
<section id="intel-openmp">
<h4>Intel OpenMP<a class="headerlink" href="#intel-openmp" title="Permalink to this headline"></a></h4>
<p>By default, PyTorch uses GNU OpenMP (GNU libgomp) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It sometimes brings more performance benefits compared to libgomp. Utilizing environment variable LD_PRELOAD can switch OpenMP library to libiomp:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export LD_PRELOAD=&lt;path&gt;/libiomp5.so:$LD_PRELOAD
</pre></div>
</div>
<p>Similar to GNU OpenMP, beside OMP_NUM_THREADS, there are several Intel OpenMP specific environment variables control behaviour of OpenMP threads.</p>
<ul class="simple">
<li><p>KMP_AFFINITY</p></li>
</ul>
<p>KMP_AFFINITY controls how to to bind OpenMP threads to physical processing units. Depending on the system (machine) topology, application, and operating system, thread affinity can have a dramatic effect on the application speed.</p>
<p>A common usage scenario is that We would like consecutive threads to be bound close together, as is done with KMP_AFFINITY=compact, so that communication overhead, cache line invalidation overhead, and page thrashing are minimized. Now, suppose the application also had a number of parallel regions which did not utilize all of the available OpenMP threads. It is desirable to avoid binding multiple threads to the same core and leaving other cores not utilized, since a thread normally executes faster on a core where it is not competing for resources with another active thread on the same core. Since a thread normally executes faster on a core where it is not competing for resources with another active thread on the same core, it is always good to avoid binding multiple threads to the same core while leaving other cores unused. This can be achieved by the following command. Figure 2.2 illustrates this strategy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">granularity</span><span class="o">=</span><span class="n">fine</span><span class="p">,</span><span class="n">compact</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span>
</pre></div>
</div>
<p><img alt="KMP_AFFINITY=granularity=fine,compact,1,0" src="../../_images/kmp_affinity.jpg" /></p>
<p>Figure 2.2 <em>KMP_AFFINITY=granularity=fine,compact,1,0</em> The OpenMP thread n+1 is bound to a thread context as close as possible to OpenMP thread n, but on a different core. Once each core has been assigned one OpenMP thread, the subsequent OpenMP threads are assigned to the available cores in the same order, but they are assigned on different thread contexts.</p>
<p>It is also possible to bind OpenMP threads to certain CPU cores with the following command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">granularity</span><span class="o">=</span><span class="n">fine</span><span class="p">,</span><span class="n">proclist</span><span class="o">=</span><span class="p">[</span><span class="n">N</span><span class="o">-</span><span class="n">M</span><span class="p">],</span><span class="n">explicit</span>
</pre></div>
</div>
<p>More detailed information about KMP_AFFINITY can be found <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html">here</a>.</p>
<ul class="simple">
<li><p>KMP_BLOCKTIME</p></li>
</ul>
<p>KMP_BLOCKTIME sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. The default value is 200ms.</p>
<p>After completing the execution of a parallel region, threads wait for new parallel work to become available. After a certain period of time has elapsed, they stop waiting and sleep. Sleeping allows the threads to be used, until more parallel work becomes available, by non-OpenMP threaded code that may execute between parallel regions, or by other applications. A small KMP_BLOCKTIME value may offer better overall performance if application contains non-OpenMP threaded code that executes between parallel regions. A larger KMP_BLOCKTIME value may be more appropriate if threads are to be reserved solely for use for OpenMP execution, but may penalize other concurrently-running OpenMP or threaded applications. It is suggested to be set to 0 or 1 for convolutional neural network (CNN) based models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_BLOCKTIME</span><span class="o">=</span><span class="mi">0</span> <span class="p">(</span><span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="memory-allocator">
<h3>Memory Allocator<a class="headerlink" href="#memory-allocator" title="Permalink to this headline"></a></h3>
<p>Memory allocator plays an important role from performance perspective as well. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus results in a faster execution. From practical experiences, for deep learning workloads, Jemalloc or TCMalloc can get better performance by reusing memory as much as possible than default malloc funtion.</p>
<p>It is as simple as adding path of Jemalloc/TCMalloc dynamic library to environment variable LD_PRELOAD to switch memory allocator to one of them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export LD_PRELOAD=&lt;jemalloc.so/tcmalloc.so&gt;:$LD_PRELOAD
</pre></div>
</div>
<section id="jemalloc">
<h4>Jemalloc<a class="headerlink" href="#jemalloc" title="Permalink to this headline"></a></h4>
<p><a class="reference external" href="https://github.com/jemalloc/jemalloc">Jemalloc</a> is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support. More detailed introduction of performance tuning with Jemalloc can be found at <a class="reference external" href="https://android.googlesource.com/platform/external/jemalloc_new/+/6e6a93170475c05ebddbaf3f0df6add65ba19f01/TUNING.md">Jemalloc tuning guide</a></p>
<p>A recommended setting for <code class="docutils literal notranslate"><span class="pre">MALLOC_CONF</span></code> is <code class="docutils literal notranslate"><span class="pre">oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000</span></code> from performance perspective. However, in some cases the <code class="docutils literal notranslate"><span class="pre">dirty_decay_ms:9000000000,mmuzzy_decay_ms:9000000000</span></code> may cause Out-of-Memory crash. Please try <code class="docutils literal notranslate"><span class="pre">oversize_threshold:1,background_thread:true,metadata_thp:auto</span></code> instead in this case.</p>
<p>Getting Jemalloc is straight-forward.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">jemalloc</span><span class="o">/</span><span class="n">jemalloc</span>
<span class="n">cd</span> <span class="n">jemalloc</span>
<span class="o">./</span><span class="n">autogen</span><span class="o">.</span><span class="n">sh</span>
<span class="n">make</span>
<span class="n">make</span> <span class="n">install</span>
</pre></div>
</div>
</section>
<section id="tcmalloc">
<h4>TCMalloc<a class="headerlink" href="#tcmalloc" title="Permalink to this headline"></a></h4>
<p><a class="reference external" href="https://github.com/google/tcmalloc">TCMalloc</a> also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated. It is part of <a class="reference external" href="https://github.com/gperftools/gperftools">gpertools</a>, a collection of a high-performance multi-threaded malloc() implementation, plus some pretty nifty performance analysis tools.</p>
<p>Getting TCMalloc is also not complicated.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gperftools</span><span class="o">/</span><span class="n">gperftools</span><span class="o">/</span><span class="n">releases</span><span class="o">/</span><span class="n">download</span><span class="o">/</span><span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;/</span><span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
<span class="n">tar</span> <span class="n">xzvf</span> <span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
<span class="n">cd</span> <span class="n">gperftools</span><span class="o">-&lt;</span><span class="n">version</span><span class="o">&gt;</span>
<span class="o">./</span><span class="n">configure</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">cpu</span><span class="o">-</span><span class="n">profiler</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">heap</span><span class="o">-</span><span class="n">profiler</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">heap</span><span class="o">-</span><span class="n">checker</span> <span class="o">--</span><span class="n">disable</span><span class="o">-</span><span class="n">debugalloc</span> <span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">minimal</span> 
<span class="n">make</span>
<span class="n">make</span> <span class="n">install</span>
</pre></div>
</div>
</section>
</section>
<section id="denormal-number">
<h3>Denormal Number<a class="headerlink" href="#denormal-number" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Denormal_number">Denormal number</a> is used to store extremely small numbers which are close to 0. Computations with denormal numbers are remarkably slower than normalized number. To solve the low performance issue caused by denormal numbers, users can use the following PyTorch API function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_flush_denormal</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../performance_tuning.html" class="btn btn-neutral float-left" title="Performance Tuning Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="launch_script.html" class="btn btn-neutral float-right" title="Launch Script Usage Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>