<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Features &mdash; Intel&amp;#174 Extension for PyTorch* 2.1.10+xpu documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Channels Last" href="features/nhwc.html" />
    <link rel="prev" title="Introduction" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../">2.1.10+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Features</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#device-agnostic">Device-Agnostic</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#easy-to-use-python-api">Easy-to-use Python API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#channels-last">Channels Last</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/nhwc.html">Channels Last</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/auto_channels_last.html">Auto Channels Last</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/amp_cpu.html">Auto Mixed Precision (AMP) on CPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/amp_gpu.html">Auto Mixed Precision (AMP) on GPU</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#quantization">Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/int8_overview.html">Intel® Extension for PyTorch* optimizations for quantization [CPU]</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/int8_recipe_tuning_api.html">INT8 Recipe Tuning API (Experimental) [CPU]</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/int8_overview_xpu.html">Intel® Extension for PyTorch* Optimizations for Quantization [GPU]</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/int4.html">INT4 inference [GPU] (Experimental)</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/float8.html">Float8 Data Type Support [GPU] (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-training">Distributed Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/DDP.html">DistributedDataParallel (DDP)</a></li>
<li class="toctree-l4"><a class="reference internal" href="features/horovod.html">Horovod with PyTorch (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gpu-specific">GPU-Specific</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dlpack-solution">DLPack Solution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/DLPack.html">DLPack Solution</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dpc-extension">DPC++ Extension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/DPC%2B%2B_Extension.html">DPC++ Extension</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-configuration">Advanced Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/advanced_configuration.html">Advanced Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/FSDP.html">Fully Sharded Data Parallel (FSDP)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#inductor">Inductor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/torch_compile_gpu.html">torch.compile for GPU</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#legacy-profiler-tool-experimental">Legacy Profiler Tool (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/profiler_legacy.html">Legacy Profiler Tool (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#simple-trace-tool-experimental">Simple Trace Tool (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/simple_trace.html">Simple Trace Tool (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#kineto-supported-profiler-tool-experimental">Kineto Supported Profiler Tool (Experimental)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/profiler_kineto.html">Kineto Supported Profiler Tool (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#compute-engine-experimental-feature-for-debug">Compute Engine (Experimental feature for debug)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/compute_engine.html">Compute Engine (Experimental feature for debug)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cpu-specific">CPU-Specific</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#operator-optimization">Operator Optimization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#intel_extension_for_pytorch.nn.FrozenBatchNorm2d"><code class="docutils literal notranslate"><span class="pre">FrozenBatchNorm2d</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel_extension_for_pytorch.nn.functional.interaction"><code class="docutils literal notranslate"><span class="pre">interaction()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#runtime-extension">Runtime Extension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/runtime_extension.html">Runtime Extension</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#codeless-optimization-experimental-new-feature-in-1-13">Codeless Optimization (Experimental, <em>NEW feature in 1.13.*</em>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/codeless_optimization.html">Codeless Optimization (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#graph-capture-experimental-new-feature-in-1-13-0">Graph Capture (Experimental, <em>NEW feature in 1.13.0*</em>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/graph_capture.html">Graph Capture (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#hypertune-experimental-new-feature-in-1-13-0">HyperTune (Experimental, <em>NEW feature in 1.13.0*</em>)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="features/hypertune.html">HyperTune (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Features</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/features.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="features">
<h1>Features<a class="headerlink" href="#features" title="Permalink to this heading"></a></h1>
<section id="device-agnostic">
<h2>Device-Agnostic<a class="headerlink" href="#device-agnostic" title="Permalink to this heading"></a></h2>
<section id="easy-to-use-python-api">
<h3>Easy-to-use Python API<a class="headerlink" href="#easy-to-use-python-api" title="Permalink to this heading"></a></h3>
<p>Intel® Extension for PyTorch* provides simple frontend Python APIs and utilities to get performance optimizations such as operator optimization.</p>
<p>Check the <a class="reference external" href="api_doc.html">API Documentation</a> for API functions description and <a class="reference external" href="examples.html">Examples</a> for usage guidance.</p>
</section>
<section id="channels-last">
<h3>Channels Last<a class="headerlink" href="#channels-last" title="Permalink to this heading"></a></h3>
<p>Compared with the default NCHW memory format, using channels_last (NHWC) memory format can further accelerate convolutional neural networks. In Intel® Extension for PyTorch*, NHWC memory format has been enabled for most key CPU and GPU operators. More detailed information is available at <a class="reference external" href="features/nhwc.html">Channels Last</a>.</p>
<p>Intel® Extension for PyTorch* automatically converts a model to channels last memory format when users optimize the model with <code class="docutils literal notranslate"><span class="pre">ipex.optimize(model)</span></code>. With this feature, users do not need to manually apply <code class="docutils literal notranslate"><span class="pre">model=model.to(memory_format=torch.channels_last)</span></code> anymore. However, models running on Intel® Data Center GPU Flex Series will choose oneDNN layout, so users still need to manually convert the model and data to channels last format. More detailed information is available at <a class="reference external" href="features/auto_channels_last.html">Auto Channels Last</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="auto-mixed-precision-amp">
<h3>Auto Mixed Precision (AMP)<a class="headerlink" href="#auto-mixed-precision-amp" title="Permalink to this heading"></a></h3>
<p>Benefiting from less memory usage and computation, low precision data types typically speed up both training and inference workloads. Furthermore, accelerated by Intel® native hardware instructions, including Intel® Deep Learning Boost (Intel® DL Boost) on the 3rd Generation Xeon® Scalable Processors (aka Cooper Lake), as well as the Intel® Advanced Matrix Extensions (Intel® AMX) instruction set on the 4th next generation of Intel® Xeon® Scalable Processors (aka Sapphire Rapids), low precision data type, bfloat 16 and float16, provide further boosted performance. We recommend to use AMP for accelerating convolutional and matmul based neural networks.</p>
<p>The support of Auto Mixed Precision (AMP) with <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html">BFloat16 on CPU</a> and BFloat16 optimization of operators has been enabled in Intel® Extension for PyTorch*, and partially upstreamed to PyTorch master branch. These optimizations will be landed in PyTorch master through PRs that are being submitted and reviewed. On GPU side, support of BFloat16 and Float16 are both available in Intel® Extension for PyTorch*. BFloat16 is the default low precision floating data type when AMP is enabled.</p>
<p>Detailed information of AMP for GPU and CPU are available at <a class="reference external" href="features/amp_gpu.html">Auto Mixed Precision (AMP) on GPU</a> and <a class="reference external" href="features/amp_cpu.html">Auto Mixed Precision (AMP) on CPU</a> respectively.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="quantization">
<h3>Quantization<a class="headerlink" href="#quantization" title="Permalink to this heading"></a></h3>
<p>Intel® Extension for PyTorch* provides built-in INT8 quantization recipes to deliver good statistical accuracy for most popular DL workloads including CNN, NLP and recommendation models on CPU side. On top of that, if users would like to tune for a higher accuracy than what the default recipe provides, a recipe tuning API powered by Intel® Neural Compressor is provided for users to try.</p>
<p>Check more detailed information for <a class="reference external" href="features/int8_overview.html">INT8 Quantization [CPU]</a> and <a class="reference external" href="features/int8_recipe_tuning_api.html">INT8 recipe tuning API guide (Experimental, *NEW feature in 1.13.0* on CPU)</a> on CPU side.</p>
<p>Check more detailed information for <a class="reference external" href="features/int8_overview_xpu.html">INT8 Quantization [XPU]</a>.</p>
<p>On Intel® GPUs, Intel® Extension for PyTorch* also provides INT4 and FP8 Quantization.  Check more detailed information for <a class="reference external" href="./features/float8.html">FP8 Quantization</a> and <a class="reference external" href="./features/int4.html">INT4 Quantization</a></p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="distributed-training">
<h3>Distributed Training<a class="headerlink" href="#distributed-training" title="Permalink to this heading"></a></h3>
<p>To meet demands of large scale model training over multiple devices, distributed training on Intel® GPUs and CPUs are supported. Two alternative methodologies are available. Users can choose either to use PyTorch native distributed training module, <a class="reference external" href="https://pytorch.org/docs/stable/notes/ddp.html">Distributed Data Parallel (DDP)</a>, with <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html">Intel® oneAPI Collective Communications Library (oneCCL)</a> support via <a class="reference external" href="https://github.com/intel/torch-ccl">Intel® oneCCL Bindings for PyTorch (formerly known as torch_ccl)</a> or use Horovod with <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html">Intel® oneAPI Collective Communications Library (oneCCL)</a> support (Experimental).</p>
<p>For more detailed information, check <a class="reference external" href="features/DDP.html">DDP</a> and <a class="reference external" href="features/horovod.html">Horovod (Experimental)</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>
<section id="gpu-specific">
<h2>GPU-Specific<a class="headerlink" href="#gpu-specific" title="Permalink to this heading"></a></h2>
<section id="dlpack-solution">
<h3>DLPack Solution<a class="headerlink" href="#dlpack-solution" title="Permalink to this heading"></a></h3>
<p>DLPack defines a stable in-memory data structure for sharing tensors among frameworks. It enables sharing of tensor data without copying when interoparating with other libraries. Intel® Extension for PyTorch* extends DLPack support in PyTorch* for XPU device particularly.</p>
<p>For more detailed information, check <a class="reference external" href="features/DLPack.html">DLPack Solution</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="dpc-extension">
<h3>DPC++ Extension<a class="headerlink" href="#dpc-extension" title="Permalink to this heading"></a></h3>
<p>Intel® Extension for PyTorch* provides C++ APIs to get SYCL queue and configure floating-point math mode.</p>
<p>Check the <a class="reference external" href="api_doc.html">API Documentation</a> for the details of API functions. <a class="reference external" href="features/DPC++_Extension.html">DPC++ Extension</a> describes how to write customized DPC++ kernels with a practical example and build it with setuptools and CMake.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="advanced-configuration">
<h3>Advanced Configuration<a class="headerlink" href="#advanced-configuration" title="Permalink to this heading"></a></h3>
<p>The default settings for Intel® Extension for PyTorch* are sufficient for most use cases. However, if you need to customize Intel® Extension for PyTorch*, advanced configuration is available at build time and runtime.</p>
<p>For more detailed information, check <a class="reference external" href="features/advanced_configuration.html">Advanced Configuration</a>.</p>
<p>A driver environment variable <cite>ZE_FLAT_DEVICE_HIERARCHY</cite> is currently used to select the device hierarchy model with which the underlying hardware is exposed. By default, each GPU tile is used as a device. Check the <a class="reference external" href="https://spec.oneapi.io/level-zero/latest/core/PROG.html#environment-variables">Level Zero Specification Documentation</a> for more details.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="fully-sharded-data-parallel-fsdp">
<h3>Fully Sharded Data Parallel (FSDP)<a class="headerlink" href="#fully-sharded-data-parallel-fsdp" title="Permalink to this heading"></a></h3>
<p><cite>Fully Sharded Data Parallel (FSDP)</cite> is a PyTorch* module that provides industry-grade solution for large model training. FSDP is a type of data parallel training, unlike DDP, where each process/worker maintains a replica of the model, FSDP shards model parameters, optimizer states and gradients across DDP ranks to reduce the GPU memory footprint used in training. This makes the training of some large-scale models feasible.</p>
<p>For more detailed information, check <a class="reference external" href="features/FSDP.html">FSDP</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="inductor">
<h3>Inductor<a class="headerlink" href="#inductor" title="Permalink to this heading"></a></h3>
<p>Intel® Extension for PyTorch* now empowers users to seamlessly harness graph compilation capabilities for optimal PyTorch model performance on Intel GPU via the flagship <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch-compile">torch.compile</a> API through the default “inductor” backend (<a class="reference external" href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747/1">TorchInductor</a> ).</p>
<p>For more detailed information, check <a class="reference external" href="features/torch_compile_gpu.html">Inductor</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="legacy-profiler-tool-experimental">
<h3>Legacy Profiler Tool (Experimental)<a class="headerlink" href="#legacy-profiler-tool-experimental" title="Permalink to this heading"></a></h3>
<p>The legacy profiler tool is an extension of PyTorch* legacy profiler for profiling operators’ overhead on XPU devices. With this tool, you can get the information in many fields of the run models or code scripts. Build Intel® Extension for PyTorch* with profiler support as default and enable this tool by adding a <cite>with</cite> statement before the code segment.</p>
<p>For more detailed information, check <a class="reference external" href="features/profiler_legacy.html">Legacy Profiler Tool</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="simple-trace-tool-experimental">
<h3>Simple Trace Tool (Experimental)<a class="headerlink" href="#simple-trace-tool-experimental" title="Permalink to this heading"></a></h3>
<p>Simple Trace is a built-in debugging tool that lets you control printing out the call stack for a piece of code. Once enabled, it can automatically print out verbose messages of called operators in a stack format with indenting to distinguish the context.</p>
<p>For more detailed information, check <a class="reference external" href="features/simple_trace.html">Simple Trace Tool</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="kineto-supported-profiler-tool-experimental">
<h3>Kineto Supported Profiler Tool (Experimental)<a class="headerlink" href="#kineto-supported-profiler-tool-experimental" title="Permalink to this heading"></a></h3>
<p>The Kineto supported profiler tool is an extension of PyTorch* profiler for profiling operators’ executing time cost on GPU devices. With this tool, you can get information in many fields of the run models or code scripts. Build Intel® Extension for PyTorch* with Kineto support as default and enable this tool using the <cite>with</cite> statement before the code segment.</p>
<p>For more detailed information, check <a class="reference external" href="features/profiler_kineto.html">Profiler Kineto</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="compute-engine-experimental-feature-for-debug">
<h3>Compute Engine (Experimental feature for debug)<a class="headerlink" href="#compute-engine-experimental-feature-for-debug" title="Permalink to this heading"></a></h3>
<p>Compute engine is a experimental feature which provides the capacity to choose specific backend for operators with multiple implementations.</p>
<p>For more detailed information, check <a class="reference external" href="features/compute_engine.html">Compute Engine</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>
<section id="cpu-specific">
<h2>CPU-Specific<a class="headerlink" href="#cpu-specific" title="Permalink to this heading"></a></h2>
<section id="operator-optimization">
<h3>Operator Optimization<a class="headerlink" href="#operator-optimization" title="Permalink to this heading"></a></h3>
<p>Intel® Extension for PyTorch* also optimizes operators and implements several customized operators for performance boosts. A few ATen operators are replaced by their optimized counterparts in Intel® Extension for PyTorch* via the ATen registration mechanism. Some customized operators are implemented for several popular topologies. For instance, ROIAlign and NMS are defined in Mask R-CNN. To improve performance of these topologies, Intel® Extension for PyTorch* also optimized these customized operators.</p>
<dl class="py class">
<dt class="sig sig-object py" id="intel_extension_for_pytorch.nn.FrozenBatchNorm2d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">intel_extension_for_pytorch.nn.</span></span><span class="sig-name descname"><span class="pre">FrozenBatchNorm2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#intel_extension_for_pytorch.nn.FrozenBatchNorm2d" title="Permalink to this definition"></a></dt>
<dd><p>BatchNorm2d where the batch statistics and the affine parameters are fixed</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>num_features</strong> (<em>int</em>) – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p>
</dd>
</dl>
<dl class="simple">
<dt>Shape</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="intel_extension_for_pytorch.nn.functional.interaction">
<span class="sig-prename descclassname"><span class="pre">intel_extension_for_pytorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">interaction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#intel_extension_for_pytorch.nn.functional.interaction" title="Permalink to this definition"></a></dt>
<dd><p>Get the interaction feature beyond different kinds of features (like gender
or hobbies), used in DLRM model.</p>
<p>For now, we only optimized “dot” interaction at <a class="reference external" href="https://github.com/facebookresearch/dlrm/blob/main/dlrm_s_pytorch.py#L475-L495">DLRM Github repo</a>.
Through this, we use the dot product to represent the interaction feature
between two features.</p>
<p>For example, if feature 1 is “Man” which is represented by [0.1, 0.2, 0.3],
and feature 2 is “Like play football” which is represented by [-0.1, 0.3, 0.2].</p>
<p>The dot interaction feature is
([0.1, 0.2, 0.3] * [-0.1, 0.3, 0.2]^T) =  -0.1 + 0.6 + 0.6 = 1.1</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>*args</strong> – Multiple tensors which represent different features</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Input: <span class="math notranslate nohighlight">\(N * (B, D)\)</span>, where N is the number of different kinds of features,</dt><dd><p>B is the batch size, D is feature size</p>
</dd>
</dl>
</li>
<li><p>Output: <span class="math notranslate nohighlight">\((B, D + N * ( N - 1 ) / 2)\)</span></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p><strong>Auto kernel selection</strong> is a feature that enables users to tune for better performance with GEMM operations. It is provided as parameter –auto_kernel_selection, with boolean value, of the ipex.optimize() function. By default, the GEMM kernel is computed with oneMKL primitives. However, under certain circumstances oneDNN primitives run faster. Users are able to set –auto_kernel_selection to True to run GEMM kernels with oneDNN primitives.” -&gt; “We aim to provide good default performance by leveraging the best of math libraries and enabled weights_prepack, and it has been verified with broad set of models. If you would like to try other alternatives, you can use auto_kernel_selection toggle in ipex.optimize to switch, and you can disable weights_preack in ipex.optimize if you are concerning the memory footprint more than performance gain. However in majority cases, keeping default is what we recommend.</p>
</section>
<section id="runtime-extension">
<h3>Runtime Extension<a class="headerlink" href="#runtime-extension" title="Permalink to this heading"></a></h3>
<p>Intel® Extension for PyTorch* Runtime Extension provides PyTorch frontend APIs for users to get finer-grained control of the thread runtime and provides:</p>
<ul class="simple">
<li><p>Multi-stream inference via the Python frontend module MultiStreamModule.</p></li>
<li><p>Spawn asynchronous tasks from both Python and C++ frontend.</p></li>
<li><p>Program core bindings for OpenMP threads from both Python and C++ frontend.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Intel® Extension for PyTorch* Runtime extension is still in the experimental stage. The API is subject to change. More detailed descriptions are available in the <a class="reference external" href="api_doc.html">API Documentation</a>.</p>
</div>
<p>For more detailed information, check <a class="reference external" href="features/runtime_extension.html">Runtime Extension</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="codeless-optimization-experimental-new-feature-in-1-13">
<h3>Codeless Optimization (Experimental, <em>NEW feature in 1.13.*</em>)<a class="headerlink" href="#codeless-optimization-experimental-new-feature-in-1-13" title="Permalink to this heading"></a></h3>
<p>This feature enables users to get performance benefits from Intel® Extension for PyTorch* without changing Python scripts. It hopefully eases the usage and has been verified working well with broad scope of models, though in few cases there could be small overhead comparing to applying optimizations with Intel® Extension for PyTorch* APIs.</p>
<p>For more detailed information, check <a class="reference external" href="features/codeless_optimization.html">Codeless Optimization</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="graph-capture-experimental-new-feature-in-1-13-0">
<h3>Graph Capture (Experimental, <em>NEW feature in 1.13.0*</em>)<a class="headerlink" href="#graph-capture-experimental-new-feature-in-1-13-0" title="Permalink to this heading"></a></h3>
<p>Since graph mode is key for deployment performance, this feature automatically captures graphs based on set of technologies that PyTorch supports, such as TorchScript and TorchDynamo. Users won’t need to learn and try different PyTorch APIs to capture graphs, instead, they can turn on a new boolean flag <cite>–graph_mode</cite> (default off) in <cite>ipex.optimize</cite> to get the best of graph optimization.</p>
<p>For more detailed information, check <a class="reference external" href="features/graph_capture.html">Graph Capture</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="hypertune-experimental-new-feature-in-1-13-0">
<h3>HyperTune (Experimental, <em>NEW feature in 1.13.0*</em>)<a class="headerlink" href="#hypertune-experimental-new-feature-in-1-13-0" title="Permalink to this heading"></a></h3>
<p>HyperTune is an experimental feature to perform hyperparameter/execution configuration searching. The searching is used in various areas such as optimization of hyperparameters of deep learning models. The searching is extremely useful in real situations when the number of hyperparameters, including configuration of script execution, and their search spaces are huge that manually tuning these hyperparameters/configuration is impractical and time consuming. Hypertune automates this process of execution configuration searching for the <a class="reference external" href="performance_tuning/launch_script.html">launcher</a> and Intel® Extension for PyTorch*.</p>
<p>For more detailed information, check <a class="reference external" href="features/hypertune.html">HyperTune</a>.</p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="features/nhwc.html" class="btn btn-neutral float-right" title="Channels Last" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f68d58cbb80> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>