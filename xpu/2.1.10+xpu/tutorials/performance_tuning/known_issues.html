<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Troubleshooting &mdash; Intel&amp;#174 Extension for PyTorch* 2.1.10+xpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Blogs &amp; Publications" href="../blogs_publications.html" />
    <link rel="prev" title="Releases" href="../releases.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../../">2.1.10+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Troubleshooting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#gpu-specific-issues">GPU-specific Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#general-usage">General Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#library-dependencies">Library Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unit-test">Unit Test</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cpu-specific-issues">CPU-specific issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">General Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#torchdynamo">TorchDynamo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-shape">Dynamic Shape</a></li>
<li class="toctree-l3"><a class="reference internal" href="#int8">INT8</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bfloat16">BFloat16</a></li>
<li class="toctree-l3"><a class="reference internal" href="#runtime-extension">Runtime Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#result-correctness">Result Correctness</a></li>
<li class="toctree-l3"><a class="reference internal" href="#float32-training">Float32 Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Troubleshooting</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/known_issues.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="troubleshooting">
<h1>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this heading"></a></h1>
<section id="gpu-specific-issues">
<h2>GPU-specific Issues<a class="headerlink" href="#gpu-specific-issues" title="Permalink to this heading"></a></h2>
<section id="general-usage">
<h3>General Usage<a class="headerlink" href="#general-usage" title="Permalink to this heading"></a></h3>
<ul>
<li><p><strong>Problem</strong>: FP64 data type is unsupported on current platform.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: FP64 is not natively supported by the <a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/data-center-gpu/flex-series/overview.html">Intel® Data Center GPU Flex Series</a> platform.
If you run any AI workload on that platform and receive this error message, it means a kernel requires FP64 instructions that are not supported and the execution is stopped.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Runtime error <code class="docutils literal notranslate"><span class="pre">invalid</span> <span class="pre">device</span> <span class="pre">pointer</span></code> if <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">horovod.torch</span> <span class="pre">as</span> <span class="pre">hvd</span></code> before <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code></p>
<ul class="simple">
<li><p><strong>Cause</strong>: Intel® Optimization for Horovod* uses utilities provided by Intel® Extension for PyTorch*. The improper import order causes Intel® Extension for PyTorch* to be unloaded before Intel®
Optimization for Horovod* at the end of the execution and triggers this error.</p></li>
<li><p><strong>Solution</strong>: Do <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code> before <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">horovod.torch</span> <span class="pre">as</span> <span class="pre">hvd</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Number of dpcpp devices should be greater than zero.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: If you use Intel® Extension for PyTorch* in a conda environment, you might encounter this error. Conda also ships the libstdc++.so dynamic library file that may conflict with the one shipped
in the OS.</p></li>
<li><p><strong>Solution</strong>: Export the <code class="docutils literal notranslate"><span class="pre">libstdc++.so</span></code> file path in the OS to an environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Symbol undefined caused by <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ImportError:<span class="w"> </span>undefined<span class="w"> </span>symbol:<span class="w"> </span>_ZNK5torch8autograd4Node4nameB5cxx11Ev
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Cause</strong>: DPC++ does not support <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>, Intel® Extension for PyTorch* is always compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. This symbol undefined issue appears when PyTorch* is
compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>.</p></li>
<li><p><strong>Solution</strong>: Pass <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLIBCXX_USE_CXX11_ABI=1</span></code> and compile PyTorch* with particular compiler which supports <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. We recommend using prebuilt wheels
in [download server](https:// developer.intel.com/ipex-whl-stable-xpu) to avoid this issue.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Bad termination after AI model execution finishes when using Intel MPI.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: This is a random issue when the AI model (e.g. RN50 training) execution finishes in an Intel MPI environment. It is not user-friendly as the model execution ends ungracefully.</p></li>
<li><p><strong>Solution</strong>: Add <code class="docutils literal notranslate"><span class="pre">dist.destroy_process_group()</span></code> during the cleanup stage in the model script, as described
in <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a>.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: <code class="docutils literal notranslate"><span class="pre">-997</span> <span class="pre">runtime</span> <span class="pre">error</span></code> when running some AI models on Intel® Arc™ A-Series GPUs.</p>
<ul class="simple">
<li><p><strong>Cause</strong>:  Some of the <code class="docutils literal notranslate"><span class="pre">-997</span> <span class="pre">runtime</span> <span class="pre">error</span></code> are actually out-of-memory errors. As Intel® Arc™ A-Series GPUs have less device memory than Intel® Data Center GPU Flex Series 170 and Intel® Data Center GPU
Max  Series, running some AI models on them may trigger out-of-memory errors and cause them to report failure such as <code class="docutils literal notranslate"><span class="pre">-997</span> <span class="pre">runtime</span> <span class="pre">error</span></code> most likely. This is expected. Memory usage optimization is a work in progress to allow Intel® Arc™ A-Series GPUs to support more AI models.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Building from source for Intel® Arc™ A-Series GPUs fails on WSL2 without any error thrown.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: Your system probably does not have enough RAM, so Linux kernel’s Out-of-memory killer was invoked. You can verify this by running <code class="docutils literal notranslate"><span class="pre">dmesg</span></code> on bash (WSL2 terminal).</p></li>
<li><p><strong>Solution</strong>: If the OOM killer had indeed killed the build process, then you can try increasing the swap-size of WSL2, and/or decreasing the number of parallel build jobs with the environment
variable <code class="docutils literal notranslate"><span class="pre">MAX_JOBS</span></code> (by default, it’s equal to the number of logical CPU cores. So, setting <code class="docutils literal notranslate"><span class="pre">MAX_JOBS</span></code> to 1 is a very conservative approach that would slow things down a lot).</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Some workloads terminate with an error <code class="docutils literal notranslate"><span class="pre">CL_DEVICE_NOT_FOUND</span></code> after some time on WSL2.</p>
<ul class="simple">
<li><p><strong>Cause</strong>:  This issue is due to the <a class="reference external" href="https://learn.microsoft.com/en-us/windows-hardware/drivers/display/tdr-registry-keys#tdrdelay">TDR feature</a> on Windows.</p></li>
<li><p><strong>Solution</strong>: Try increasing TDRDelay in your Windows Registry to a large value, such as 20 (it is 2 seconds, by default), and reboot.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Runtime error <code class="docutils literal notranslate"><span class="pre">Unable</span> <span class="pre">to</span> <span class="pre">find</span> <span class="pre">TSan</span> <span class="pre">function</span></code> might be raised when running some CPU AI workloads in certain scenarios.</p>
<ul class="simple">
<li><p><strong>Cause</strong>:  This issue is probably caused by the compatibility issue of OMP tool libraries.</p></li>
<li><p><strong>Solution</strong>: Please try the workaround: disable OMP tool libraries by <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMP_TOOL=&quot;disabled&quot;</span></code>, to unblock your workload. We are working on the final solution and will release it as soon as possible.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: The profiled data on GPU operators using legacy profiler is not accurate sometimes.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: Compiler in 2024.0 oneAPI basekit optimizes barrier implementation which brings negative impact on legacy profiler.</p></li>
<li><p><strong>Solution</strong>: Use Kineto profiler instead. Or use legacy profiler with <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">UR_L0_IN_ORDER_BARRIER_BY_SIGNAL=0</span></code> to workaround this issue.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Random bad termination after AI model convergence test (&gt;24 hours) finishes.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: This is a random issue when some AI model convergence test execution finishes. It is not user-friendly as the model execution ends ungracefully.</p></li>
<li><p><strong>Solution</strong>: Kill the process after the convergence test finished, or use checkpoints to divide the convergence test into several phases and execute separately.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Random GPU hang issue when executing the first allreduce in LLM inference workloads on 1 Intel® Data Center GPU Max 1550 card.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: Race condition happens between oneDNN kernels and oneCCL Bindings for Pytorch* allreduce primitive.</p></li>
<li><p><strong>Solution</strong>: Use <code class="docutils literal notranslate"><span class="pre">TORCH_LLM_ALLREDUCE=0</span></code> to workaround this issue.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: GPU hang issue when executing LLM inference workloads on multi Intel® Data Center GPU Max series cards over PCIe communication.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: oneCCL Bindings for Pytorch* allreduce primitive does not support PCIe for cross-cards communication.</p></li>
<li><p><strong>Solution</strong>: Enable XeLink for cross-cards communication, or use <code class="docutils literal notranslate"><span class="pre">TORCH_LLM_ALLREDUCE=0</span></code> for the PCIe only environments.</p></li>
</ul>
</li>
</ul>
</section>
<section id="library-dependencies">
<h3>Library Dependencies<a class="headerlink" href="#library-dependencies" title="Permalink to this heading"></a></h3>
<ul>
<li><p><strong>Problem</strong>: Cannot find oneMKL library when building Intel® Extension for PyTorch* without oneMKL.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_sycl
/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_intel_ilp64
/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_core
/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_tbb_thread
dpcpp:<span class="w"> </span>error:<span class="w"> </span>linker<span class="w"> </span><span class="nb">command</span><span class="w"> </span>failed<span class="w"> </span>with<span class="w"> </span><span class="nb">exit</span><span class="w"> </span>code<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">(</span>use<span class="w"> </span>-v<span class="w"> </span>to<span class="w"> </span>see<span class="w"> </span>invocation<span class="o">)</span>
</pre></div>
</div>
<ul>
<li><p><strong>Cause</strong>: When PyTorch* is built with oneMKL library and Intel® Extension for PyTorch* is built without MKL library, this linker issue may occur.</p></li>
<li><p><strong>Solution</strong>: Resolve the issue by setting:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">USE_ONEMKL</span><span class="o">=</span>OFF
<span class="nb">export</span><span class="w"> </span><span class="nv">MKL_DPCPP_ROOT</span><span class="o">=</span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/intel/oneapi/mkl/latest
</pre></div>
</div>
</li>
</ul>
<p>Then clean build Intel® Extension for PyTorch*.</p>
</li>
<li><p><strong>Problem</strong>: Undefined symbol: <code class="docutils literal notranslate"><span class="pre">mkl_lapack_dspevd</span></code>. Intel MKL FATAL ERROR: cannot load <code class="docutils literal notranslate"><span class="pre">libmkl_vml_avx512.so.2</span></code> or `libmkl_vml_def.so.2.</p>
<ul>
<li><p><strong>Cause</strong>: This issue may occur when Intel® Extension for PyTorch* is built with oneMKL library and PyTorch* is not build with any MKL library. The oneMKL kernel may run into CPU backend incorrectly
and trigger this issue.</p></li>
<li><p><strong>Solution</strong>: Resolve the issue by installing the oneMKL library from conda:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>mkl
conda<span class="w"> </span>install<span class="w"> </span>mkl-include
</pre></div>
</div>
</li>
</ul>
<p>Then clean build PyTorch*.</p>
</li>
<li><p><strong>Problem</strong>: OSError: <code class="docutils literal notranslate"><span class="pre">libmkl_intel_lp64.so.2</span></code>: cannot open shared object file: No such file or directory.</p>
<ul>
<li><p><strong>Cause</strong>: Wrong MKL library is used when multiple MKL libraries exist in system.</p></li>
<li><p><strong>Solution</strong>: Preload oneMKL by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_lp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_ilp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_gnu_thread.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_core.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_sycl.so.2
</pre></div>
</div>
<p>If you continue seeing similar issues for other shared object files, add the corresponding files under <code class="docutils literal notranslate"><span class="pre">${MKL_DPCPP_ROOT}/lib/intel64/</span></code> by <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>. Note that the suffix of the libraries may change (e.g. from .1 to .2), if more than one oneMKL library is installed on the system.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="unit-test">
<h3>Unit Test<a class="headerlink" href="#unit-test" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Unit test failures on Intel® Data Center GPU Flex Series 170</p>
<p>The following unit test fails on Intel® Data Center GPU Flex Series 170 but the same test case passes on Intel® Data Center GPU Max Series. The root cause of the failure is under investigation.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_weight_norm.py::TestNNMethod::test_weight_norm_differnt_type</span></code></p></li>
</ul>
<p>The following unit tests fail in Windows environment on Intel® Arc™ A770 Graphic card. The root cause of the failures is under investigation.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_foreach.py::TestTorchMethod::test_foreach_cos</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_foreach.py::TestTorchMethod::test_foreach_sin</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_polar.py::TestTorchMethod::test_polar_float</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_special_ops.py::TestTorchMethod::test_special_spherical_bessel_j0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_transducer_loss.py::TestNNMethod::test_vallina_transducer_loss</span></code></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="cpu-specific-issues">
<h2>CPU-specific issues<a class="headerlink" href="#cpu-specific-issues" title="Permalink to this heading"></a></h2>
<section id="id1">
<h3>General Usage<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<ul>
<li><p><strong>Problem</strong>: Issues with the <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> PyTorch package.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: Certain Python packages may have PyTorch as a hard dependency. If you installed the <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> version of PyTorch, installation of these packages might replace the <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> version with the default version released on Pypi.org.</p></li>
<li><p><strong>Solution</strong>: Reinstall the <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> version back.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: The workload running with Intel® Extension for PyTorch* occupies a remarkably large amount of memory.</p>
<ul class="simple">
<li><p><strong>Solution</strong>: Try to reduce the occupied memory size by setting the <code class="docutils literal notranslate"><span class="pre">--weights_prepack</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: The <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding feature of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function does not work if inference is done with a custom function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_pytorch_extension</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="k">class</span> <span class="nc">Module</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Module</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">m</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Cause</strong>: PyTorch FX limitation.</p></li>
<li><p><strong>Solution</strong>: You can avoid this error by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O0&quot;)</span></code>, which doesn’t apply ipex optimization, or disable <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O1&quot;,</span> <span class="pre">conv_bn_folding=False)</span></code>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="torchdynamo">
<h3>TorchDynamo<a class="headerlink" href="#torchdynamo" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Problem</strong>: A workload that uses <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> fails to run or demonstrates poor performance.</p>
<ul>
<li><p><strong>Cause</strong>: The support of <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> with <code class="docutils literal notranslate"><span class="pre">ipex</span></code> as the backend is still an experimental feature. Currently, the following HuggingFace models fail to run using <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> with <code class="docutils literal notranslate"><span class="pre">ipex</span></code> backend due to memory issues:</p>
<ul>
<li><p>masked-language-modeling+xlm-roberta-base</p></li>
<li><p>casual-language-modeling+gpt2</p></li>
<li><p>casual-language-modeling+xlm-roberta-base</p></li>
<li><p>summarization+t5-base</p></li>
<li><p>text-classification+allenai-longformer-base-409</p></li>
</ul>
</li>
<li><p><strong>Solution</strong>: Use the <code class="docutils literal notranslate"><span class="pre">torch.jit</span></code> APIs and graph optimization APIs of the Intel® Extension for PyTorch*.</p></li>
</ul>
</li>
</ul>
</section>
<section id="dynamic-shape">
<h3>Dynamic Shape<a class="headerlink" href="#dynamic-shape" title="Permalink to this heading"></a></h3>
<ul>
<li><p><strong>Problem</strong>: When working with an NLP model inference with dynamic input data length using TorchScript (either <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code>), performance with Intel® Extension for PyTorch* may be less than that without Intel®
Extension for PyTorch*.</p>
<ul>
<li><p><strong>Solution</strong>: Use the workaround below:</p>
<ul>
<li><p>Python interface</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_texpr_fuser_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>C++ interface</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/jit/passes/tensorexpr_fuser.h&gt;</span>
<span class="n">torch</span><span class="o">::</span><span class="n">jit</span><span class="o">::</span><span class="n">setTensorExprFuserEnabled</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="int8">
<h3>INT8<a class="headerlink" href="#int8" title="Permalink to this heading"></a></h3>
<ul>
<li><p><strong>Problem</strong>: Low performance with INT8 support for dynamic shapes.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the
Intel® Extension for PyTorch* INT8 path may slow down the model inference.</p></li>
<li><p><strong>Solution</strong>: Use stock PyTorch INT8 functionality.
<strong>Note</strong>: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issue.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Overflow</span> <span class="pre">when</span> <span class="pre">unpacking</span> <span class="pre">long</span></code> when a tensor’s min max value exceeds int range while performing int8 calibration.</p>
<ul class="simple">
<li><p><strong>Solution</strong>: Customize QConfig to use min-max calibration method.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Incorrect results with large tensors when calibrating with <code class="docutils literal notranslate"><span class="pre">quantize_per_tensor</span></code>, when benchmarking with 1 OpenMP* thread (find more detailed info <a class="reference external" href="https://github.com/pytorch/pytorch/issues/80501">here</a>.</p>
<ul>
<li><p><strong>Solution</strong>: Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREAEDS=1</span></code> for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is used.</p>
<p>Workaround pseudocode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># perform convert/trace/freeze with omp_num_threads &gt; 1(N)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">converted_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">freezed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># run freezed model to apply optimization pass</span>
<span class="n">freezed_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># benchmarking with omp_num_threads = 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">run_benchmark</span><span class="p">(</span><span class="n">freezed_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>For models with dynamic control flow, please try dynamic quantization. Users are likely to get performance gain for GEMM models.</p></li>
<li><p>Support for <code class="docutils literal notranslate"><span class="pre">EmbeddingBag</span></code> with INT8 when bag size &gt; 1 is work in progress.</p></li>
</ul>
</section>
<section id="bfloat16">
<h3>BFloat16<a class="headerlink" href="#bfloat16" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains <code class="docutils literal notranslate"><span class="pre">Conv</span></code>, <code class="docutils literal notranslate"><span class="pre">Matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, and <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code></p></li>
</ul>
</section>
<section id="runtime-extension">
<h3>Runtime Extension<a class="headerlink" href="#runtime-extension" title="Permalink to this heading"></a></h3>
<p>The following limitations currently exist:</p>
<ul class="simple">
<li><p>Runtime extension of <code class="docutils literal notranslate"><span class="pre">MultiStreamModule</span></code> does not support DLRM inference, since the input of DLRM (EmbeddingBag specifically) cannot be simply batch split.</p></li>
<li><p>Runtime extension of <code class="docutils literal notranslate"><span class="pre">MultiStreamModule</span></code> has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (<code class="docutils literal notranslate"><span class="pre">joint_net</span></code> specifically) can be jit traced into graph. However, in one batch inference, <code class="docutils literal notranslate"><span class="pre">joint_net</span></code> is invoked multiple times.
It increases the   overhead of <code class="docutils literal notranslate"><span class="pre">MultiStreamModule</span></code> as input batch split, thread synchronization and output concat.</p></li>
</ul>
</section>
<section id="result-correctness">
<h3>Result Correctness<a class="headerlink" href="#result-correctness" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Problem</strong>: Incorrect Conv and Linear result if the number of OMP threads is changed at runtime.</p>
<ul>
<li><p><strong>Cause</strong>: The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.</p></li>
</ul>
</li>
</ul>
</section>
<section id="float32-training">
<h3>Float32 Training<a class="headerlink" href="#float32-training" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Problem</strong>: Low throughput with DLRM FP32 Train.</p>
<ul>
<li><p><strong>Solution</strong>: A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending on review. The issue will be fixed when the PR is merged.</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../releases.html" class="btn btn-neutral float-left" title="Releases" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../blogs_publications.html" class="btn btn-neutral float-right" title="Blogs &amp; Publications" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fc02a285880> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>