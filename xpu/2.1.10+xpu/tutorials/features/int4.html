<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>INT4 inference [GPU] (Experimental) &mdash; Intel&amp;#174 Extension for PyTorch* 2.1.10+xpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Float8 Data Type Support [GPU] (Experimental)" href="float8.html" />
    <link rel="prev" title="Intel® Extension for PyTorch* Optimizations for Quantization [GPU]" href="int8_overview_xpu.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../../">2.1.10+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#device-agnostic">Device-Agnostic</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../features.html#easy-to-use-python-api">Easy-to-use Python API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../features.html#channels-last">Channels Last</a></li>
<li class="toctree-l3"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../features.html#quantization">Quantization</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="int8_overview.html">Intel® Extension for PyTorch* optimizations for quantization [CPU]</a></li>
<li class="toctree-l4"><a class="reference internal" href="int8_recipe_tuning_api.html">INT8 Recipe Tuning API (Experimental) [CPU]</a></li>
<li class="toctree-l4"><a class="reference internal" href="int8_overview_xpu.html">Intel® Extension for PyTorch* Optimizations for Quantization [GPU]</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">INT4 inference [GPU] (Experimental)</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#int4-data-type">INT4 Data Type</a></li>
<li class="toctree-l5"><a class="reference internal" href="#int4-quantization">INT4 Quantization</a></li>
<li class="toctree-l5"><a class="reference internal" href="#supported-running-mode">Supported running mode</a></li>
<li class="toctree-l5"><a class="reference internal" href="#supported-operators">Supported operators</a></li>
<li class="toctree-l5"><a class="reference internal" href="#int4-usage-example">INT4 usage example</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#weight-only-quantization-tool">Weight Only Quantization Tool</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="float8.html">Float8 Data Type Support [GPU] (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../features.html#distributed-training">Distributed Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#gpu-specific">GPU-Specific</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#cpu-specific">CPU-Specific</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../features.html">Features</a></li>
      <li class="breadcrumb-item active">INT4 inference [GPU] (Experimental)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/int4.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="int4-inference-gpu-experimental">
<h1>INT4 inference [GPU] (Experimental)<a class="headerlink" href="#int4-inference-gpu-experimental" title="Permalink to this heading"></a></h1>
<section id="int4-data-type">
<h2>INT4 Data Type<a class="headerlink" href="#int4-data-type" title="Permalink to this heading"></a></h2>
<p>INT4 is a 4-bit fixed point data type, which is used to reduce memory footprint, improve the computation efficiency, and save power in Deep Learning domain.</p>
<p>INT4 data type is being used in weight only quantization in current stage. It will be converted to Float16 data type for computation.</p>
</section>
<section id="int4-quantization">
<h2>INT4 Quantization<a class="headerlink" href="#int4-quantization" title="Permalink to this heading"></a></h2>
<p>On GPU, offline Weight Only Quantization (WOQ) is used for INT4 data compression. WOQ calibration tool using Generative Pre-trained Transformer models Quantization (GPT-Q) algorithm is created for improving the accuracy for INT4 weight quantization.</p>
</section>
<section id="supported-running-mode">
<h2>Supported running mode<a class="headerlink" href="#supported-running-mode" title="Permalink to this heading"></a></h2>
<p>DNN Inference is supported with INT4 data type.</p>
</section>
<section id="supported-operators">
<h2>Supported operators<a class="headerlink" href="#supported-operators" title="Permalink to this heading"></a></h2>
<p>INT4 Linear operator and widely used linear fusion operators in Large Langugue Models like <code class="docutils literal notranslate"><span class="pre">mm_qkv_int4</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_bias_int4</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_silu_int4</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_resmul_int4</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_bias_gelu_int4</span></code>, <code class="docutils literal notranslate"><span class="pre">mm_bias_resadd_resadd_int4</span></code> are supported.</p>
</section>
<section id="int4-usage-example">
<h2>INT4 usage example<a class="headerlink" href="#int4-usage-example" title="Permalink to this heading"></a></h2>
<p>You can use a well quantized INT4 model to perform INT4 inference directly, or use the WOQ tool to compress the high precision model to INT4 model firstly, then to execute INT4 inference with IPEX on GPU.</p>
<section id="weight-only-quantization-tool">
<h3>Weight Only Quantization Tool<a class="headerlink" href="#weight-only-quantization-tool" title="Permalink to this heading"></a></h3>
<p>This tool is used for applying quantization to the given model using gptq method.</p>
<p>Please note that we only support HuggingFace transformers model structure at present. GPT-J-6B is a model we intensively verified.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTJForCausalLM</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTJForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">_gptq</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="s1">&#39;quantized_weight.pt&#39;</span><span class="p">,</span> <span class="n">wbits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="int8_overview_xpu.html" class="btn btn-neutral float-left" title="Intel® Extension for PyTorch* Optimizations for Quantization [GPU]" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="float8.html" class="btn btn-neutral float-right" title="Float8 Data Type Support [GPU] (Experimental)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f68d5d99c70> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>