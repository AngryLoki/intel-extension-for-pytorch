<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Known Issues &mdash; intel_extension_for_pytorch 1.13.120+xpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Technical Details" href="../technical_details.html" />
    <link rel="prev" title="TorchServe with Intel® Extension for PyTorch*" href="torchserve.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../../">1.13.120+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Known Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#known-issues-specific-to-gpu">Known Issues Specific to GPU</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dependency-libraries">Dependency Libraries</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unittest">UnitTest</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#known-issues-specific-to-cpu">Known Issues Specific to CPU</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#torchdynamo">TorchDynamo</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-shape">Dynamic Shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="#int8">INT8</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bfloat16">BFloat16</a></li>
<li class="toctree-l4"><a class="reference internal" href="#runtime-extension">Runtime Extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="#correctness">Correctness</a></li>
<li class="toctree-l4"><a class="reference internal" href="#float32-training">Float32 Training</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../performance_tuning.html">Performance Tuning Guide</a> &raquo;</li>
      <li>Known Issues</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/known_issues.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="known-issues">
<h1>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading"></a></h1>
<section id="known-issues-specific-to-gpu">
<h2>Known Issues Specific to GPU<a class="headerlink" href="#known-issues-specific-to-gpu" title="Permalink to this heading"></a></h2>
<section id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this heading"></a></h3>
<ul>
<li><p>FP64 data type is unsupported on current platform</p>
<p>FP64 is not natively supported by the <a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/data-center-gpu/flex-series/overview.html">Intel® Data Center GPU Flex Series</a> platform. If you run any AI workload on that platform and receive this error message, it means a kernel requiring FP64 instructions but not supported and the execution is stopped.</p>
</li>
<li><p>MaxPool2d operator only supports 4D input for ceil mode</p>
<p>If 3D input is detected, MaxPool2d will throw unsupported error message and stop execution.</p>
</li>
<li><p>Runtime error <code class="docutils literal notranslate"><span class="pre">invalid</span> <span class="pre">device</span> <span class="pre">pointer</span></code> if <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">horovod.torch</span> <span class="pre">as</span> <span class="pre">hvd</span></code> before <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code></p>
<p>Intel® Optimization for Horovod* need use utilities provided by Intel® Extension for PyTorch*. The improper import order will cause Intel® Extension for PyTorch* be unloaded before Intel® Optimization for Horovod* at the end of the execution and trigger this error. The recommended usage is to <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code> before <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">horovod.torch</span> <span class="pre">as</span> <span class="pre">hvd</span></code>.</p>
</li>
<li><p>RuntimeError: Number of dpcpp devices should be greater than zero!</p>
<ul class="simple">
<li><p>Scenario 1: Running some AI models (e.g. 3D-Unet inference) on Ubuntu22.04 may trigger this runtime error, as oneAPI Base Toolkit 2023.1 fails to return available GPU device on ubuntu22.04 in such scenario. The workaround solution is to update the model script to make sure <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch</span></code> and <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code> happen before importing other libraries.</p></li>
<li><p>Scenario 2: If you use Intel® Extension for PyTorch*  in a conda environment, this error might occur. Conda also ships with a libstdc++.so dynamic library file. It may conflict with the one shipped in the OS. Exporting the libstdc++.so file path in OS to an environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> could workaround this issue.</p></li>
</ul>
</li>
<li><p>symbol undefined caused by <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ImportError: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev
</pre></div>
</div>
<p>DPC++ does not support <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>, Intel® Extension for PyTorch* is always compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. This symbol undefined issue appears when PyTorch* is compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>. Pass <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLIBCXX_USE_CXX11_ABI=1</span></code> and compile PyTorch* with particular compiler which supports <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. We recommend using prebuilt wheels in <a class="reference external" href="https://pytorch-extension.intel.com/release-whl/stable/xpu/us/">download server</a> to avoid this issue.</p>
</li>
</ul>
</section>
<section id="dependency-libraries">
<h3>Dependency Libraries<a class="headerlink" href="#dependency-libraries" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Can’t find oneMKL library when build Intel® Extension for PyTorch* without oneMKL</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/bin/ld: cannot find -lmkl_sycl
/usr/bin/ld: cannot find -lmkl_intel_ilp64
/usr/bin/ld: cannot find -lmkl_core
/usr/bin/ld: cannot find -lmkl_tbb_thread
dpcpp: error: linker <span class="nb">command</span> failed with <span class="nb">exit</span> code <span class="m">1</span> <span class="o">(</span>use -v to see invocation<span class="o">)</span>
</pre></div>
</div>
<p>When PyTorch* is built with oneMKL library and Intel® Extension for PyTorch* is built without oneMKL library, this linker issue may occur. Resolve it by setting:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">USE_ONEMKL</span><span class="o">=</span>OFF
<span class="nb">export</span> <span class="nv">MKL_DPCPP_ROOT</span><span class="o">=</span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/intel/oneapi/mkl/latest
</pre></div>
</div>
<p>Then clean build Intel® Extension for PyTorch*.</p>
</li>
<li><p>undefined symbol: <code class="docutils literal notranslate"><span class="pre">mkl_lapack_dspevd</span></code>. Intel MKL FATAL ERROR: cannot load <code class="docutils literal notranslate"><span class="pre">libmkl_vml_avx512.so.2</span></code> or <code class="docutils literal notranslate"><span class="pre">libmkl_vml_def.so.2</span></code></p>
<p>This issue may occur when Intel® Extension for PyTorch* is built with oneMKL library and PyTorch* is not build with any MKL library. The oneMKL kernel may run into CPU backend incorrectly and trigger this issue. Resolve it by installing MKL library from conda:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda install mkl
conda install mkl-include
</pre></div>
</div>
<p>then clean build PyTorch*.</p>
</li>
<li><p>OSError: <code class="docutils literal notranslate"><span class="pre">libmkl_intel_lp64.so.2</span></code>: cannot open shared object file: No such file or directory</p>
<p>Wrong MKL library is used when multiple MKL libraries exist in system. Preload oneMKL by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_lp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_ilp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_gnu_thread.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_core.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_sycl.so.2
</pre></div>
</div>
<p>If you continue seeing similar issues for other shared object files, add the corresponding files under <code class="docutils literal notranslate"><span class="pre">${MKL_DPCPP_ROOT}/lib/intel64/</span></code> by <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>. Note that the suffix of the libraries may change (e.g. from .1 to .2), if more than one oneMKL library is installed on the system.</p>
</li>
<li><p>OpenMP library could not be found</p>
<p>Build Intel® Extension for PyTorch* on SLES15 SP3 using default GCC 7.5 and CentOS8 using default GCC 8.5 may trigger this build error.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Make Error at third_party/ideep/mkl-dnn/third_party/oneDNN/cmake/OpenMP.cmake:118 <span class="o">(</span>message<span class="o">)</span>:
  OpenMP library could not be found.  Proceeding might lead to highly
  sub-optimal performance.
Call Stack <span class="o">(</span>most recent call first<span class="o">)</span>:
  third_party/ideep/mkl-dnn/third_party/oneDNN/CMakeLists.txt:117 <span class="o">(</span>include<span class="o">)</span>
</pre></div>
</div>
<p>The root cause is GCC 7.5 or 8.5 does not support <code class="docutils literal notranslate"><span class="pre">-Wno-error=redundant-move</span></code> option. Uplift to GCC version &gt;=9 can solve this issue.</p>
</li>
</ul>
</section>
<section id="unittest">
<h3>UnitTest<a class="headerlink" href="#unittest" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Unit test failures on Intel® Data Center GPU Flex Series 170</p>
<p>The following unit tests fail on Intel® Data Center GPU Flex Series 170.</p>
<ul class="simple">
<li><p>test_linalg.py::TestTorchMethod::test_tensorinv_empty</p></li>
<li><p>test_distributions.py::TestDistributions::test_dirichlet_mean_var</p></li>
<li><p>test_adaptive_avg_pool2d.py::TestNNMethod::test_adaptive_avg_pool2d</p></li>
<li><p>test_multilabel_margin_loss.py::TestNNMethod::test_multiabel_margin_loss</p></li>
</ul>
<p>The same test cases pass on Intel® Data Center GPU Max Series. The root cause of the failures is under investigation.</p>
</li>
<li><p>Unit test failures on Intel® Data Center GPU Max Series</p>
<p>The following unit tests randomly fail on Intel® Data Center GPU Flex Max Series.</p>
<ul class="simple">
<li><p>test_nn.py::TestNNDeviceTypeXPU::test_activations_bfloat16_xpu</p></li>
<li><p>test_lstm.py::TestNNMethod::test_lstm_rnnt_onednn</p></li>
<li><p>test_eigh.py::TestTorchMethod::test_linalg_eigh</p></li>
</ul>
<p>The test cases rarely fail if running with other test cases together using <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">-v</span></code>. These cases pass if run individually on the same environment. The root cause of the failures is under investigation.</p>
</li>
</ul>
</section>
</section>
<section id="known-issues-specific-to-cpu">
<h2>Known Issues Specific to CPU<a class="headerlink" href="#known-issues-specific-to-cpu" title="Permalink to this heading"></a></h2>
<section id="id1">
<h3>Usage<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<ul>
<li><p>There might be Python packages having PyTorch as their hard dependency. If you installed <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> version of PyTorch, installation of these packages might replace the <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> version with the default version released on Pypi.org. If anything goes wrong, please reinstall the <code class="docutils literal notranslate"><span class="pre">+cpu</span></code> version back.</p></li>
<li><p>If you found the workload runs with Intel® Extension for PyTorch* occupies a remarkably large amount of memory, you can try to reduce the occupied memory size by setting the <code class="docutils literal notranslate"><span class="pre">--weights_prepack</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>If inference is done with a custom function, <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding feature of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function doesn’t work.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_pytorch_extension</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="k">class</span> <span class="nc">Module</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Module</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">m</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a PyTorch FX limitation. You can avoid this error by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O0&quot;)</span></code>, which doesn’t apply ipex optimization, or disable <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O1&quot;,</span> <span class="pre">conv_bn_folding=False)</span></code>.</p>
</li>
</ul>
</section>
<section id="torchdynamo">
<h3>TorchDynamo<a class="headerlink" href="#torchdynamo" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>The support of torch.compile() with ipex as the backend is still an experimental feature. If the workload fails to run or demonstrates poor performance, you can use the <code class="docutils literal notranslate"><span class="pre">torch.jit</span></code> APIs and graph optimization APIs of ipex. Currently, the below HuggingFace models fail to run using torch.compile() with ipex backend due to memory issues:</p>
<ul>
<li><p>masked-language-modeling+xlm-roberta-base</p></li>
<li><p>casual-language-modeling+gpt2</p></li>
<li><p>casual-language-modeling+xlm-roberta-base</p></li>
<li><p>summarization+t5-base</p></li>
<li><p>text-classification+allenai-longformer-base-409</p></li>
</ul>
</li>
</ul>
</section>
<section id="dynamic-shape">
<h3>Dynamic Shape<a class="headerlink" href="#dynamic-shape" title="Permalink to this heading"></a></h3>
<ul>
<li><p>When working with an NLP model inference with dynamic input data length appling with TorchScript (either <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.jit.script</span></code>), performance with Intel® Extension for PyTorch* is possible to be less than that without Intel® Extension for PyTorch*. In this case, adding the workarounds below would help solve this issue.</p>
<ul>
<li><p>Python interface</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_texpr_fuser_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>C++ interface</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/jit/passes/tensorexpr_fuser.h&gt;</span><span class="cp"></span>
<span class="n">torch</span><span class="o">::</span><span class="n">jit</span><span class="o">::</span><span class="n">setTensorExprFuserEnabled</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="int8">
<h3>INT8<a class="headerlink" href="#int8" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Low performance with INT8 support for dynamic shapes</p>
<p>The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.</p>
<p><strong>Note</strong>: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.</p>
</li>
<li><p>Supporting of EmbeddingBag with INT8 when bag size &gt; 1 is working in progress.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Overflow</span> <span class="pre">when</span> <span class="pre">unpacking</span> <span class="pre">long</span></code> when a tensor’s min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.</p></li>
<li><p>For models with dynamic control flow, please try dynamic quantization. Users are likely to get performance gain for GEMM models.</p></li>
<li><p>Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP* thread, results might be incorrect with large tensors (find more detailed info <a class="reference external" href="https://github.com/pytorch/pytorch/issues/80501">here</a>. Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.</p>
<p>Workaround pseudocode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># perform convert/trace/freeze with omp_num_threads &gt; 1(N)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">converted_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">freezed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># run freezed model to apply optimization pass</span>
<span class="n">freezed_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># benchmarking with omp_num_threads = 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">run_benchmark</span><span class="p">(</span><span class="n">freezed_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="bfloat16">
<h3>BFloat16<a class="headerlink" href="#bfloat16" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains <code class="docutils literal notranslate"><span class="pre">Conv</span></code>, <code class="docutils literal notranslate"><span class="pre">Matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, and <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code></p></li>
</ul>
</section>
<section id="runtime-extension">
<h3>Runtime Extension<a class="headerlink" href="#runtime-extension" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Runtime extension of MultiStreamModule doesn’t support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can’t be simplely batch split.</p></li>
<li><p>Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, <code class="docutils literal notranslate"><span class="pre">joint_net</span></code> is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.</p></li>
</ul>
</section>
<section id="correctness">
<h3>Correctness<a class="headerlink" href="#correctness" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Incorrect Conv and Linear result if the number of OMP threads is changed at runtime</p>
<p>The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.</p>
</li>
</ul>
</section>
<section id="float32-training">
<h3>Float32 Training<a class="headerlink" href="#float32-training" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Low throughput with DLRM FP32 Train</p>
<p>A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending on review. The issue will be fixed when the PR is merged.</p>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchserve.html" class="btn btn-neutral float-left" title="TorchServe with Intel® Extension for PyTorch*" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../technical_details.html" class="btn btn-neutral float-right" title="Technical Details" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>