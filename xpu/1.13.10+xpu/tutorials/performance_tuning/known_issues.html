<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Known Issues &mdash; intel_extension_for_pytorch 1.13.10+xpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Technical Details" href="../technical_details.html" />
    <link rel="prev" title="TorchServe with Intel® Extension for PyTorch*" href="torchserve.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../../">1.13.10+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Known Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#known-issues-in-gpu-specific">Known Issues in GPU-Specific</a></li>
<li class="toctree-l3"><a class="reference internal" href="#known-issues-in-cpu-specific">Known Issues in CPU-Specific</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../performance_tuning.html">Performance Tuning Guide</a></li>
      <li class="breadcrumb-item active">Known Issues</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/known_issues.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="known-issues">
<h1>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading"></a></h1>
<section id="known-issues-in-gpu-specific">
<h2>Known Issues in GPU-Specific<a class="headerlink" href="#known-issues-in-gpu-specific" title="Permalink to this heading"></a></h2>
<ul>
<li><p>[CRITICAL ERROR] Kernel ‘XXX’ removed due to usage of FP64 instructions unsupported by the targeted hardware</p>
<p>FP64 is not natively supported by the <a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/data-center-gpu/flex-series/overview.html">Intel® Data Center GPU Flex Series</a> platform. If you run any AI workload on that platform and receive this error message, it means a kernel requiring FP64 instructions is removed and not executed, hence the accuracy of the whole workload is wrong.</p>
</li>
<li><p>symbol undefined caused by <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ImportError: undefined symbol: _ZNK5torch8autograd4Node4nameB5cxx11Ev
</pre></div>
</div>
<p>DPC++ does not support <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>, Intel® Extension for PyTorch* is always compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. This symbol undefined issue appears when PyTorch* is compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>. Pass <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLIBCXX_USE_CXX11_ABI=1</span></code> and compile PyTorch* with particular compiler which supports <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. We recommend using prebuilt wheels in <a class="reference external" href="https://developer.intel.com/ipex-whl-stable-xpu">download server</a> to avoid this issue.</p>
</li>
<li><p>Can’t find oneMKL library when build Intel® Extension for PyTorch* without oneMKL</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/bin/ld: cannot find -lmkl_sycl
/usr/bin/ld: cannot find -lmkl_intel_ilp64
/usr/bin/ld: cannot find -lmkl_core
/usr/bin/ld: cannot find -lmkl_tbb_thread
dpcpp: error: linker <span class="nb">command</span> failed with <span class="nb">exit</span> code <span class="m">1</span> <span class="o">(</span>use -v to see invocation<span class="o">)</span>
</pre></div>
</div>
<p>When PyTorch* is built with oneMKL library and Intel® Extension for PyTorch* is built without oneMKL library, this linker issue may occur. Resolve it by setting:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">USE_ONEMKL</span><span class="o">=</span>OFF
<span class="nb">export</span> <span class="nv">MKL_DPCPP_ROOT</span><span class="o">=</span><span class="si">${</span><span class="nv">PATH_To_Your_oneMKL</span><span class="si">}</span>/__release_lnx/mkl
</pre></div>
</div>
<p>Then clean build Intel® Extension for PyTorch*.</p>
</li>
<li><p>undefined symbol: <code class="docutils literal notranslate"><span class="pre">mkl_lapack_dspevd</span></code>. Intel MKL FATAL ERROR: cannot load <code class="docutils literal notranslate"><span class="pre">libmkl_vml_avx512.so.2</span></code> or <code class="docutils literal notranslate"><span class="pre">libmkl_vml_def.so.2</span></code></p>
<p>This issue may occur when Intel® Extension for PyTorch* is built with oneMKL library and PyTorch* is not build with any MKL library. The oneMKL kernel may run into CPU backend incorrectly and trigger this issue. Resolve it by installing MKL library from conda:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda install mkl
conda install mkl-include
</pre></div>
</div>
<p>then clean build PyTorch*.</p>
</li>
<li><p>OSError: <code class="docutils literal notranslate"><span class="pre">libmkl_intel_lp64.so.2</span></code>: cannot open shared object file: No such file or directory</p>
<p>Wrong MKL library is used when multiple MKL libraries exist in system. Preload oneMKL by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_lp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_ilp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_gnu_thread.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_core.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_sycl.so.2
</pre></div>
</div>
<p>If you continue seeing similar issues for other shared object files, add the corresponding files under <code class="docutils literal notranslate"><span class="pre">${MKL_DPCPP_ROOT}/lib/intel64/</span></code> by <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>. Note that the suffix of the libraries may change (e.g. from .1 to .2), if more than one oneMKL library is installed on the system.</p>
</li>
<li><p>RuntimeError: Number of dpcpp devices should be greater than zero!</p>
<p>Running some AI models (e.g. 3D-Unet inference) on Ubuntu22.04 may trigger this runtime error, as oneAPI Base Toolkit 2023.0 fails to return available GPU device on ubuntu22.04 in such scenario. The workaround solution is to update the model script to make sure <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch</span></code> and <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code> happen before importing other libraries.</p>
</li>
<li><p>OpenMP library could not be found</p>
<p>Build Intel® Extension for PyTorch* on SLES15 SP3 using default GCC 7.5 and CentOS8 using default GCC 8.5 may trigger this build error.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Make Error at third_party/ideep/mkl-dnn/third_party/oneDNN/cmake/OpenMP.cmake:118 <span class="o">(</span>message<span class="o">)</span>:
  OpenMP library could not be found.  Proceeding might lead to highly
  sub-optimal performance.
Call Stack <span class="o">(</span>most recent call first<span class="o">)</span>:
  third_party/ideep/mkl-dnn/third_party/oneDNN/CMakeLists.txt:117 <span class="o">(</span>include<span class="o">)</span>
</pre></div>
</div>
<p>The root cause is GCC 7.5 or 8.5 does not support <code class="docutils literal notranslate"><span class="pre">-Wno-error=redundant-move</span></code> option. Uplift to GCC version &gt;=9 can solve this issue.</p>
</li>
<li><p>Unit test failures on Intel® Data Center GPU Flex Series 170</p>
<p>The following unit tests fail on Intel® Data Center GPU Flex Series 170.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_groupnorm</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">TestTorchMethod</span><span class="p">::</span><span class="n">test_group_norm_backward</span> 
<span class="n">test_groupnorm_channels_last</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">TestTorchMethod</span><span class="p">::</span><span class="n">test_group_norm_backward</span>
<span class="n">test_fusion</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">TestNNMethod</span><span class="p">::</span><span class="n">test_conv_binary_mul</span>
</pre></div>
</div>
<p>The same test cases pass on Intel® Data Center GPU Max Series. The root cause of the failures is under investigation.</p>
</li>
</ul>
</section>
<section id="known-issues-in-cpu-specific">
<h2>Known Issues in CPU-Specific<a class="headerlink" href="#known-issues-in-cpu-specific" title="Permalink to this heading"></a></h2>
<ul>
<li><p>If you found the workload runs with Intel® Extension for PyTorch* occupies a remarkably large amount of memory, you can try to reduce the occupied memory size by setting the <code class="docutils literal notranslate"><span class="pre">--weights_prepack</span></code> parameter of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>Supporting of EmbeddingBag with INT8 when bag size &gt; 1 is working in progress.</p></li>
<li><p>Compiling with gcc 11 might result in <code class="docutils literal notranslate"><span class="pre">illegal</span> <span class="pre">instruction</span></code> error.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Overflow</span> <span class="pre">when</span> <span class="pre">unpacking</span> <span class="pre">long</span></code> when a tensor’s min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.</p></li>
<li><p>For models with dynamic control flow, please try dynamic quantization. Users are likely to get performance gain for GEMM models.</p></li>
<li><p>Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP* thread, results might be incorrect with large tensors (find more detailed info <a class="reference external" href="https://github.com/pytorch/pytorch/issues/80501">here</a>. Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.</p>
<p>Workaround pseudocode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># perform convert/trace/freeze with omp_num_threads &gt; 1(N)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">converted_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">freezed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># run freezed model to apply optimization pass</span>
<span class="n">freezed_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># benchmarking with omp_num_threads = 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">run_benchmark</span><span class="p">(</span><span class="n">freezed_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains <code class="docutils literal notranslate"><span class="pre">Conv</span></code>, <code class="docutils literal notranslate"><span class="pre">Matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, and <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code></p></li>
<li><p>Runtime extension of MultiStreamModule doesn’t support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can’t be simplely batch split.</p></li>
<li><p>Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, <code class="docutils literal notranslate"><span class="pre">joint_net</span></code> is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.</p></li>
<li><p>Incorrect Conv and Linear result if the number of OMP threads is changed at runtime</p>
<p>The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.</p>
</li>
<li><p>Low performance with INT8 support for dynamic shapes</p>
<p>The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.</p>
<p><strong>Note</strong>: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.</p>
</li>
<li><p>Low throughput with DLRM FP32 Train</p>
<p>A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending on review. The issue will be fixed when the PR is merged.</p>
</li>
<li><p>If inference is done with a custom function, <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding feature of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function doesn’t work.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_pytorch_extension</span> <span class="k">as</span> <span class="nn">ipex</span>

<span class="k">class</span> <span class="nc">Module</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Module</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">m</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a PyTorch FX limitation. You can avoid this error by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O0&quot;)</span></code>, which doesn’t apply ipex optimization, or disable <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O1&quot;,</span> <span class="pre">conv_bn_folding=False)</span></code>.</p>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchserve.html" class="btn btn-neutral float-left" title="TorchServe with Intel® Extension for PyTorch*" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../technical_details.html" class="btn btn-neutral float-right" title="Technical Details" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>