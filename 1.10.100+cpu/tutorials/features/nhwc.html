<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Channels Last &mdash; intel_extension_for_pytorch 1.10.100+cpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Auto Mixed Precision (AMP)" href="amp.html" />
    <link rel="prev" title="Features" href="../features.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../versions.html">1.10.100+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#ease-of-use-python-api">Ease-of-use Python API</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#channels-last">Channels Last</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Channels Last</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-is-channels-last">What is Channels Last</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-format-is-all-that-matters">Memory Format Is All That Matters</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-nchw-default">a. NCHW (default)</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-nhwc-wip-for-cpu">b. NHWC (WIP for CPU)</a></li>
<li class="toctree-l5"><a class="reference internal" href="#c-blocked-nchw16c">c. Blocked (nChw16c)</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#pytorch-strided-layout">PyTorch Strided Layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pytorch-channels-last-memory-format-apis">PyTorch Channels Last Memory Format APIs</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-tensor-creation">a. tensor creation</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-tensor-conversion">b. tensor conversion</a></li>
<li class="toctree-l5"><a class="reference internal" href="#c-model-conversion">c. model conversion</a></li>
<li class="toctree-l5"><a class="reference internal" href="#d-operator-coverage">d. operator coverage</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#writing-channels-last-kernels">Writing Channels Last Kernels</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-status-on-cpu">a. Status on CPU</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-register-channels-last-kernel-in-aten-native-manner">b. Register Channels Last Kernel in ATen Native Manner</a></li>
<li class="toctree-l5"><a class="reference internal" href="#c-register-onednn-kernel-on-channels-last">c. Register oneDNN Kernel on Channels Last</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#onednn-nhwc-apis">oneDNN NHWC APIs</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-create-nhwc-memory">a. Create NHWC Memory</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-create-convolution-primitive">b. Create Convolution Primitive</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#cpu-channels-last-targets">CPU Channels Last Targets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#operator-optimization">Operator Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#optimizer-optimization">Optimizer Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#runtime-extension-experimental">Runtime Extension (Experimental)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#int8-quantization-experimental">INT8 Quantization (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../features.html">Features</a> &raquo;</li>
      <li>Channels Last</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/nhwc.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="channels-last">
<h1>Channels Last<a class="headerlink" href="#channels-last" title="Permalink to this headline"></a></h1>
<section id="what-is-channels-last">
<h2>What is Channels Last<a class="headerlink" href="#what-is-channels-last" title="Permalink to this headline"></a></h2>
<p><strong>NB</strong>: <strong>Memory format</strong> refers to data representation that describes how multidimensional arrays (nD) are stored in linear (1D) memory address space. <strong>Memory format</strong> has the same semantic with <strong>layout</strong> in oneDNN. <strong>Layout</strong> in PyTorch has other semantic ofdescribing <strong>dense</strong> or <strong>sparse</strong> with the attributes: ‘torch.strided’, ‘torch.sparse_coo’.</p>
<p>On CNN models, the canonical order of tensor dimensions are assigned with semantic meaning. For example the input tensor of 2D convolution is of NCHW by default on PyTorch - &lt;batch_size, channels, height, width&gt;. NHWC is an alternative way of describing the tensor dimensions - &lt;batch_size, height, width, channels&gt;.</p>
<p>Take a look at the following image of illustrating NCHW and NHWC when N=1. Actually when N=1, NHWC has the same format with BMP file image.
<img alt="fig-1-memory-layout" src="../../_images/figure1_memory_layout.png" /></p>
<p>PyTorch refers NCHW as <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code> which is the default memory format and NHWC as <code class="docutils literal notranslate"><span class="pre">torch.channels_last</span></code> which is an new feature from 1.5 release.</p>
<p>TensorFlow takes NHWC as the default memory format and from the performance point of view NHWC has advantage over NCHW. On CPU platforms, we propose to optimize Channels Last memory path out of the following reasones:</p>
<ul class="simple">
<li><p><strong>Performance</strong> - NHWC performance is not as good as blocked memory format (nChw16c), but it is close and much better than NCHW.</p></li>
<li><p><strong>User Experience</strong> - Operator coverage of NHWC would be higher than blocked memory format (<code class="docutils literal notranslate"><span class="pre">to_mkldnn()</span></code> method) so user experience is better. To be specific, it would be very difficult to enable operator that manipulates <code class="docutils literal notranslate"><span class="pre">dim</span></code> on blocked format such as <code class="docutils literal notranslate"><span class="pre">sum(dim=?)</span></code> so you need to convert tensor from blocked memory format back to NHWC by <code class="docutils literal notranslate"><span class="pre">to_dense()</span></code> before feeding it into <code class="docutils literal notranslate"><span class="pre">sum()</span></code>. But it is naturally supported on Channels Last memory format already.</p></li>
<li><p><strong>Upstream</strong> - Will be easier since CPU doesn’t hold secret ingredient and both inference and training will be covered.</p></li>
</ul>
</section>
<section id="memory-format-is-all-that-matters">
<h2>Memory Format Is All That Matters<a class="headerlink" href="#memory-format-is-all-that-matters" title="Permalink to this headline"></a></h2>
<p>On CNN models, memory format is all most the foundation of any upper level design. One imporant fact is that converting memory format could be very expensive. Thus, in case that multiple CNN operators are performed in sequence, e.g. <code class="docutils literal notranslate"><span class="pre">Conv2d</span> <span class="pre">-&gt;</span> <span class="pre">ReLU</span> <span class="pre">-&gt;</span> <span class="pre">Conv2d</span></code>, it’s beneficial to transform them from different memory formats once, do computation and reorder them back.</p>
<p>On PyTorch, you can use 3 types of memory formats on CNN models:</p>
<section id="a-nchw-default">
<h3>a. NCHW (default)<a class="headerlink" href="#a-nchw-default" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## NB: internally blocked format will still be used.</span>
<span class="c1">##   aka. we do &#39;reorder&#39; for &#39;input&#39;, &#39;weight&#39; and &#39;output&#39;,</span>
<span class="c1">##   and believe me this is expensive, roughly 50% perf loss...</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="b-nhwc-wip-for-cpu">
<h3>b. NHWC (WIP for CPU)<a class="headerlink" href="#b-nhwc-wip-for-cpu" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1">## NB: convert to Channels Last memory format.</span>
<span class="c1">##   oneDNN supports NHWC for feature maps (input, output),</span>
<span class="c1">##   but weight still needs to be of blocked format.</span>
<span class="c1">##   Still we can save reorders for feature maps.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="c-blocked-nchw16c">
<h3>c. Blocked (nChw16c)<a class="headerlink" href="#c-blocked-nchw16c" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">mkldnn</span> <span class="k">as</span> <span class="n">mkldnn_utils</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1">## NB: convert to blocked memory format.</span>
<span class="c1">##   Note that &#39;output&#39; is in blocked memory format,</span>
<span class="c1">##   in case the subsequent operator doesn&#39;t support blocked memory format</span>
<span class="c1">##   you need to manually reorder it back to NCHW by output.to_dense()</span>
<span class="c1">##   mkldnn_utils.to_mkldnn(model) is used to prepack the weight, this will save weight reorder time</span>
<span class="c1">##   for inference. For training, it is not needed.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to_mkldnn</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mkldnn_utils</span><span class="o">.</span><span class="n">to_mkldnn</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>Better to explain the concepts here with a diagram, the <strong>dotted lines</strong> indicate simple memory view, no hard copy.
<img alt="fig-2(1)-pt-conv-layout-path-dispatch" src="../../_images/figure2_dispatch.png" /></p>
<p><strong>Conclusion</strong> is that NHWC path saves the reorders from feature maps compared with NCHW path, but still weight reorder is necessary since oneDNN requires weights to be in blocked memory format. From performance perspective, when <code class="docutils literal notranslate"><span class="pre">batch_size=N</span></code>, weight reorder is minimum compared to feature map reorder. But when <code class="docutils literal notranslate"><span class="pre">batch_size=1</span></code>, weight reoder is usually not negligible. So whether to enable weight prepacking on channels last memory format needs further discussion.</p>
</section>
</section>
<section id="pytorch-strided-layout">
<h2>PyTorch Strided Layout<a class="headerlink" href="#pytorch-strided-layout" title="Permalink to this headline"></a></h2>
<p>Before moving on, I feel it is necessary to explain how PyTorch organizes tensors in memory - the <strong>layout</strong>. Here we only focus on <strong>dense</strong> tensors, skip ‘coo’ layout of <strong>sparse</strong> tensor.</p>
<p>The question itself can be reinterpreted as for a tensor of size &lt;N, C, H, W&gt;, how does PyTorch accesses the element with index &lt;n, w, h, w&gt; from memory, the answer is <strong>stride</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="o">&gt;</span>
<span class="n">index</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="o">&gt;</span>
<span class="n">strides</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">CHW</span><span class="p">,</span> <span class="n">HW</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;</span>
<span class="n">offset</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">=</span> <span class="n">stride_n</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">stride_c</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">stride_h</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">stride_w</span> <span class="o">*</span> <span class="n">w</span>
                <span class="o">=</span> <span class="n">CHW</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">HW</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">W</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">w</span>
</pre></div>
</div>
<p>One merit of introducing <strong>stride</strong> is it will be able to express noncontiguous tensors, e.g. a slice of big tensor. For example, the ‘Xs’ in the following image has a stride of &lt;n1+n2, 1&gt;.</p>
<p><img alt="fig-3-pytorch-strided-layout" src="../../_images/figure3_strided_layout.png" /></p>
<p>Keep in mind that PyTorch Tensor does not have an attribute so called ‘memory_format’ or something else. The memory format expression completely relies on <strong>size</strong> and <strong>stride</strong>, design principle can be found at reference: <a class="reference external" href="https://github.com/pytorch/pytorch/issues/19092">RFC: Memory format (aka layout aka NHWC) support</a>. So no matter what the tensor’s memory format is, we need a logical canonical order for the dimensions - that is <strong>NCHW</strong> on PyTorch. Thus, <strong>size</strong> and <strong>stride</strong> are ALWAYS described in the order of <strong>NCHW</strong>. OK let’s take a look at the Channels Last case of the previous question:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="o">&gt;</span>
<span class="n">index</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="o">&gt;</span>
<span class="n">strides</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">HWC</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WC</span><span class="p">,</span> <span class="n">C</span><span class="o">&gt;</span>
<span class="n">offset</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">=</span> <span class="n">stride_n</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">stride_c</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">stride_h</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">stride_w</span> <span class="o">*</span> <span class="n">w</span>
                <span class="o">=</span> <span class="n">HWC</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">WC</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">C</span> <span class="o">*</span> <span class="n">w</span>
</pre></div>
</div>
<p>Actually, this pattern applies to ALL other memory formats as long as it is 4-dim, e.g. strides for CHWN would be &lt;1, HWN, WN, N&gt;.</p>
</section>
<section id="pytorch-channels-last-memory-format-apis">
<h2>PyTorch Channels Last Memory Format APIs<a class="headerlink" href="#pytorch-channels-last-memory-format-apis" title="Permalink to this headline"></a></h2>
<section id="a-tensor-creation">
<h3>a. tensor creation<a class="headerlink" href="#a-tensor-creation" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="b-tensor-conversion">
<h3>b. tensor conversion<a class="headerlink" href="#b-tensor-conversion" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## .contiguous() transforms NHWC noncontiguous to NHWC contiguous.</span>
<span class="c1">## .to() converts NCHW tensor to NHWC one, it is outplace.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>

<span class="c1">## contiguous check</span>
<span class="n">x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="c-model-conversion">
<h3>c. model conversion<a class="headerlink" href="#c-model-conversion" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## NB: tensor.to() is an outplace operation</span>
<span class="c1">##   model.to() is inplace. It calls _apply() which is inplace.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="d-operator-coverage">
<h3>d. operator coverage<a class="headerlink" href="#d-operator-coverage" title="Permalink to this headline"></a></h3>
<p>Detailed operator coverage information has been listed at reference <a class="reference external" href="https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support">Operators-with-Channels-Last-support</a>. In brief, ImageNet training topologies on GPU already have full support on Channels Last memory format, while CPU doesn’t.</p>
<p>Some spontaneous questions:</p>
<ul class="simple">
<li><p><strong>How to tell whether this model or operator support Channels Last?</strong> - This requires mannual memory format check, aka. ‘torch.channels_last’ input and weight shall NOT generate ‘torch.contiguous_format’ output.</p></li>
<li><p><strong>What if the model comprises of operator not supported Channels Last?</strong> - No errors messages will be shown, the NHWC tensor will be handled by the operator as a non-contiguous NCHW tensor, so result might not be correct depending on the algorithm of this operator.</p></li>
</ul>
</section>
</section>
<section id="writing-channels-last-kernels">
<h2>Writing Channels Last Kernels<a class="headerlink" href="#writing-channels-last-kernels" title="Permalink to this headline"></a></h2>
<section id="a-status-on-cpu">
<h3>a. Status on CPU<a class="headerlink" href="#a-status-on-cpu" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><strong>No support</strong> - Requires to register Channels Last kernel for CPU path, e.g. Conv2d;</p></li>
<li><p><strong>Explicit support</strong> - Already have Channels Last kernel for CPU path (in ATen native manner), need to compare oneDNN counterpart performance, e.g. BatchNorm;</p></li>
<li><p><strong>Implicit support</strong> - Supported via meta structures like ‘TensorIterator’, need to compare oneDNN counterpart performance, e.g. ReLU.</p></li>
</ul>
</section>
<section id="b-register-channels-last-kernel-in-aten-native-manner">
<h3>b. Register Channels Last Kernel in ATen Native Manner<a class="headerlink" href="#b-register-channels-last-kernel-in-aten-native-manner" title="Permalink to this headline"></a></h3>
<p>The general guideline has been listed under reference <a class="reference external" href="https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators">Writing-memory-format-aware-operators</a>, not to repeat here. You may take one of my recent PR <a class="reference external" href="https://github.com/pytorch/pytorch/pull/34864">optimize upsample performance linear mode on CPU</a> as an example, which also demonstrates NHWC performance advantage over NCHW because of the ease of vectorization.</p>
</section>
<section id="c-register-onednn-kernel-on-channels-last">
<h3>c. Register oneDNN Kernel on Channels Last<a class="headerlink" href="#c-register-onednn-kernel-on-channels-last" title="Permalink to this headline"></a></h3>
<p>Essence of registering an oneDNN kernel under Channels Last memory format on CPU is no differenct from <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23861">cuDNN</a>: Only very few upper level change is needed such as accommodate ‘contiguous()’ to ‘contiguous(suggested_memory_format)’. The automatic reorder of oneDNN weight shall been hided in ideep.</p>
</section>
</section>
<section id="onednn-nhwc-apis">
<h2>oneDNN NHWC APIs<a class="headerlink" href="#onednn-nhwc-apis" title="Permalink to this headline"></a></h2>
<p>Compared to NCHW interfaces, 2 parts need to be addressed on NHWC inferfaces:</p>
<section id="a-create-nhwc-memory">
<h3>a. Create NHWC Memory<a class="headerlink" href="#a-create-nhwc-memory" title="Permalink to this headline"></a></h3>
<p>The logical size and stride description of oneDNN is always in NCHW, this is identical to PyTorch. Example code such as</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cm">/* create md from memory::format_tag */</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">src_md</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">},</span><span class="w"> </span><span class="c1">// logical dims, the order is defined by a primitive</span>
<span class="w">        </span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span><span class="w"> </span><span class="c1">// tensor&#39;s data type</span>
<span class="w">        </span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">nhwc</span><span class="w"> </span><span class="c1">// memory format, NHWC in this case</span>
<span class="p">);</span><span class="w"></span>

<span class="cm">/* alternative: create md from strides */</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">src_md</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="p">{</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">},</span><span class="w"> </span><span class="c1">// logical dims, the order is defined by a primitive</span>
<span class="w">        </span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span><span class="w"> </span><span class="c1">// tensor&#39;s data type</span>
<span class="w">        </span><span class="p">{</span><span class="n">stride_N</span><span class="p">,</span><span class="w"> </span><span class="n">stride_C</span><span class="p">,</span><span class="w"> </span><span class="n">stride_H</span><span class="p">,</span><span class="w"> </span><span class="n">stride_W</span><span class="p">}</span><span class="w"> </span><span class="c1">// the strides</span>
<span class="p">);</span><span class="w"></span>

<span class="cm">/* create memory */</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">src_mem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memory</span><span class="p">(</span><span class="n">src_md</span><span class="p">,</span><span class="w"> </span><span class="n">src_data_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">engine</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="b-create-convolution-primitive">
<h3>b. Create Convolution Primitive<a class="headerlink" href="#b-create-convolution-primitive" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p><strong>NCHW</strong> - create <code class="docutils literal notranslate"><span class="pre">memory::desc</span></code> with <em>any</em> card for ‘input’, ‘output’ and ‘weight’; query proposed <code class="docutils literal notranslate"><span class="pre">memory::desc</span></code> from convolution primitive;</p></li>
<li><p><strong>NHWC</strong> - create <code class="docutils literal notranslate"><span class="pre">memory::desc</span></code> with <code class="docutils literal notranslate"><span class="pre">format_tag::nhwc</span></code> for ‘input’ and ‘output’, use <em>any</em> for ‘weight’; if we use <code class="docutils literal notranslate"><span class="pre">hwio</span></code> for ‘weight’ convolution primitive will be created with gemm rather jit avx512.</p></li>
</ul>
</section>
</section>
<section id="cpu-channels-last-targets">
<h2>CPU Channels Last Targets<a class="headerlink" href="#cpu-channels-last-targets" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>User Experience</strong> - No special user level code change, only ‘input’ and ‘model’ conversion is required;</p></li>
<li><p><strong>Scenarios</strong> - cover both training and inference;</p></li>
<li><p><strong>Models</strong> - ResNet50 and ResNext101, extended targets: torchvision models, detectron2;</p></li>
<li><p><strong>Performance Targets</strong> - training &gt;0.8x blocked; inference throughput &gt; 0.8x blocked; inference latency? (need further discussion)</p></li>
<li><p><strong>Operator Converage</strong> - No less than GPU path;</p></li>
<li><p><strong>BFloat16</strong> - This part shall align with big picture of BFloat16 integration (need further discussion);</p></li>
<li><p><strong>int8</strong> - Need further discussion.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../features.html" class="btn btn-neutral float-left" title="Features" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="amp.html" class="btn btn-neutral float-right" title="Auto Mixed Precision (AMP)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
